{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f83bcd13",
   "metadata": {},
   "source": [
    "# SECOM Yield Prediction — End-to-End Notebook\n",
    "\n",
    "**Goal:** Predict *Fail* outcomes from process measurements to reduce scrap and downtime.\n",
    "\n",
    "**Data:** `data/secom.data`, `data/secom_labels.data`, `data/secom.names` (UCI ML Repository, real fab data).\n",
    "\n",
    "**Primary metric:** Recall on *Fail* at acceptable precision. Report PR-AUC and Balanced Error Rate (BER)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea5fe5b",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e1f56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Global style for plots\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 12,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"axes.edgecolor\": \"black\",\n",
    "    \"axes.linewidth\": 1,\n",
    "    \"xtick.color\": \"black\",\n",
    "    \"ytick.color\": \"black\",\n",
    "    \"xtick.direction\": \"out\",\n",
    "    \"ytick.direction\": \"out\",\n",
    "    \"xtick.bottom\": True,\n",
    "    \"ytick.left\": True,\n",
    "    \"xtick.top\": False,\n",
    "    \"ytick.right\": False,\n",
    "    \"figure.dpi\": 150,\n",
    "    \"legend.frameon\": True,\n",
    "    \"legend.facecolor\": \"white\",\n",
    "    \"legend.edgecolor\": \"black\",\n",
    "    \"legend.fontsize\": 12\n",
    "})\n",
    "\n",
    "# Apply to seaborn\n",
    "sns.set_theme(context=\"notebook\", style=\"ticks\")\n",
    "\n",
    "# Reproducibility\n",
    "rnd_num = 42\n",
    "rng = np.random.seed(rnd_num)\n",
    "\n",
    "# Root directory\n",
    "ROOT = Path(os.getcwd()).parent\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = Path(ROOT/\"data\")\n",
    "RAW = Path(DATA_DIR/\"raw\")\n",
    "assert (RAW/\"secom.data\").exists() and (RAW/\"secom_labels.data\").exists(), \"Data files are missing!\"\n",
    "\n",
    "# Results directory\n",
    "RESULT_DIR = Path(ROOT/\"results\")\n",
    "RESULT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Model directory\n",
    "MODEL_DIR = Path(ROOT/\"models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bc4af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure saver helper (use after each plot)\n",
    "RESULT_DIR_AR = Path(RESULT_DIR/\"advanced_research\")\n",
    "RESULT_DIR_AR.mkdir(exist_ok=True, parents=True)\n",
    "def savefig(name):\n",
    "    out = RESULT_DIR_AR/name\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out, dpi=300, bbox_inches=\"tight\")\n",
    "    print(\"Saved:\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e73ee30",
   "metadata": {},
   "source": [
    "### 2. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fef6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "import shap\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import brier_score_loss, average_precision_score, roc_auc_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "from sklearn.metrics import (precision_recall_curve, average_precision_score,\n",
    "                             roc_auc_score, balanced_accuracy_score,\n",
    "                             confusion_matrix, classification_report,\n",
    "                             brier_score_loss, roc_curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f3dd2",
   "metadata": {},
   "source": [
    "### 3. Load data from 04_Interpretability.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eb1043",
   "metadata": {},
   "outputs": [],
   "source": [
    "ART  = ROOT/\"artifacts\"\n",
    "\n",
    "splits   = joblib.load(ART/\"data_splits.joblib\")\n",
    "feat_cols = splits[\"feat_cols\"]\n",
    "Xva, yva, Xte, yte = splits[\"Xva\"], splits[\"yva\"], splits[\"Xte\"], splits[\"yte\"]\n",
    "\n",
    "test_probs = joblib.load(ART/\"test_probs.joblib\")    # model -> p_test\n",
    "val_probs  = joblib.load(ART/\"val_probs.joblib\")     # model -> p_val\n",
    "ths        = joblib.load(ART/\"thresholds.joblib\")    # dicts: thr_f1, thr_r10\n",
    "thr_f1, thr_r10 = ths[\"thr_f1\"], ths[\"thr_r10\"]\n",
    "\n",
    "# Optional artifacts\n",
    "shap_topk   = joblib.load(ART/\"shap_topk.joblib\") if (ART/\"shap_topk.joblib\").exists() else {}\n",
    "df_cost     = joblib.load(ART/\"cost_sweep_df.joblib\") if (ART/\"cost_sweep_df.joblib\").exists() else None\n",
    "df_boot     = joblib.load(ART/\"bootstrap_df.joblib\") if (ART/\"bootstrap_df.joblib\").exists() else None\n",
    "\n",
    "print(\"Loaded:\", list(test_probs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c113fbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only models present in both val & test and with thresholds\n",
    "common = set(test_probs) & set(val_probs) & set(thr_f1) & set(thr_r10)\n",
    "test_probs = {k: test_probs[k] for k in common}\n",
    "val_probs  = {k: val_probs[k]  for k in common}\n",
    "thr_f1     = {k: thr_f1[k]     for k in common}\n",
    "thr_r10    = {k: thr_r10[k]    for k in common}\n",
    "print(\"Models in scope:\", sorted(common))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73979493",
   "metadata": {},
   "source": [
    "### 1. Isotonic calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a559a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- fit isotonic per model on validation ---\n",
    "def _fit_iso(y_val, p_val):\n",
    "    iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "    iso.fit(np.asarray(p_val, float), np.asarray(y_val, int))\n",
    "    return iso\n",
    "\n",
    "def _basic_scores(y, p):\n",
    "    return dict(\n",
    "        Brier=brier_score_loss(y, p),\n",
    "        PR_AUC=average_precision_score(y, p),\n",
    "        ROC_AUC=roc_auc_score(y, p),\n",
    "    )\n",
    "\n",
    "iso_maps       = {}   # model -> IsotonicRegression\n",
    "cal_val_probs  = {}   # model -> calibrated p_val\n",
    "cal_test_probs = {}   # model -> calibrated p_test\n",
    "rows = []\n",
    "\n",
    "for model in sorted(test_probs.keys()):\n",
    "    p_val  = np.asarray(val_probs[model],  dtype=float)\n",
    "    p_test = np.asarray(test_probs[model], dtype=float)\n",
    "\n",
    "    iso = _fit_iso(yva, p_val)\n",
    "    iso_maps[model] = iso\n",
    "\n",
    "    p_val_iso  = iso.predict(p_val)\n",
    "    p_test_iso = iso.predict(p_test)\n",
    "    cal_val_probs[model]  = p_val_iso\n",
    "    cal_test_probs[model] = p_test_iso\n",
    "\n",
    "    m_val_raw = _basic_scores(yva, p_val)\n",
    "    m_val_iso = _basic_scores(yva, p_val_iso)\n",
    "    m_te_raw  = _basic_scores(yte, p_test)\n",
    "    m_te_iso  = _basic_scores(yte, p_test_iso)\n",
    "\n",
    "    rows.append({\n",
    "        \"Model\": model, \"Calibrator\": \"isotonic\",\n",
    "        \"Brier_VAL_raw\": m_val_raw[\"Brier\"], \"Brier_VAL_cal\": m_val_iso[\"Brier\"],\n",
    "        \"PRAUC_VAL_raw\": m_val_raw[\"PR_AUC\"], \"PRAUC_VAL_cal\": m_val_iso[\"PR_AUC\"],\n",
    "        \"ROCAUC_VAL_raw\": m_val_raw[\"ROC_AUC\"], \"ROCAUC_VAL_cal\": m_val_iso[\"ROC_AUC\"],\n",
    "        \"Brier_TEST_raw\": m_te_raw[\"Brier\"], \"Brier_TEST_cal\": m_te_iso[\"Brier\"],\n",
    "        \"PRAUC_TEST_raw\": m_te_raw[\"PR_AUC\"], \"PRAUC_TEST_cal\": m_te_iso[\"PR_AUC\"],\n",
    "        \"ROCAUC_TEST_raw\": m_te_raw[\"ROC_AUC\"], \"ROCAUC_TEST_cal\": m_te_iso[\"ROC_AUC\"],\n",
    "    })\n",
    "\n",
    "df_iso = pd.DataFrame(rows).round(4)\n",
    "display(df_iso)\n",
    "\n",
    "# persist artifacts\n",
    "ART.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(cal_test_probs, ART/\"test_probs_iso.joblib\")\n",
    "joblib.dump(cal_val_probs,  ART/\"val_probs_iso.joblib\")\n",
    "joblib.dump({k: (iso_maps[k].X_thresholds_, iso_maps[k].y_thresholds_) for k in iso_maps},\n",
    "            ART/\"iso_maps_thresholds.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Helper: access raw vs calibrated probs ----------\n",
    "def get_probs(model, split=\"test\", calibrated=\"raw\"):\n",
    "    \"\"\"\n",
    "    split: 'val' or 'test'\n",
    "    calibrated: 'raw' or 'iso'\n",
    "    \"\"\"\n",
    "    if split not in {\"val\",\"test\"}:\n",
    "        raise ValueError(\"split must be 'val' or 'test'\")\n",
    "    base = val_probs if split==\"val\" else test_probs\n",
    "\n",
    "    if calibrated == \"iso\":\n",
    "        iso_dict = cal_val_probs if split==\"val\" else cal_test_probs\n",
    "        return iso_dict.get(model, base.get(model))\n",
    "    return base.get(model)\n",
    "\n",
    "# ---------- Reliability plots (TEST; raw vs isotonic) ----------\n",
    "def reliability_plot(model, bins=8):\n",
    "    p_raw = get_probs(model, split=\"test\", calibrated=\"raw\")\n",
    "    p_iso = get_probs(model, split=\"test\", calibrated=\"iso\")\n",
    "    if p_raw is None or p_iso is None:\n",
    "        print(f\"[skip] {model} missing probabilities.\")\n",
    "        return\n",
    "    f_raw, m_raw = calibration_curve(yte, p_raw, n_bins=bins, strategy=\"quantile\")\n",
    "    f_iso, m_iso = calibration_curve(yte, p_iso, n_bins=bins, strategy=\"quantile\")\n",
    "\n",
    "    plt.figure(figsize=(6.2, 4.8))\n",
    "    plt.plot([0,1],[0,1],\"--\", color=\"gray\", lw=1, label=\"perfect\")\n",
    "    plt.plot(m_raw, f_raw, \"o-\", lw=1.2, label=f\"{model} raw\")\n",
    "    plt.plot(m_iso, f_iso, \"o-\", lw=1.2, label=f\"{model} isotonic\")\n",
    "    plt.xlabel(\"Mean predicted probability\"); plt.ylabel(\"Empirical frequency\")\n",
    "    plt.title(f\"Reliability — {model}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    savefig(f\"01_reliability_{model.replace(' ','_')}.png\")\n",
    "    plt.show()\n",
    "\n",
    "for m in sorted(test_probs.keys()):\n",
    "    reliability_plot(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e759a3c5",
   "metadata": {},
   "source": [
    "### 2. Decision Curve Analysis (TEST; raw vs isotonic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4f8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Decision Curve Analysis (TEST; raw vs isotonic) ----------\n",
    "def decision_curve(y, p, thresholds):\n",
    "    y = np.asarray(y, dtype=int)\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    N = len(y); prev = y.mean()\n",
    "    out = []\n",
    "    for t in thresholds:\n",
    "        pred = (p >= t).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "        w = t / (1.0 - t + 1e-12)\n",
    "        nb_model = (tp / N) - (fp / N) * w\n",
    "        nb_all   = prev - (1 - prev) * w\n",
    "        out.append((t, nb_model, nb_all))\n",
    "    out = np.array(out)\n",
    "    return out[:,0], out[:,1], out[:,2]\n",
    "\n",
    "t_grid = np.linspace(0.01, 0.15, 60)\n",
    "\n",
    "def plot_dca(models=None, calibrated=\"raw\"):\n",
    "    if models is None:\n",
    "        models = sorted(test_probs.keys())\n",
    "\n",
    "    plt.figure(figsize=(7.8, 5.0))\n",
    "    show_baselines = True\n",
    "    for m in models:\n",
    "        p = get_probs(m, split=\"test\", calibrated=calibrated)\n",
    "        if p is None: \n",
    "            continue\n",
    "        t, nb, nb_all = decision_curve(yte, p, t_grid)\n",
    "        plt.plot(t, nb, lw=1.8, label=f\"{m} ({calibrated})\")\n",
    "        if show_baselines:\n",
    "            plt.plot(t, nb_all, \"--\", color=\"gray\", label=\"treat-all\")\n",
    "            plt.plot(t, np.zeros_like(t), \":\", color=\"gray\", label=\"treat-none\")\n",
    "            show_baselines = False\n",
    "\n",
    "    plt.xlabel(\"Threshold probability (t)\")\n",
    "    plt.ylabel(\"Net benefit\")\n",
    "    plt.title(f\"Decision Curves — {calibrated.capitalize()}\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(ncol=2, fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    savefig(f\"02_dca_{calibrated}.png\")\n",
    "    plt.show()\n",
    "\n",
    "plot_dca(calibrated=\"raw\")\n",
    "plot_dca(calibrated=\"iso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be250a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Dynamic comparison: Raw vs Isotonic (VAL/TEST + DCA summary) ===\n",
    "def _delta(a_cal, a_raw): \n",
    "    return float(a_cal) - float(a_raw)\n",
    "\n",
    "# 1) Start from df_iso already computed\n",
    "cols_keep = [\n",
    "    \"Model\",\n",
    "    \"Brier_VAL_raw\",\"Brier_VAL_cal\",\n",
    "    \"PRAUC_VAL_raw\",\"PRAUC_VAL_cal\",\n",
    "    \"ROCAUC_VAL_raw\",\"ROCAUC_VAL_cal\",\n",
    "    \"Brier_TEST_raw\",\"Brier_TEST_cal\",\n",
    "    \"PRAUC_TEST_raw\",\"PRAUC_TEST_cal\",\n",
    "    \"ROCAUC_TEST_raw\",\"ROCAUC_TEST_cal\"\n",
    "]\n",
    "tbl = df_iso[cols_keep].copy()\n",
    "\n",
    "# 2) Add VAL/TEST deltas (calibrated - raw)\n",
    "tbl[\"ΔBrier_VAL\"] = tbl[\"Brier_VAL_cal\"] - tbl[\"Brier_VAL_raw\"]          # negative is better\n",
    "tbl[\"ΔPR_AUC_VAL\"] = tbl[\"PRAUC_VAL_cal\"] - tbl[\"PRAUC_VAL_raw\"]\n",
    "tbl[\"ΔROC_AUC_VAL\"] = tbl[\"ROCAUC_VAL_cal\"] - tbl[\"ROCAUC_VAL_raw\"]\n",
    "\n",
    "tbl[\"ΔBrier_TEST\"] = tbl[\"Brier_TEST_cal\"] - tbl[\"Brier_TEST_raw\"]        # negative is better\n",
    "tbl[\"ΔPR_AUC_TEST\"] = tbl[\"PRAUC_TEST_cal\"] - tbl[\"PRAUC_TEST_raw\"]\n",
    "tbl[\"ΔROC_AUC_TEST\"] = tbl[\"ROCAUC_TEST_cal\"] - tbl[\"ROCAUC_TEST_raw\"]\n",
    "\n",
    "# 3) Decision-curve utility summary: area under net benefit (AUNB) on TEST\n",
    "def aunb_for(model, calibrated=\"raw\", thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = t_grid\n",
    "    p = get_probs(model, split=\"test\", calibrated=calibrated)\n",
    "    if p is None:\n",
    "        return np.nan\n",
    "    t, nb, _nb_all = decision_curve(yte, p, thresholds)\n",
    "    # simple trapezoidal area over threshold range\n",
    "    return float(np.trapz(nb, t))\n",
    "\n",
    "tbl[\"AUNB_raw\"] = tbl[\"Model\"].apply(lambda m: aunb_for(m, \"raw\", t_grid))\n",
    "tbl[\"AUNB_iso\"] = tbl[\"Model\"].apply(lambda m: aunb_for(m, \"iso\", t_grid))\n",
    "tbl[\"ΔAUNB\"]    = tbl[\"AUNB_iso\"] - tbl[\"AUNB_raw\"]\n",
    "\n",
    "# 4) Nicely formatted view\n",
    "view_cols = [\n",
    "    \"Model\",\n",
    "    # TEST metrics first (often primary)\n",
    "    \"Brier_TEST_raw\",\"Brier_TEST_cal\",\"ΔBrier_TEST\",\n",
    "    \"PRAUC_TEST_raw\",\"PRAUC_TEST_cal\",\"ΔPR_AUC_TEST\",\n",
    "    \"ROCAUC_TEST_raw\",\"ROCAUC_TEST_cal\",\"ΔROC_AUC_TEST\",\n",
    "    # VAL (diagnostic)\n",
    "    \"Brier_VAL_raw\",\"Brier_VAL_cal\",\"ΔBrier_VAL\",\n",
    "    \"PRAUC_VAL_raw\",\"PRAUC_VAL_cal\",\"ΔPR_AUC_VAL\",\n",
    "    \"ROCAUC_VAL_raw\",\"ROCAUC_VAL_cal\",\"ΔROC_AUC_VAL\",\n",
    "    # DCA summary\n",
    "    \"AUNB_raw\",\"AUNB_iso\",\"ΔAUNB\"\n",
    "]\n",
    "tbl_view = (tbl[view_cols]\n",
    "            .sort_values([\"ΔBrier_TEST\",\"ΔAUNB\"], ascending=[True, False])  # best: lower ΔBrier, higher ΔAUNB\n",
    "            .reset_index(drop=True)\n",
    "            .round(4))\n",
    "\n",
    "display(tbl_view)\n",
    "\n",
    "# Optional: quick legend for interpretation\n",
    "print(\"Notes: Δ values are (calibrated − raw). For Brier and ΔBrier, more negative = better; for AUCs and ΔAUNB, more positive = better.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fee8fe",
   "metadata": {},
   "source": [
    "### 3. Compact forest-style plot for calibration deltas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb1d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute deltas (calibrated - raw), flipping Brier so that positive = improvement\n",
    "delta_rows = []\n",
    "for _, row in df_iso.iterrows():\n",
    "    model = row[\"Model\"]\n",
    "    # flip Brier: improvement = raw - cal\n",
    "    d_brier = row[\"Brier_TEST_raw\"] - row[\"Brier_TEST_cal\"]\n",
    "    d_prauc = row[\"PRAUC_TEST_cal\"] - row[\"PRAUC_TEST_raw\"]\n",
    "    d_roc   = row[\"ROCAUC_TEST_cal\"] - row[\"ROCAUC_TEST_raw\"]\n",
    "    delta_rows.append({\n",
    "        \"Model\": model,\n",
    "        \"Delta_Brier\": d_brier,\n",
    "        \"Delta_PRAUC\": d_prauc,\n",
    "        \"Delta_ROCAUC\": d_roc\n",
    "    })\n",
    "\n",
    "df_delta = pd.DataFrame(delta_rows)\n",
    "\n",
    "# --- Plot forest-style (all positive = improvement) ---\n",
    "fig, ax = plt.subplots(figsize=(7.5, 4.5))\n",
    "metrics = [\"Delta_Brier\", \"Delta_PRAUC\", \"Delta_ROCAUC\"]\n",
    "colors = {\"Delta_Brier\": \"tab:blue\", \"Delta_PRAUC\": \"tab:green\", \"Delta_ROCAUC\": \"tab:red\"}\n",
    "\n",
    "y = np.arange(len(df_delta))\n",
    "bar_height = 0.2\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax.barh(\n",
    "        y + (i-1)*bar_height,\n",
    "        df_delta[metric],\n",
    "        height=bar_height,\n",
    "        color=colors[metric],\n",
    "        alpha=0.8,\n",
    "        label=metric\n",
    "    )\n",
    "\n",
    "ax.axvline(0, color=\"gray\", lw=1, ls=\"--\")\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(df_delta[\"Model\"])\n",
    "ax.set_xlabel(\"Delta (improvement after calibration)\")\n",
    "ax.set_title(\"Calibration impact on metrics (Test set)\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(df_delta.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a850bd",
   "metadata": {},
   "source": [
    "### 4. Re-tune thresholds on calibrated (isotonic) probabilities and compare against raw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f2ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Helpers: thresholds on validation, eval on test ---\n",
    "def _align_pr(y, p):\n",
    "    P, R, T = precision_recall_curve(y, p)\n",
    "    return P[:-1], R[:-1], T  # align thresholds (T has length n-1)\n",
    "\n",
    "def thr_f1_from_val(y, p, beta=1.0):\n",
    "    P, R, T = _align_pr(y, p)\n",
    "    F = (1+beta**2)*(P*R) / (beta**2*P + R + 1e-12)\n",
    "    i = int(np.nanargmax(F)) if len(F) else 0\n",
    "    return float(T[i]) if len(T) else 0.5\n",
    "\n",
    "def thr_recall_from_val(y, p, recall_floor=0.10):\n",
    "    P, R, T = _align_pr(y, p)\n",
    "    ok = np.where(R >= recall_floor)[0]\n",
    "    if ok.size:\n",
    "        j = ok[np.argmax(P[ok])]  # highest precision subject to recall >= floor\n",
    "        return float(T[j])\n",
    "    return float(T[-1]) if len(T) else 0.0\n",
    "\n",
    "def eval_at(y, p, t):\n",
    "    pred = (p >= t).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "    precision = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
    "    recall    = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "    balacc    = balanced_accuracy_score(y, pred)\n",
    "    return precision, recall, balacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4c7be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) Re-tune thresholds on VAL (raw vs iso), report on TEST ---\n",
    "rows = []\n",
    "models = sorted(test_probs.keys())\n",
    "for m in models:\n",
    "    for cal in (\"raw\", \"iso\"):\n",
    "        pv = get_probs(m, split=\"val\",  calibrated=cal)\n",
    "        pt = get_probs(m, split=\"test\", calibrated=cal)\n",
    "        if pv is None or pt is None:\n",
    "            continue\n",
    "\n",
    "        t_f1  = thr_f1_from_val(yva, pv)\n",
    "        t_r10 = thr_recall_from_val(yva, pv, recall_floor=0.10)\n",
    "\n",
    "        p_f1, r_f1, b_f1   = eval_at(yte, pt, t_f1)\n",
    "        p_r10, r_r10, b_r10 = eval_at(yte, pt, t_r10)\n",
    "\n",
    "        rows.append({\n",
    "            \"Model\": m, \"Calib\": cal,\n",
    "            \"thr_F1\": t_f1, \"Precision@F1\": p_f1, \"Recall@F1\": r_f1, \"BalancedAcc@F1\": b_f1,\n",
    "            \"thr_Rec>=10%\": t_r10, \"Precision@Rec>=10%\": p_r10, \"Recall@Rec>=10%\": r_r10, \"BalancedAcc@Rec>=10%\": b_r10,\n",
    "            \"PR_AUC_test\": average_precision_score(yte, pt),\n",
    "        })\n",
    "\n",
    "df_thr = (pd.DataFrame(rows)\n",
    "          .sort_values([\"Model\",\"Calib\"])\n",
    "          .reset_index(drop=True))\n",
    "\n",
    "display(df_thr.round({\n",
    "    \"thr_F1\":4, \"Precision@F1\":3, \"Recall@F1\":3, \"BalancedAcc@F1\":3,\n",
    "    \"thr_Rec>=10%\":4, \"Precision@Rec>=10%\":3, \"Recall@Rec>=10%\":3, \"BalancedAcc@Rec>=10%\":3,\n",
    "    \"PR_AUC_test\":3\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a626ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) Compact diffs (iso - raw) at both operating points ---\n",
    "def _pivot_diff(df, metric_col):\n",
    "    wide = (df.pivot(index=\"Model\", columns=\"Calib\", values=metric_col)\n",
    "              .reindex(columns=[\"raw\",\"iso\"]))\n",
    "    if {\"raw\",\"iso\"}.issubset(wide.columns):\n",
    "        return (wide[\"iso\"] - wide[\"raw\"]).rename(metric_col + \" Δ(iso-raw)\")\n",
    "    return pd.Series(dtype=float, name=metric_col + \" Δ(iso-raw)\")\n",
    "\n",
    "diff_cols = [\n",
    "    \"Precision@F1\",\"Recall@F1\",\"BalancedAcc@F1\",\n",
    "    \"Precision@Rec>=10%\",\"Recall@Rec>=10%\",\"BalancedAcc@Rec>=10%\",\n",
    "    \"PR_AUC_test\"\n",
    "]\n",
    "diff_table = pd.concat([_pivot_diff(df_thr, c) for c in diff_cols], axis=1)\n",
    "display(diff_table.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3adaec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PR-AUC before vs after (paired bars with connector lines) ===\n",
    "df_pairs = (df_iso[[\"Model\",\"PRAUC_TEST_raw\",\"PRAUC_TEST_cal\"]]\n",
    "            .rename(columns={\"PRAUC_TEST_raw\":\"raw\",\"PRAUC_TEST_cal\":\"iso\"}))\n",
    "df_pairs[\"delta\"] = df_pairs[\"iso\"] - df_pairs[\"raw\"]\n",
    "df_pairs = df_pairs.sort_values(\"delta\")  # sort by change\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.8, 4.8))\n",
    "y = np.arange(len(df_pairs))\n",
    "ax.barh(y-0.15, df_pairs[\"raw\"], height=0.28, label=\"raw\")\n",
    "ax.barh(y+0.15, df_pairs[\"iso\"], height=0.28, label=\"iso\")\n",
    "\n",
    "# connector lines\n",
    "for i, (_, r) in enumerate(df_pairs.iterrows()):\n",
    "    ax.plot([r[\"raw\"], r[\"iso\"]], [y[i]-0.15, y[i]+0.15], color=\"gray\", lw=1)\n",
    "\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(df_pairs[\"Model\"])\n",
    "ax.set_xlabel(\"PR-AUC (test)\")\n",
    "ax.set_title(\"PR-AUC before vs after isotonic calibration\")\n",
    "ax.legend()\n",
    "ax.grid(True, axis=\"x\", alpha=0.3)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4816113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Delta-only forest (iso - raw), annotated ===\n",
    "df_delta_prau = df_pairs[[\"Model\",\"delta\"]].copy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "y = np.arange(len(df_delta_prau))\n",
    "ax.barh(y, df_delta_prau[\"delta\"], height=0.5, color=np.where(df_delta_prau[\"delta\"]>=0, \"tab:blue\", \"tab:red\"))\n",
    "ax.axvline(0, color=\"gray\", lw=1, ls=\"--\")\n",
    "for i, v in enumerate(df_delta_prau[\"delta\"]):\n",
    "    ax.text(v + (0.00003 if v>=0 else -0.00003), i, f\"{v:+.3f}\",\n",
    "            va=\"center\", ha=\"left\" if v>=0 else \"right\", fontsize=9)\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(df_delta_prau[\"Model\"])\n",
    "ax.set_xlabel(\"Delta PR-AUC (iso - raw)\")\n",
    "ax.set_title(\"Change in PR-AUC after isotonic calibration\")\n",
    "ax.grid(True, axis=\"x\", alpha=0.3)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4494451c",
   "metadata": {},
   "source": [
    "### 5. Cost-sensitive sweep: RAW vs ISOTONIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e12d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses your existing get_probs(model, split, calibrated) helper.\n",
    "\n",
    "BASELINE  = \"Logistic\"   # change if needed\n",
    "MAX_RATIO = 50\n",
    "STEP      = 1\n",
    "K_STABLE  = 3\n",
    "ratios    = list(range(1, MAX_RATIO+1, STEP))\n",
    "models    = sorted(test_probs.keys())\n",
    "\n",
    "def best_cost_for_ratio(y_true, p, ratio):\n",
    "    # search thresholds from PR curve + extremes\n",
    "    prec, rec, thr = precision_recall_curve(y_true, p)\n",
    "    thr_grid = np.unique(np.r_[thr, 0.0, 1.0])\n",
    "    best = None\n",
    "    for t in thr_grid:\n",
    "        pred = (p >= t).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, pred).ravel()\n",
    "        cost = fp*1 + fn*ratio\n",
    "        if (best is None) or (cost < best[\"Cost\"]):\n",
    "            best = {\"thr\": float(t), \"Cost\": int(cost),\n",
    "                    \"Precision\": tp/(tp+fp) if (tp+fp)>0 else 0.0,\n",
    "                    \"Recall\":    tp/(tp+fn) if (tp+fn)>0 else 0.0,\n",
    "                    \"BalancedAcc\": balanced_accuracy_score(y_true, pred),\n",
    "                    \"PR_AUC\": average_precision_score(y_true, p),\n",
    "                    \"ROC_AUC\": roc_auc_score(y_true, p),\n",
    "                    \"TP\": int(tp), \"FP\": int(fp), \"TN\": int(tn), \"FN\": int(fn)}\n",
    "    return best\n",
    "\n",
    "def sweep_cost(curve_source=\"raw\"):\n",
    "    rows = []\n",
    "    for m in models:\n",
    "        p_val  = get_probs(m, split=\"val\",  calibrated=curve_source)\n",
    "        p_test = get_probs(m, split=\"test\", calibrated=curve_source)\n",
    "        if p_val is None or p_test is None: \n",
    "            continue\n",
    "        for r in ratios:\n",
    "            # tune threshold on VAL under cost ratio r\n",
    "            best_val = best_cost_for_ratio(yva, p_val, r)\n",
    "            # evaluate on TEST with that threshold\n",
    "            t = best_val[\"thr\"]\n",
    "            res = best_cost_for_ratio(yte, p_test, r)\n",
    "            res.update({\"Model\": m, \"Ratio\": r, \"thr\": t})\n",
    "            rows.append(res)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df_cost_raw = sweep_cost(\"raw\")\n",
    "df_cost_iso = sweep_cost(\"iso\")\n",
    "\n",
    "def per_ratio_winners(df):\n",
    "    return (df.loc[df.groupby(\"Ratio\")[\"Cost\"].idxmin(), \n",
    "                   [\"Ratio\",\"Model\",\"Cost\",\"thr\",\"Precision\",\"Recall\",\"BalancedAcc\"]]\n",
    "              .reset_index(drop=True))\n",
    "\n",
    "print(\"Per-ratio winners — RAW\")\n",
    "display(per_ratio_winners(df_cost_raw))\n",
    "print(\"Per-ratio winners — ISOTONIC\")\n",
    "display(per_ratio_winners(df_cost_iso))\n",
    "\n",
    "# ---- stable crossover vs baseline (same logic as earlier) ----\n",
    "def crossovers_vs_baseline(df_cost, baseline, ratios, k=K_STABLE, max_ratio=MAX_RATIO):\n",
    "    if baseline not in df_cost[\"Model\"].unique():\n",
    "        return pd.DataFrame(columns=[\"Model\",\"Crossover_at_ratio\"])\n",
    "    base = (df_cost[df_cost.Model==baseline].set_index(\"Ratio\")[\"Cost\"].reindex(ratios))\n",
    "    out = []\n",
    "    for name in df_cost[\"Model\"].unique():\n",
    "        if name == baseline: \n",
    "            continue\n",
    "        other = (df_cost[df_cost.Model==name].set_index(\"Ratio\")[\"Cost\"].reindex(ratios))\n",
    "        diff = (other - base).to_numpy()  # negative => other cheaper\n",
    "\n",
    "        # 1) stable: first window of k consecutive cheaper points\n",
    "        stable_idx = None\n",
    "        if len(diff) >= k:\n",
    "            run = np.convolve((diff < 0).astype(int), np.ones(k, dtype=int), mode=\"valid\")\n",
    "            hits = np.where(run == k)[0]\n",
    "            if hits.size: stable_idx = hits[0]\n",
    "        if stable_idx is not None:\n",
    "            out.append({\"Model\": name, \"Crossover_at_ratio\": f\"{ratios[stable_idx]}:1\"}); continue\n",
    "\n",
    "        # 2) any sign flip\n",
    "        sign = np.sign(diff)\n",
    "        flip = np.where(sign[:-1]*sign[1:] < 0)[0]\n",
    "        if flip.size:\n",
    "            out.append({\"Model\": name, \"Crossover_at_ratio\": f\"{ratios[flip[0]+1]}:1\"}); continue\n",
    "\n",
    "        # 3) dominance\n",
    "        if np.all(diff < 0):\n",
    "            out.append({\"Model\": name, \"Crossover_at_ratio\": f\"always cheaper (1–{max_ratio}:1)\"})\n",
    "        elif np.all(diff > 0):\n",
    "            out.append({\"Model\": name, \"Crossover_at_ratio\": f\"always costlier (1–{max_ratio}:1)\"})\n",
    "        else:\n",
    "            out.append({\"Model\": name, \"Crossover_at_ratio\": \"no clean crossover\"})\n",
    "    return pd.DataFrame(out).sort_values(\"Model\")\n",
    "\n",
    "print(\"\\nStable crossovers vs baseline (RAW):\", BASELINE)\n",
    "display(crossovers_vs_baseline(df_cost_raw, BASELINE, ratios))\n",
    "print(\"\\nStable crossovers vs baseline (ISOTONIC):\", BASELINE)\n",
    "display(crossovers_vs_baseline(df_cost_iso, BASELINE, ratios))\n",
    "\n",
    "# ---- plot cost curves (optional, compact) ----\n",
    "def plot_cost_curves(df, title):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    for m in models:\n",
    "        sub = df[df[\"Model\"]==m]\n",
    "        if not sub.empty:\n",
    "            plt.plot(sub[\"Ratio\"], sub[\"Cost\"], label=m, lw=1.6)\n",
    "    plt.xlabel(\"FN:FP cost ratio\")\n",
    "    plt.ylabel(\"Min expected cost (test)\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.35); plt.legend(ncol=2, fontsize=9)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "plot_cost_curves(df_cost_raw, \"Cost vs FN:FP (RAW probs)\")\n",
    "plot_cost_curves(df_cost_iso, \"Cost vs FN:FP (ISOTONIC probs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576849f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Visual Summary: Calibration + Cost-Sensitive Sweeps ----\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,5))\n",
    "\n",
    "# (1) Calibration impact (Δ metrics)\n",
    "metrics = [\"Delta_Brier\", \"Delta_PRAUC\", \"Delta_ROCAUC\"]\n",
    "colors  = {\"Delta_Brier\":\"tab:blue\", \"Delta_PRAUC\":\"tab:green\", \"Delta_ROCAUC\":\"tab:red\"}\n",
    "y = np.arange(len(df_delta))\n",
    "bar_height = 0.22\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[0].barh(y + (i-1)*bar_height, df_delta[metric], \n",
    "                 height=bar_height, color=colors[metric], alpha=0.8, label=metric)\n",
    "\n",
    "axes[0].axvline(0, color=\"gray\", lw=1, ls=\"--\")\n",
    "axes[0].set_yticks(y)\n",
    "axes[0].set_yticklabels(df_delta[\"Model\"])\n",
    "axes[0].set_xlabel(\"Δ (calibrated - raw)\")\n",
    "axes[0].set_title(\"Calibration impact on test metrics\")\n",
    "axes[0].legend()\n",
    "\n",
    "# (2) Cost-sensitive sweep (winner per ratio, raw vs isotonic)\n",
    "# Winners (RAW)\n",
    "winners_raw = (\n",
    "    df_cost_raw.loc[df_cost_raw.groupby(\"Ratio\")[\"Cost\"].idxmin()]\n",
    "    [[\"Ratio\",\"Model\",\"Cost\",\"thr\",\"Precision\",\"Recall\",\"BalancedAcc\"]]\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Winners (ISOTONIC)\n",
    "winners_iso = (\n",
    "    df_cost_iso.loc[df_cost_iso.groupby(\"Ratio\")[\"Cost\"].idxmin()]\n",
    "    [[\"Ratio\",\"Model\",\"Cost\",\"thr\",\"Precision\",\"Recall\",\"BalancedAcc\"]]\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "for label, df_winners, style in [\n",
    "    (\"Raw\", winners_raw, \"-\"),\n",
    "    (\"Isotonic\", winners_iso, \"--\")\n",
    "]:\n",
    "    for model in df_winners[\"Model\"].unique():\n",
    "        sub = df_winners[df_winners[\"Model\"]==model]\n",
    "        axes[1].plot(sub[\"Ratio\"], sub[\"Cost\"], style, lw=2,\n",
    "                     label=f\"{model} ({label})\")\n",
    "\n",
    "axes[1].set_xlabel(\"FN:FP cost ratio\")\n",
    "axes[1].set_ylabel(\"Minimum expected cost\")\n",
    "axes[1].set_title(\"Cost vs FN:FP ratio (Test set)\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULT_DIR / \"calibration_cost_panel.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123e49c6",
   "metadata": {},
   "source": [
    "### Further experiments:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf02dbf",
   "metadata": {},
   "source": [
    "#### Experiment A: Crossover-shift analysis (RAW vs ISOTONIC) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbfccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_int_or_nan(txt):\n",
    "    if isinstance(txt, (int, float)): \n",
    "        return int(txt)\n",
    "    if isinstance(txt, str) and \":\" in txt and txt.split(\":\")[0].isdigit():\n",
    "        return int(txt.split(\":\")[0])\n",
    "    return np.nan\n",
    "\n",
    "# 1) Crossover tables vs Logistic baseline\n",
    "xover_raw = crossovers_vs_baseline(df_cost_raw, BASELINE, ratios)\n",
    "xover_iso = crossovers_vs_baseline(df_cost_iso, BASELINE, ratios)\n",
    "\n",
    "tab = (xover_raw.rename(columns={\"Crossover_at_ratio\":\"Raw_crossover\"})\n",
    "       .merge(xover_iso.rename(columns={\"Crossover_at_ratio\":\"Iso_crossover\"}),\n",
    "              on=\"Model\", how=\"outer\"))\n",
    "tab[\"Raw_ratio\"] = tab[\"Raw_crossover\"].apply(_to_int_or_nan)\n",
    "tab[\"Iso_ratio\"] = tab[\"Iso_crossover\"].apply(_to_int_or_nan)\n",
    "tab[\"Shift_in_ratio\"] = tab[\"Iso_ratio\"] - tab[\"Raw_ratio\"]\n",
    "\n",
    "display(tab[[\"Model\",\"Raw_crossover\",\"Iso_crossover\",\"Shift_in_ratio\"]]\n",
    "        .sort_values(\"Model\"))\n",
    "\n",
    "# 2) Winner-share summaries\n",
    "def share_wins(df_winners):\n",
    "    counts = df_winners[\"Model\"].value_counts().rename_axis(\"Model\").reset_index(name=\"Wins\")\n",
    "    counts[\"Share\"] = counts[\"Wins\"] / len(df_winners)\n",
    "    return counts.sort_values(\"Share\", ascending=False)\n",
    "\n",
    "print(\"\\nWinner share by ratio — RAW\")\n",
    "display(share_wins(winners_raw))\n",
    "print(\"\\nWinner share by ratio — ISOTONIC\")\n",
    "display(share_wins(winners_iso))\n",
    "\n",
    "# 3) Visualize crossover shifts\n",
    "_shift = tab[~tab[\"Shift_in_ratio\"].isna()].copy()\n",
    "_shift = _shift.sort_values(\"Shift_in_ratio\")\n",
    "\n",
    "plt.figure(figsize=(6.8, 3.8))\n",
    "y = np.arange(len(_shift))\n",
    "plt.barh(y, _shift[\"Shift_in_ratio\"], height=0.5)\n",
    "plt.axvline(0, color=\"gray\", lw=1, ls=\"--\")\n",
    "plt.yticks(y, _shift[\"Model\"])\n",
    "plt.xlabel(\"Shift in crossover ratio (iso - raw)\")\n",
    "plt.title(\"Crossover shift vs Logistic (negative = earlier after isotonic)\")\n",
    "plt.tight_layout()\n",
    "savefig(\"03_crossover_shifts.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d69050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Experiment B — Robustness under resampling (RAW vs ISOTONIC)\n",
    "# =========================\n",
    "# Goal:\n",
    "# - Fix thresholds on VALIDATION for each (model, ratio) and for each calibration mode (raw/iso)\n",
    "# - Bootstrap TEST indices, evaluate cost using FIXED thresholds (no leakage)\n",
    "# - Summaries: winner share by ratio, crossover distributions vs Logistic, median cost curves + 95% CI\n",
    "\n",
    "# ---------- config ----------\n",
    "BASELINE  = \"Logistic\"\n",
    "RATIOS    = list(range(1, 51))       # 1..50\n",
    "K_STABLE  = 3                        # consecutive cheaper points required for crossover\n",
    "B_LIST    = [200, 500, 1000]         # bootstraps to run; you can subset\n",
    "RNG       = np.random.default_rng(42)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def best_val_threshold(y_val, p_val, ratio):\n",
    "    \"\"\"Pick the threshold that minimizes cost on VALIDATION for a given ratio (FN cost=ratio, FP cost=1).\"\"\"\n",
    "    P, R, T = precision_recall_curve(y_val, p_val)\n",
    "    thr_grid = np.unique(np.r_[T, 0.0, 1.0])\n",
    "    best_t, best_cost = None, None\n",
    "    for t in thr_grid:\n",
    "        pred = (p_val >= t).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_val, pred).ravel()\n",
    "        cost = fp*1 + fn*ratio\n",
    "        if (best_cost is None) or (cost < best_cost):\n",
    "            best_cost, best_t = cost, float(t)\n",
    "    return best_t\n",
    "\n",
    "def eval_cost_at_threshold(y, p, t, ratio):\n",
    "    pred = (p >= t).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "    return int(fp*1 + fn*ratio)\n",
    "\n",
    "def crossovers_vs_baseline_from_costdf(df_cost, baseline, ratios, k=K_STABLE, max_ratio=None):\n",
    "    \"\"\"df_cost: rows (Model, Ratio, Cost). Return crossover table vs baseline.\"\"\"\n",
    "    if max_ratio is None: max_ratio = max(ratios)\n",
    "    if baseline not in df_cost[\"Model\"].unique():\n",
    "        return pd.DataFrame(columns=[\"Model\",\"Crossover_at_ratio\"])\n",
    "    base = (df_cost[df_cost.Model==baseline].set_index(\"Ratio\")[\"Cost\"].reindex(ratios))\n",
    "    out = []\n",
    "    for name in df_cost[\"Model\"].unique():\n",
    "        if name == baseline: \n",
    "            continue\n",
    "        other = (df_cost[df_cost.Model==name].set_index(\"Ratio\")[\"Cost\"].reindex(ratios))\n",
    "        diff = (other - base).to_numpy()  # negative => other cheaper\n",
    "\n",
    "        # Stable crossover: first window of k consecutive cheaper points\n",
    "        stable_idx = None\n",
    "        if len(diff) >= k:\n",
    "            run = np.convolve((diff < 0).astype(int), np.ones(k, dtype=int), mode=\"valid\")\n",
    "            hits = np.where(run == k)[0]\n",
    "            if hits.size: stable_idx = hits[0]\n",
    "        if stable_idx is not None:\n",
    "            out.append({\"Model\": name, \"Crossover_at_ratio\": ratios[stable_idx]})\n",
    "            continue\n",
    "\n",
    "        # Any sign flip\n",
    "        sign = np.sign(diff)\n",
    "        flip = np.where(sign[:-1]*sign[1:] < 0)[0]\n",
    "        if flip.size:\n",
    "            out.append({\"Model\": name, \"Crossover_at_ratio\": ratios[flip[0]+1]})\n",
    "            continue\n",
    "\n",
    "        # Dominance\n",
    "        if np.all(diff < 0):\n",
    "            out.append({\"Model\": name, \"Crossover_at_ratio\": f\"always cheaper (1-{max_ratio})\"})\n",
    "        elif np.all(diff > 0):\n",
    "            out.append({\"Model\": name, \"Crossover_at_ratio\": f\"always costlier (1-{max_ratio})\"})\n",
    "        else:\n",
    "            out.append({\"Model\": name, \"Crossover_at_ratio\": \"no clean crossover\"})\n",
    "    return pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa7a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 1) Precompute VALIDATION thresholds for RAW and ISOTONIC ----------\n",
    "models = sorted(test_probs.keys())  # keep consistent scope\n",
    "thr_val_raw = {m:{} for m in models}\n",
    "thr_val_iso = {m:{} for m in models}\n",
    "\n",
    "for m in models:\n",
    "    p_val_raw = np.asarray(val_probs.get(m),  dtype=float)\n",
    "    p_val_iso = np.asarray(cal_val_probs.get(m), dtype=float) if 'cal_val_probs' in globals() else None\n",
    "    for r in RATIOS:\n",
    "        if p_val_raw is not None:\n",
    "            thr_val_raw[m][r] = best_val_threshold(yva, p_val_raw, r)\n",
    "        if p_val_iso is not None:\n",
    "            thr_val_iso[m][r] = best_val_threshold(yva, p_val_iso, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607af7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 2) A function to run one bootstrap experiment ----------\n",
    "def run_bootstrap(B=200, calibrated=\"raw\"):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - df_cost_boot: long df with columns [b, Model, Ratio, Cost]\n",
    "      - winners: per-b replicate winners table [b, Ratio, Model]\n",
    "      - xovers: list of crossover tables per bootstrap (one DataFrame per b)\n",
    "    \"\"\"\n",
    "    df_rows = []\n",
    "    winners_rows = []\n",
    "    xovers = []\n",
    "\n",
    "    # choose dicts by calibrated mode\n",
    "    test_dict = cal_test_probs if calibrated==\"iso\" else test_probs\n",
    "    thr_dict  = thr_val_iso   if calibrated==\"iso\" else thr_val_raw\n",
    "\n",
    "    p_test_all = {m: np.asarray(test_dict[m], float) for m in models if m in test_dict}\n",
    "    n = len(yte)\n",
    "    idx_mat = RNG.integers(0, n, size=(B, n))\n",
    "\n",
    "    for b in range(B):\n",
    "        idx = idx_mat[b]\n",
    "        yb = yte[idx]\n",
    "        # per-ratio costs per model\n",
    "        for m in models:\n",
    "            if m not in p_test_all: \n",
    "                continue\n",
    "            pb = p_test_all[m][idx]\n",
    "            for r in RATIOS:\n",
    "                t_val = thr_dict.get(m, {}).get(r, None)\n",
    "                if t_val is None:\n",
    "                    continue\n",
    "                c = eval_cost_at_threshold(yb, pb, t_val, r)\n",
    "                df_rows.append({\"b\":b, \"Model\":m, \"Ratio\":r, \"Cost\":c})\n",
    "\n",
    "        # winners this bootstrap\n",
    "        df_b = pd.DataFrame([row for row in df_rows if row[\"b\"]==b])\n",
    "        if not df_b.empty:\n",
    "            win_b = (df_b.loc[df_b.groupby(\"Ratio\")[\"Cost\"].idxmin(), [\"Ratio\",\"Model\"]]\n",
    "                          .assign(b=b))\n",
    "            winners_rows.extend(win_b.to_dict(\"records\"))\n",
    "\n",
    "            # crossover vs baseline for this bootstrap\n",
    "            xover_b = crossovers_vs_baseline_from_costdf(df_b, BASELINE, RATIOS, k=K_STABLE, max_ratio=max(RATIOS))\n",
    "            xover_b[\"b\"] = b\n",
    "            xovers.append(xover_b)\n",
    "\n",
    "    df_cost_boot = pd.DataFrame(df_rows)\n",
    "    winners = pd.DataFrame(winners_rows)\n",
    "    return df_cost_boot, winners, xovers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d2e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 3) Run for each B in B_LIST, for RAW and ISOTONIC ----------\n",
    "results_B = {}\n",
    "for B in B_LIST:\n",
    "    print(f\"[Experiment B] Running bootstraps: B={B}\")\n",
    "    df_cost_raw_B, winners_raw_B, xovers_raw_B = run_bootstrap(B=B, calibrated=\"raw\")\n",
    "    df_cost_iso_B, winners_iso_B, xovers_iso_B = run_bootstrap(B=B, calibrated=\"iso\")\n",
    "    results_B[B] = dict(\n",
    "        cost_raw=df_cost_raw_B,\n",
    "        cost_iso=df_cost_iso_B,\n",
    "        winners_raw=winners_raw_B,\n",
    "        winners_iso=winners_iso_B,\n",
    "        xovers_raw=xovers_raw_B,\n",
    "        xovers_iso=xovers_iso_B\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 4) Summaries per B ----------\n",
    "summaries = {}\n",
    "for B, pack in results_B.items():\n",
    "    # winner share by ratio (average over bootstraps)\n",
    "    def winner_share(winners_df):\n",
    "        tbl = (winners_df\n",
    "               .groupby([\"Ratio\",\"Model\"]).size()\n",
    "               .rename(\"Wins\").reset_index())\n",
    "        tbl[\"Share\"] = tbl[\"Wins\"] / winners_df[\"b\"].nunique()\n",
    "        return tbl.sort_values([\"Ratio\",\"Share\"], ascending=[True, False])\n",
    "\n",
    "    share_raw = winner_share(pack[\"winners_raw\"])\n",
    "    share_iso = winner_share(pack[\"winners_iso\"])\n",
    "\n",
    "    # median cost curves + 95% CI per model\n",
    "    def cost_curve_ci(df_cost):\n",
    "        rows = []\n",
    "        for m in models:\n",
    "            for r in RATIOS:\n",
    "                sub = df_cost[(df_cost[\"Model\"]==m) & (df_cost[\"Ratio\"]==r)]\n",
    "                if sub.empty:\n",
    "                    continue\n",
    "                med = sub[\"Cost\"].median()\n",
    "                lo, hi = np.percentile(sub[\"Cost\"], [2.5, 97.5])\n",
    "                rows.append({\"Model\":m, \"Ratio\":r, \"Cost_median\":med, \"Cost_lo\":lo, \"Cost_hi\":hi})\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    curve_raw = cost_curve_ci(pack[\"cost_raw\"])\n",
    "    curve_iso = cost_curve_ci(pack[\"cost_iso\"])\n",
    "\n",
    "    # crossover distributions vs baseline\n",
    "    def xover_summary(xovers_list):\n",
    "        # gather first numeric crossover only (ignore strings like \"no clean\")\n",
    "        rows = []\n",
    "        for df in xovers_list:\n",
    "            b = df.get(\"b\", pd.Series([np.nan])).iloc[0] if isinstance(df, pd.DataFrame) and not df.empty else np.nan\n",
    "            for _, r in df.iterrows():\n",
    "                val = r[\"Crossover_at_ratio\"]\n",
    "                if isinstance(val, (int, float, np.integer, np.floating)):\n",
    "                    rows.append({\"b\": b, \"Model\": r[\"Model\"], \"Crossover\": int(val)})\n",
    "        if not rows:\n",
    "            return pd.DataFrame(columns=[\"Model\",\"Median\",\"IQR_low\",\"IQR_high\",\"N\"])\n",
    "        d = pd.DataFrame(rows)\n",
    "        out = (d.groupby(\"Model\")[\"Crossover\"]\n",
    "                 .agg(Median=\"median\",\n",
    "                      IQR_low=lambda x: np.percentile(x, 25),\n",
    "                      IQR_high=lambda x: np.percentile(x, 75),\n",
    "                      N=\"count\")\n",
    "                 .reset_index())\n",
    "        return out\n",
    "\n",
    "    xsum_raw = xover_summary(pack[\"xovers_raw\"])\n",
    "    xsum_iso = xover_summary(pack[\"xovers_iso\"])\n",
    "\n",
    "    summaries[B] = dict(\n",
    "        winner_share_raw=share_raw,\n",
    "        winner_share_iso=share_iso,\n",
    "        curve_raw=curve_raw,\n",
    "        curve_iso=curve_iso,\n",
    "        xover_raw=xsum_raw,\n",
    "        xover_iso=xsum_iso\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11428571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 5) Example displays (choose one B to inspect quickly) ----------\n",
    "#B_show = max(B_LIST)  # e.g., 1000\n",
    "B_show = 200\n",
    "s = summaries[B_show]\n",
    "\n",
    "print(f\"\\n[Experiment B] Winner share by ratio — RAW (B={B_show})\")\n",
    "display(s[\"winner_share_raw\"].head(20))\n",
    "print(f\"\\n[Experiment B] Winner share by ratio — ISOTONIC (B={B_show})\")\n",
    "display(s[\"winner_share_iso\"].head(20))\n",
    "\n",
    "print(f\"\\n[Experiment B] Crossover distributions vs {BASELINE} — RAW (B={B_show})\")\n",
    "display(s[\"xover_raw\"])\n",
    "print(f\"\\n[Experiment B] Crossover distributions vs {BASELINE} — ISOTONIC (B={B_show})\")\n",
    "display(s[\"xover_iso\"])\n",
    "\n",
    "print(f\"\\n[Experiment B] Median cost curves with 95% CI — RAW (B={B_show})\")\n",
    "display(s[\"curve_raw\"].head(12))\n",
    "print(f\"\\n[Experiment B] Median cost curves with 95% CI — ISOTONIC (B={B_show})\")\n",
    "display(s[\"curve_iso\"].head(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c236a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_show = 500\n",
    "s = summaries[B_show]\n",
    "\n",
    "print(f\"\\n[Experiment B] Winner share by ratio — RAW (B={B_show})\")\n",
    "display(s[\"winner_share_raw\"].head(20))\n",
    "print(f\"\\n[Experiment B] Winner share by ratio — ISOTONIC (B={B_show})\")\n",
    "display(s[\"winner_share_iso\"].head(20))\n",
    "\n",
    "print(f\"\\n[Experiment B] Crossover distributions vs {BASELINE} — RAW (B={B_show})\")\n",
    "display(s[\"xover_raw\"])\n",
    "print(f\"\\n[Experiment B] Crossover distributions vs {BASELINE} — ISOTONIC (B={B_show})\")\n",
    "display(s[\"xover_iso\"])\n",
    "\n",
    "print(f\"\\n[Experiment B] Median cost curves with 95% CI — RAW (B={B_show})\")\n",
    "display(s[\"curve_raw\"].head(12))\n",
    "print(f\"\\n[Experiment B] Median cost curves with 95% CI — ISOTONIC (B={B_show})\")\n",
    "display(s[\"curve_iso\"].head(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac6bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_show = max(B_LIST)\n",
    "s = summaries[B_show]\n",
    "\n",
    "print(f\"\\n[Experiment B] Winner share by ratio — RAW (B={B_show})\")\n",
    "display(s[\"winner_share_raw\"].head(20))\n",
    "print(f\"\\n[Experiment B] Winner share by ratio — ISOTONIC (B={B_show})\")\n",
    "display(s[\"winner_share_iso\"].head(20))\n",
    "\n",
    "print(f\"\\n[Experiment B] Crossover distributions vs {BASELINE} — RAW (B={B_show})\")\n",
    "display(s[\"xover_raw\"])\n",
    "print(f\"\\n[Experiment B] Crossover distributions vs {BASELINE} — ISOTONIC (B={B_show})\")\n",
    "display(s[\"xover_iso\"])\n",
    "\n",
    "print(f\"\\n[Experiment B] Median cost curves with 95% CI — RAW (B={B_show})\")\n",
    "display(s[\"curve_raw\"].head(12))\n",
    "print(f\"\\n[Experiment B] Median cost curves with 95% CI — ISOTONIC (B={B_show})\")\n",
    "display(s[\"curve_iso\"].head(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9630dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Save everything\n",
    "JOBDIR = ART / \"expB_bootstrap\"\n",
    "JOBDIR.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(results_B,  JOBDIR / \"results_B_all.joblib\")\n",
    "joblib.dump(summaries,  JOBDIR / \"summaries_B_all.joblib\")\n",
    "print(\"Saved Experiment B artifacts to:\", JOBDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f8d484",
   "metadata": {},
   "source": [
    "#### Experiment C — Robustness under prevalence shift (RAW vs ISOTONIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92db7701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Experiment C — Robustness under prevalence shift (RAW vs ISOTONIC)\n",
    "# =========================\n",
    "# Assumes Experiment B code already ran:\n",
    "# - models, RATIOS, BASELINE, K_STABLE, RNG are defined\n",
    "# - get_probs, best_val_threshold, eval_cost_at_threshold, crossovers_vs_baseline_from_costdf exist\n",
    "# - thr_val_raw, thr_val_iso are precomputed validation thresholds per (model, ratio)\n",
    "\n",
    "# ---------- config ----------\n",
    "TARGET_PREVS = [0.01, 0.03, 0.05, 0.10]  # target test prevalences to simulate\n",
    "B_C         = 200                        # bootstraps per prevalence\n",
    "SEED_C      = 123\n",
    "RNG_C       = np.random.default_rng(SEED_C)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def resample_to_prevalence(y, size, target_prev, rng):\n",
    "    \"\"\"Return index array of length 'size' resampled with replacement to match target_prev.\"\"\"\n",
    "    y = np.asarray(y, int)\n",
    "    idx_pos = np.where(y == 1)[0]\n",
    "    idx_neg = np.where(y == 0)[0]\n",
    "    n1 = int(round(target_prev * size))\n",
    "    n0 = size - n1\n",
    "    if len(idx_pos) == 0 or len(idx_neg) == 0:\n",
    "        raise ValueError(\"Cannot resample: one of the classes has zero count.\")\n",
    "    samp_pos = rng.choice(idx_pos, size=max(n1, 0), replace=True)\n",
    "    samp_neg = rng.choice(idx_neg, size=max(n0, 0), replace=True)\n",
    "    return np.r_[samp_pos, samp_neg]\n",
    "\n",
    "def decision_curve(y, p, thresholds):\n",
    "    \"\"\"Net benefit curve (treat if p >= t). Returns t, nb, nb_all.\"\"\"\n",
    "    y = np.asarray(y, int)\n",
    "    p = np.asarray(p, float)\n",
    "    N = len(y); prev = y.mean()\n",
    "    out = []\n",
    "    for t in thresholds:\n",
    "        pred = (p >= t).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "        w = t / (1.0 - t + 1e-12)\n",
    "        nb_model = (tp / N) - (fp / N) * w\n",
    "        nb_all   = prev - (1 - prev) * w\n",
    "        out.append((t, nb_model, nb_all))\n",
    "    out = np.array(out)\n",
    "    return out[:,0], out[:,1], out[:,2]\n",
    "\n",
    "def aunb_for(y, p, thresholds):\n",
    "    \"\"\"Simple trapezoidal AUNB over thresholds.\"\"\"\n",
    "    t, nb, _nb_all = decision_curve(y, p, thresholds)\n",
    "    return float(np.trapz(nb, t))\n",
    "\n",
    "# Pack raw/iso test probabilities once for speed\n",
    "test_raw = {m: np.asarray(get_probs(m, split=\"test\", calibrated=\"raw\"), float) for m in models}\n",
    "test_iso = {m: np.asarray(get_probs(m, split=\"test\", calibrated=\"iso\"), float) for m in models}\n",
    "\n",
    "# ---------- main runner ----------\n",
    "def run_prevalence_scenario(target_prev, B=B_C, calibrated=\"raw\", thresholds_grid=None):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - df_cost_boot: [b, Model, Ratio, Cost]\n",
    "      - winners: [b, Ratio, Model] per bootstrap\n",
    "      - xovers: list of crossover tables per bootstrap\n",
    "      - aunb_summary: per-model AUNB median and 95% CI (optional quick utility summary)\n",
    "    \"\"\"\n",
    "    if thresholds_grid is None:\n",
    "        thresholds_grid = np.linspace(0.01, 0.15, 60)\n",
    "\n",
    "    df_rows = []\n",
    "    winners_rows = []\n",
    "    xovers = []\n",
    "    aunb_rows = []\n",
    "\n",
    "    # choose dicts by mode\n",
    "    p_test_all = test_iso if calibrated == \"iso\" else test_raw\n",
    "    thr_dict   = thr_val_iso if calibrated == \"iso\" else thr_val_raw\n",
    "\n",
    "    n = len(yte)\n",
    "    for b in range(B):\n",
    "        idx = resample_to_prevalence(yte, n, target_prev, RNG_C)\n",
    "        yb  = yte[idx]\n",
    "\n",
    "        # Per-model, per-ratio costs using VALIDATION-fixed thresholds\n",
    "        for m in models:\n",
    "            pb = p_test_all.get(m, None)\n",
    "            if pb is None: \n",
    "                continue\n",
    "            pb = pb[idx]\n",
    "            # Collect AUNB once per bootstrap per model (cheap enough)\n",
    "            try:\n",
    "                aunb_rows.append({\"b\": b, \"Model\": m, \"AUNB\": aunb_for(yb, pb, thresholds_grid)})\n",
    "            except Exception:\n",
    "                pass\n",
    "            for r in RATIOS:\n",
    "                t_val = thr_dict.get(m, {}).get(r, None)\n",
    "                if t_val is None: \n",
    "                    continue\n",
    "                c = eval_cost_at_threshold(yb, pb, t_val, r)\n",
    "                df_rows.append({\"b\": b, \"Model\": m, \"Ratio\": r, \"Cost\": c})\n",
    "\n",
    "        # Winner table and crossover vs baseline for this bootstrap\n",
    "        df_b = pd.DataFrame([row for row in df_rows if row[\"b\"] == b])\n",
    "        if not df_b.empty:\n",
    "            win_b = (df_b.loc[df_b.groupby(\"Ratio\")[\"Cost\"].idxmin(), [\"Ratio\",\"Model\"]]\n",
    "                          .assign(b=b))\n",
    "            winners_rows.extend(win_b.to_dict(\"records\"))\n",
    "            xover_b = crossovers_vs_baseline_from_costdf(df_b, BASELINE, RATIOS, k=K_STABLE, max_ratio=max(RATIOS))\n",
    "            xover_b[\"b\"] = b\n",
    "            xovers.append(xover_b)\n",
    "\n",
    "    df_cost_boot = pd.DataFrame(df_rows)\n",
    "    winners = pd.DataFrame(winners_rows)\n",
    "\n",
    "    # AUNB summary (median and 95% CI per model)\n",
    "    aunb_df = pd.DataFrame(aunb_rows)\n",
    "    if not aunb_df.empty:\n",
    "        aunb_summary = (aunb_df.groupby(\"Model\")[\"AUNB\"]\n",
    "                        .agg(median=\"median\",\n",
    "                             lo=lambda x: np.percentile(x, 2.5),\n",
    "                             hi=lambda x: np.percentile(x, 97.5))\n",
    "                        .reset_index())\n",
    "    else:\n",
    "        aunb_summary = pd.DataFrame(columns=[\"Model\",\"median\",\"lo\",\"hi\"])\n",
    "\n",
    "    return df_cost_boot, winners, xovers, aunb_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ac259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- orchestrate Experiment C ----------\n",
    "EXP_C = {}\n",
    "base_prev = float(np.mean(yte))\n",
    "print(f\"[Experiment C] Base test prevalence: {base_prev:.4f}\")\n",
    "\n",
    "for prev in TARGET_PREVS:\n",
    "    print(f\"[Experiment C] Running prevalence={prev:.2%}  (B={B_C})\")\n",
    "    # RAW\n",
    "    cost_raw, wins_raw, xovers_raw, aunb_raw = run_prevalence_scenario(prev, B=B_C, calibrated=\"raw\")\n",
    "    # ISO\n",
    "    cost_iso, wins_iso, xovers_iso, aunb_iso = run_prevalence_scenario(prev, B=B_C, calibrated=\"iso\")\n",
    "\n",
    "    # Winner share by ratio (averaged over bootstraps)\n",
    "    def winner_share(winners_df):\n",
    "        if winners_df.empty: \n",
    "            return pd.DataFrame(columns=[\"Ratio\",\"Model\",\"Wins\",\"Share\"])\n",
    "        tbl = (winners_df.groupby([\"Ratio\",\"Model\"]).size().rename(\"Wins\").reset_index())\n",
    "        Bn = winners_df[\"b\"].nunique()\n",
    "        tbl[\"Share\"] = tbl[\"Wins\"] / (Bn if Bn > 0 else 1)\n",
    "        return tbl.sort_values([\"Ratio\",\"Share\"], ascending=[True, False])\n",
    "\n",
    "    share_raw = winner_share(wins_raw)\n",
    "    share_iso = winner_share(wins_iso)\n",
    "\n",
    "    # Crossover distributions vs Logistic (numeric only)\n",
    "    def xover_summary(xovers_list):\n",
    "        rows = []\n",
    "        for df in xovers_list:\n",
    "            if isinstance(df, pd.DataFrame) and not df.empty:\n",
    "                for _, r in df.iterrows():\n",
    "                    v = r[\"Crossover_at_ratio\"]\n",
    "                    if isinstance(v, (int, float, np.integer, np.floating)):\n",
    "                        rows.append({\"Model\": r[\"Model\"], \"Crossover\": int(v)})\n",
    "        if not rows:\n",
    "            return pd.DataFrame(columns=[\"Model\",\"Median\",\"IQR_low\",\"IQR_high\",\"N\"])\n",
    "        d = pd.DataFrame(rows)\n",
    "        out = (d.groupby(\"Model\")[\"Crossover\"]\n",
    "                 .agg(Median=\"median\",\n",
    "                      IQR_low=lambda x: np.percentile(x, 25),\n",
    "                      IQR_high=lambda x: np.percentile(x, 75),\n",
    "                      N=\"count\")\n",
    "                 .reset_index())\n",
    "        return out\n",
    "\n",
    "    xsum_raw = xover_summary(xovers_raw)\n",
    "    xsum_iso = xover_summary(xovers_iso)\n",
    "\n",
    "    EXP_C[prev] = dict(\n",
    "        cost_raw=cost_raw, cost_iso=cost_iso,\n",
    "        winners_raw=wins_raw, winners_iso=wins_iso,\n",
    "        share_raw=share_raw, share_iso=share_iso,\n",
    "        xover_raw=xsum_raw, xover_iso=xsum_iso,\n",
    "        aunb_raw=aunb_raw, aunb_iso=aunb_iso\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387e87d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_PREVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8536e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- quick displays for one prevalence (edit as needed) ----------\n",
    "#P_SHOW = TARGET_PREVS[0]  # e.g., 0.01\n",
    "for P_SHOW in TARGET_PREVS:\n",
    "    print(f\"\\n[Experiment C] Summaries for prevalence={P_SHOW:.2%} (B={B_C})\")\n",
    "    pack = EXP_C[P_SHOW]\n",
    "    print(f\"\\n[Experiment C] Prevalence {P_SHOW:.2%} — Winner share by ratio (RAW)\")\n",
    "    display(pack[\"share_raw\"].head(20))\n",
    "    print(f\"\\n[Experiment C] Prevalence {P_SHOW:.2%} — Winner share by ratio (ISOTONIC)\")\n",
    "    display(pack[\"share_iso\"].head(20))\n",
    "\n",
    "    print(f\"\\n[Experiment C] Prevalence {P_SHOW:.2%} — Crossovers vs {BASELINE} (RAW)\")\n",
    "    display(pack[\"xover_raw\"])\n",
    "    print(f\"\\n[Experiment C] Prevalence {P_SHOW:.2%} — Crossovers vs {BASELINE} (ISOTONIC)\")\n",
    "    display(pack[\"xover_iso\"])\n",
    "\n",
    "    print(f\"\\n[Experiment C] Prevalence {P_SHOW:.2%} — AUNB summary (RAW)\")\n",
    "    display(pack[\"aunb_raw\"].round(4))\n",
    "    print(f\"\\n[Experiment C] Prevalence {P_SHOW:.2%} — AUNB summary (ISOTONIC)\")\n",
    "    display(pack[\"aunb_iso\"].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089ffb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save artifacts\n",
    "JOBDIR_C = ART / \"expC_prevalence\"\n",
    "JOBDIR_C.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(EXP_C, JOBDIR_C / \"expC_results.joblib\")\n",
    "print(\"Saved Experiment C artifacts to:\", JOBDIR_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fcad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2D heatmap of ΔAUNB (isotonic − raw), numbers in each cell, no arrows ===\n",
    "from matplotlib import cm\n",
    "def plot_aunb_heatmap(\n",
    "    aunb_raw_dict,\n",
    "    aunb_iso_dict,\n",
    "    title=\"Experiment C — ΔAUNB (iso − raw)\",\n",
    "    outfile=None,\n",
    "    model_order=None,\n",
    "    background=\"raw\",      # \"raw\" or \"iso\" heatmap background\n",
    "    values=\"delta\",        # numbers shown in cells: \"delta\" | \"raw\" | \"iso\"\n",
    "    fmt_delta=\"+.3g\",      # format for delta numbers\n",
    "    fmt_bg=\".3g\"           # format for raw/iso numbers\n",
    "):\n",
    "    \"\"\"Prevalence × model heatmap. Numbers per cell. No arrows.\"\"\"\n",
    "\n",
    "    # --- normalize prevalence keys (accept 0.05, 5, \"5%\") ---\n",
    "    def _to_float_prev(k):\n",
    "        if isinstance(k, (int, float, np.integer, np.floating)):\n",
    "            v = float(k); return v/100.0 if v > 1.0 else v\n",
    "        v = float(str(k).strip().replace(\"%\",\"\"))\n",
    "        return v/100.0 if v > 1.0 else v\n",
    "\n",
    "    def _as_label(p): return f\"{int(round(p*100))}%\"\n",
    "\n",
    "    def _standardize(d):\n",
    "        out = {}\n",
    "        for k, df in d.items():\n",
    "            p = _to_float_prev(k)\n",
    "            if df is None or df.empty:\n",
    "                continue\n",
    "            cols = {c.lower(): c for c in df.columns}\n",
    "            if \"model\" not in cols or \"median\" not in cols:\n",
    "                raise ValueError(\"Each AUNB table must have columns 'Model' and 'median'.\")\n",
    "            out[p] = (df[[cols[\"model\"], cols[\"median\"]]]\n",
    "                        .rename(columns={cols[\"model\"]: \"Model\", cols[\"median\"]: \"median\"})\n",
    "                        .dropna())\n",
    "        return out\n",
    "\n",
    "    raw = _standardize(aunb_raw_dict)\n",
    "    iso = _standardize(aunb_iso_dict)\n",
    "    prevalences = sorted(set(raw) | set(iso))\n",
    "    if not prevalences:\n",
    "        raise ValueError(\"No prevalences found. Run Experiment C first.\")\n",
    "\n",
    "    models = sorted(\n",
    "        set().union(*[set(df[\"Model\"]) for df in raw.values()] if raw else [set()],\n",
    "                    *[set(df[\"Model\"]) for df in iso.values()] if iso else [set()])\n",
    "    )\n",
    "    if not models:\n",
    "        raise ValueError(\"No models found in AUNB tables.\")\n",
    "\n",
    "    if model_order:\n",
    "        models = [m for m in model_order if m in models] + [m for m in models if m not in (model_order or [])]\n",
    "\n",
    "    # matrices (rows=models, cols=prevalences)\n",
    "    R = pd.DataFrame(index=models, columns=prevalences, dtype=float)\n",
    "    I = pd.DataFrame(index=models, columns=prevalences, dtype=float)\n",
    "\n",
    "    for p in prevalences:\n",
    "        if p in raw:\n",
    "            R[p] = raw[p].set_index(\"Model\")[\"median\"].reindex(models).astype(float)\n",
    "        if p in iso:\n",
    "            I[p] = iso[p].set_index(\"Model\")[\"median\"].reindex(models).astype(float)\n",
    "\n",
    "    D = I - R\n",
    "    B = I if background == \"iso\" else R  # heatmap background matrix\n",
    "    V = {\"delta\": D, \"raw\": R, \"iso\": I}[values]  # numbers to print\n",
    "\n",
    "    # --- plotting ---\n",
    "    fig, ax = plt.subplots(figsize=(max(6.5, 0.75*len(prevalences)+1),\n",
    "                                    max(4.5, 0.45*len(models)+1)))\n",
    "\n",
    "    im = ax.imshow(B.values, aspect=\"auto\", interpolation=\"nearest\")\n",
    "    ax.set_xticks(np.arange(len(prevalences)))\n",
    "    ax.set_yticks(np.arange(len(models)))\n",
    "    ax.set_xticklabels([_as_label(p) for p in prevalences])\n",
    "    ax.set_yticklabels(models)\n",
    "    ax.set_xlabel(\"Prevalence\")\n",
    "    ax.set_ylabel(\"Model\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label(f\"AUNB ({'iso' if background=='iso' else 'raw'} median)\")\n",
    "\n",
    "    # numbers in cells with auto-contrast\n",
    "    vmin = np.nanmin(B.values) if np.isfinite(B.values).any() else 0.0\n",
    "    vmax = np.nanmax(B.values) if np.isfinite(B.values).any() else 1.0\n",
    "    cmap = im.get_cmap() if hasattr(im, \"get_cmap\") else cm.get_cmap()\n",
    "\n",
    "    def _text_color(bval):\n",
    "        if not np.isfinite(bval) or vmax == vmin:\n",
    "            return \"black\"\n",
    "        t = (bval - vmin) / (vmax - vmin + 1e-12)\n",
    "        r, g, b, _ = cmap(t)\n",
    "        # perceived luminance\n",
    "        L = 0.299*r + 0.587*g + 0.114*b\n",
    "        return \"black\" if L > 0.6 else \"white\"\n",
    "\n",
    "    n_rows, n_cols = B.shape\n",
    "    for r in range(n_rows):\n",
    "        for c in range(n_cols):\n",
    "            val_print = V.iat[r, c]\n",
    "            bg_val = B.iat[r, c]\n",
    "            if not np.isfinite(val_print):\n",
    "                continue\n",
    "            txt = format(val_print, fmt_delta if values == \"delta\" else fmt_bg)\n",
    "            ax.text(c, r, txt, ha=\"center\", va=\"center\",\n",
    "                    fontsize=9, color=_text_color(bg_val))\n",
    "\n",
    "    # grid\n",
    "    ax.set_xticks(np.arange(-0.5, len(prevalences), 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-0.5, len(models), 1), minor=True)\n",
    "    ax.grid(which=\"minor\", linestyle=\":\", linewidth=0.5)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # safe save\n",
    "    if outfile:\n",
    "        outpath = Path(outfile)\n",
    "        try:\n",
    "            outdir = outpath.parent\n",
    "            if str(outdir) in {\"\", \".\", None}:\n",
    "                outdir = Path(\"./results\")\n",
    "            outdir.mkdir(parents=True, exist_ok=True)\n",
    "            plt.savefig(outdir / outpath.name, dpi=300, bbox_inches=\"tight\")\n",
    "            print(\"Saved figure to:\", outdir / outpath.name)\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Could not save to {outfile} ({e}). Showing only.\")\n",
    "    plt.show()\n",
    "    return D\n",
    "\n",
    "# -------- Build inputs from EXP_C and plot --------\n",
    "assert \"EXP_C\" in globals() and isinstance(EXP_C, dict), \"Run Experiment C first.\"\n",
    "\n",
    "aunb_raw_dict = {}\n",
    "aunb_iso_dict  = {}\n",
    "for prev, pack in EXP_C.items():\n",
    "    df_r = pack.get(\"aunb_raw\", pd.DataFrame())\n",
    "    df_i = pack.get(\"aunb_iso\", pd.DataFrame())\n",
    "    if not df_r.empty:\n",
    "        aunb_raw_dict[prev] = df_r[[\"Model\",\"median\"]]\n",
    "    if not df_i.empty:\n",
    "        aunb_iso_dict[prev] = df_i[[\"Model\",\"median\"]]\n",
    "\n",
    "preferred_order = [\n",
    "    \"RandomForest\", \"Logistic\", \"FeatureSel (best pipe)\",\n",
    "    \"XGBoost\", \"MLP\", \"Stacking (meta)\"\n",
    "]\n",
    "\n",
    "D = plot_aunb_heatmap(\n",
    "        aunb_raw_dict,\n",
    "        aunb_iso_dict,\n",
    "        title=\"Experiment C — ΔAUNB (isotonic − raw)\",\n",
    "        outfile=Path(\"./results/aunb_delta_heatmap.png\"),\n",
    "        model_order=preferred_order,\n",
    "        background=\"raw\",   # heatmap shows raw AUNB\n",
    "        values=\"delta\"      # numbers are ΔAUNB (iso − raw)\n",
    ")\n",
    "\n",
    "display(pd.DataFrame(D).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee678f81",
   "metadata": {},
   "source": [
    "#### Experiment D — Alarm-load frontier (FP per 1000 wafers vs Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58287283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Experiment D — Alarm-load frontier (FP per 1000 wafers vs Recall) ===\n",
    "\n",
    "# Helpers\n",
    "def fp_per_1000(y, pred):\n",
    "    \"\"\"False positives per 1000 wafers.\"\"\"\n",
    "    y = np.asarray(y, int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "    N = len(y)\n",
    "    return 1000.0 * (fp / N)\n",
    "\n",
    "def curve_fp1000_vs_recall(y, p, strategy=\"pr_thresholds\"):\n",
    "    \"\"\"\n",
    "    Return arrays (recall, fp_per_1000) by sweeping thresholds.\n",
    "    strategy:\n",
    "      - 'pr_thresholds': use PR-curve thresholds (recommended)\n",
    "      - 'linspace': uniform thresholds in [0,1]\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, int); p = np.asarray(p, float)\n",
    "    if strategy == \"pr_thresholds\":\n",
    "        P, R, T = precision_recall_curve(y, p)\n",
    "        thr = np.r_[T, 1.0]  # align to length of R\n",
    "    else:\n",
    "        thr = np.linspace(0.0, 1.0, 501)\n",
    "        # compute recall separately below\n",
    "\n",
    "    rec_list, fp1k_list = [], []\n",
    "    for t in thr:\n",
    "        pred = (p >= t).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "        rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        rec_list.append(rec)\n",
    "        fp1k_list.append(1000.0 * fp / len(y))\n",
    "    r = np.asarray(rec_list, float)\n",
    "    f = np.asarray(fp1k_list, float)\n",
    "\n",
    "    # keep Pareto frontier: for each recall keep min FP/1000\n",
    "    # bin by recall to smooth jagged duplicates\n",
    "    bins = np.linspace(0, 1, 201)\n",
    "    idx = np.digitize(r, bins, right=True)\n",
    "    best = {}\n",
    "    for i, b in enumerate(idx):\n",
    "        if b not in best or f[i] < best[b][1]:\n",
    "            best[b] = (r[i], f[i])\n",
    "    out = np.array(sorted(best.values(), key=lambda x: x[0]))\n",
    "    return out[:,0], out[:,1]\n",
    "\n",
    "def plot_alarm_frontier(models=(\"Logistic\",\"RandomForest\"),\n",
    "                        calibrated_modes=(\"raw\",\"iso\"),\n",
    "                        title=\"Experiment D — Alarm-load frontier (FP/1000 vs Recall)\",\n",
    "                        outfile=RESULT_DIR/\"alarm_frontier.png\"):\n",
    "    plt.figure(figsize=(7.8, 5.0))\n",
    "    styles = {(\"raw\"): \"-\", (\"iso\"): \"--\"}\n",
    "    for m in models:\n",
    "        for cal in calibrated_modes:\n",
    "            p = get_probs(m, split=\"test\", calibrated=cal)\n",
    "            if p is None: \n",
    "                continue\n",
    "            r, f = curve_fp1000_vs_recall(yte, p, strategy=\"pr_thresholds\")\n",
    "            lbl = f\"{m} ({cal})\"\n",
    "            plt.plot(r, f, styles[cal], lw=1.8, label=lbl)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"False positives per 1000 wafers\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(ncol=2, fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, dpi=300, bbox_inches=\"tight\")\n",
    "    print(\"Saved figure to:\", outfile)\n",
    "    plt.show()\n",
    "\n",
    "plot_alarm_frontier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a40151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: numeric summary at target recall levels\n",
    "TARGET_RECALL = [0.10, 0.20, 0.30]\n",
    "\n",
    "def nearest_fp1k_at_recall(y, p, r_target):\n",
    "    r, f = curve_fp1000_vs_recall(y, p)\n",
    "    if len(r) == 0: \n",
    "        return np.nan\n",
    "    j = int(np.argmin(np.abs(r - r_target)))\n",
    "    return float(f[j])\n",
    "\n",
    "rows = []\n",
    "for m in (\"Logistic\",\"RandomForest\"):\n",
    "    for cal in (\"raw\",\"iso\"):\n",
    "        p = get_probs(m, split=\"test\", calibrated=cal)\n",
    "        if p is None: \n",
    "            continue\n",
    "        for rt in TARGET_RECALL:\n",
    "            fp1k = nearest_fp1k_at_recall(yte, p, rt)\n",
    "            rows.append({\"Model\": m, \"Calib\": cal, \"Recall_target\": rt, \"FP_per_1000\": fp1k})\n",
    "\n",
    "df_frontier = pd.DataFrame(rows)\n",
    "# wide view with delta (iso - raw)\n",
    "wide = (df_frontier\n",
    "        .pivot_table(index=[\"Model\",\"Recall_target\"], columns=\"Calib\", values=\"FP_per_1000\")\n",
    "        .reindex(columns=[\"raw\",\"iso\"]))\n",
    "wide[\"ΔFP/1000 (iso-raw)\"] = wide[\"iso\"] - wide[\"raw\"]\n",
    "display(wide.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb1ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: per-model overlay plots for report\n",
    "def plot_per_model_frontier(model):\n",
    "    plt.figure(figsize=(6.2, 4.6))\n",
    "    for cal, style in ((\"raw\",\"-\"), (\"iso\",\"--\")):\n",
    "        p = get_probs(model, split=\"test\", calibrated=cal)\n",
    "        if p is None: \n",
    "            continue\n",
    "        r, f = curve_fp1000_vs_recall(yte, p)\n",
    "        plt.plot(r, f, style, lw=1.8, label=f\"{model} ({cal})\")\n",
    "    for rt in TARGET_RECALL:\n",
    "        plt.axvline(rt, ls=\":\", color=\"gray\", lw=1)  # guide\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"False positives per 1000 wafers\")\n",
    "    plt.title(f\"Alarm-load frontier — {model}\")\n",
    "    plt.grid(True, alpha=0.3); plt.legend(fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    savefig(f\"04_alarm_frontier_{model}.png\")\n",
    "    plt.show()\n",
    "\n",
    "for m in (\"Logistic\",\"RandomForest\"):\n",
    "    plot_per_model_frontier(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fb500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Executive Digest (A–D) — Fully data-driven ===\n",
    "# Requirements (from your notebook):\n",
    "# - Experiment A table:        `tab` with columns [\"Model\",\"Raw_crossover\",\"Iso_crossover\",\"Shift_in_ratio\"]\n",
    "# - Experiment B summaries:    `summaries` dict keyed by B with 'xover_raw'/'xover_iso' (Median per Model)\n",
    "# - Experiment C prevalence:   `EXP_C` dict with per-prev 'xover_raw'/'xover_iso' (Median per Model)\n",
    "# - Experiment D helpers:      `curve_fp1000_vs_recall`, `get_probs`, `yte`\n",
    "# - Paths:                     `RESULT_DIR`\n",
    "\n",
    "# -------- Panel A: Crossover shift (iso − raw) vs Logistic, from Experiment A `tab`\n",
    "def panel_A(ax):\n",
    "    if 'tab' not in globals() or tab is None or tab.empty:\n",
    "        ax.text(0.5, 0.5, \"Experiment A results not found\", ha=\"center\", va=\"center\")\n",
    "        ax.axis(\"off\"); return\n",
    "\n",
    "    dfA = tab.copy()\n",
    "    # Numeric shifts only\n",
    "    df_num = dfA[pd.to_numeric(dfA[\"Shift_in_ratio\"], errors=\"coerce\").notna()].copy()\n",
    "    df_num[\"Shift_in_ratio\"] = df_num[\"Shift_in_ratio\"].astype(float)\n",
    "    if df_num.empty:\n",
    "        ax.text(0.5, 0.5, \"No numeric crossovers to display\", ha=\"center\", va=\"center\")\n",
    "        ax.axis(\"off\"); return\n",
    "\n",
    "    df_num = df_num.sort_values(\"Shift_in_ratio\")\n",
    "    colors = np.where(df_num[\"Shift_in_ratio\"] < 0, \"tab:green\", \"tab:red\")\n",
    "    ax.barh(df_num[\"Model\"], df_num[\"Shift_in_ratio\"], color=colors)\n",
    "    ax.axvline(0, color=\"gray\", ls=\"--\", lw=1)\n",
    "    ax.set_xlabel(\"Shift in ratio (iso − raw)\"); ax.set_title(\"A. Crossover Shift (vs Logistic)\")\n",
    "    ax.grid(True, axis=\"x\", alpha=0.3)\n",
    "\n",
    "    # Optional annotations\n",
    "    for y, v in enumerate(df_num[\"Shift_in_ratio\"].to_numpy()):\n",
    "        ax.text(v, y, f\"{v:+.0f}\", va=\"center\",\n",
    "                ha=\"left\" if v>=0 else \"right\", fontsize=8)\n",
    "\n",
    "# -------- Panel B: Bootstrap robustness — median crossover ratio (vs Logistic), from `summaries`\n",
    "def panel_B(ax):\n",
    "    if 'summaries' not in globals() or not isinstance(summaries, dict) or not summaries:\n",
    "        ax.text(0.5, 0.5, \"Experiment B summaries not found\", ha=\"center\", va=\"center\")\n",
    "        ax.axis(\"off\"); return\n",
    "\n",
    "    # pick the largest B that exists\n",
    "    B_pick = max(summaries.keys())\n",
    "    sx = summaries[B_pick]\n",
    "    xr = sx.get(\"xover_raw\", pd.DataFrame())\n",
    "    xi = sx.get(\"xover_iso\", pd.DataFrame())\n",
    "    if xr is None or xi is None or xr.empty or xi.empty:\n",
    "        ax.text(0.5, 0.5, \"Missing xover medians\", ha=\"center\", va=\"center\")\n",
    "        ax.axis(\"off\"); return\n",
    "\n",
    "    # Merge medians for models present in either; drop Logistic and non-numeric\n",
    "    def clean(df):\n",
    "        d = df.copy()\n",
    "        d = d[pd.to_numeric(d[\"Median\"], errors=\"coerce\").notna()]\n",
    "        d[\"Median\"] = d[\"Median\"].astype(float)\n",
    "        return d[~d[\"Model\"].str.contains(\"^Logistic$\", case=False, na=False)]\n",
    "\n",
    "    xr = clean(xr).rename(columns={\"Median\":\"Raw\"})\n",
    "    xi = clean(xi).rename(columns={\"Median\":\"Iso\"})\n",
    "    m = pd.merge(xr[[\"Model\",\"Raw\"]], xi[[\"Model\",\"Iso\"]], on=\"Model\", how=\"outer\").dropna()\n",
    "\n",
    "    if m.empty:\n",
    "        ax.text(0.5, 0.5, \"No common models with medians\", ha=\"center\", va=\"center\")\n",
    "        ax.axis(\"off\"); return\n",
    "\n",
    "    m = m.sort_values(\"Raw\")\n",
    "    x = np.arange(len(m))\n",
    "    ax.bar(x-0.2, m[\"Raw\"], width=0.4, label=\"Raw\")\n",
    "    ax.bar(x+0.2, m[\"Iso\"], width=0.4, label=\"Iso\")\n",
    "    ax.set_xticks(x); ax.set_xticklabels(m[\"Model\"], rotation=90)\n",
    "    ax.set_ylabel(\"Median crossover ratio (vs Logistic)\")\n",
    "    ax.set_title(f\"B. Bootstrap Robustness (B={B_pick})\")\n",
    "    ax.grid(True, axis=\"y\", alpha=0.3)\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "# -------- Panel C: Prevalence shift — RF vs Logistic median crossover across prevalences, from `EXP_C`\n",
    "def panel_C(ax):\n",
    "    if 'EXP_C' not in globals() or not isinstance(EXP_C, dict) or not EXP_C:\n",
    "        ax.text(0.5, 0.5, \"Experiment C results not found\", ha=\"center\", va=\"center\")\n",
    "        ax.axis(\"off\"); return\n",
    "\n",
    "    prevs = []\n",
    "    rf_raw = []\n",
    "    rf_iso = []\n",
    "    for prev, pack in sorted(EXP_C.items(), key=lambda kv: float(kv[0])):\n",
    "        xr = pack.get(\"xover_raw\", pd.DataFrame())\n",
    "        xi = pack.get(\"xover_iso\", pd.DataFrame())\n",
    "        def get_med(df, model=\"RandomForest\"):\n",
    "            if df is None or df.empty: return np.nan\n",
    "            d = df[df[\"Model\"].str.contains(\"^RandomForest$\", case=False, na=False)]\n",
    "            if d.empty: return np.nan\n",
    "            v = pd.to_numeric(d[\"Median\"], errors=\"coerce\")\n",
    "            return float(v.iloc[0]) if v.notna().any() else np.nan\n",
    "        r_raw = get_med(xr); r_iso = get_med(xi)\n",
    "        if np.isfinite(r_raw) and np.isfinite(r_iso):\n",
    "            prevs.append(float(prev)*100 if float(prev)<=1 else float(prev))\n",
    "            rf_raw.append(r_raw); rf_iso.append(r_iso)\n",
    "\n",
    "    if not prevs:\n",
    "        ax.text(0.5, 0.5, \"No RF crossover medians across prevalences\", ha=\"center\", va=\"center\")\n",
    "        ax.axis(\"off\"); return\n",
    "\n",
    "    ax.plot(prevs, rf_raw, \"o-\", label=\"Raw crossover\")\n",
    "    ax.plot(prevs, rf_iso, \"s--\", label=\"Iso crossover\")\n",
    "    ax.set_xticks(prevs); ax.set_xlabel(\"Prevalence (%)\")\n",
    "    ax.set_ylabel(\"Median crossover ratio (RF vs Logistic)\")\n",
    "    ax.set_title(\"C. Prevalence Shift\")\n",
    "    ax.grid(True, alpha=0.3); ax.legend(fontsize=9)\n",
    "\n",
    "# -------- Panel D: Alarm-load frontier — FP/1000 vs Recall for RF (raw vs iso), recomputed from probs\n",
    "def panel_D(ax):\n",
    "    # Needs Experiment D helpers\n",
    "    missing = []\n",
    "    for name in [\"curve_fp1000_vs_recall\", \"get_probs\", \"yte\"]:\n",
    "        if name not in globals():\n",
    "            missing.append(name)\n",
    "    if missing:\n",
    "        ax.text(0.5, 0.5, f\"Missing: {', '.join(missing)}\", ha=\"center\", va=\"center\")\n",
    "        ax.axis(\"off\"); return\n",
    "\n",
    "    curves = {}\n",
    "    for cal, style in ((\"raw\",\"-\"), (\"iso\",\"--\")):\n",
    "        p = get_probs(\"RandomForest\", split=\"test\", calibrated=cal)\n",
    "        if p is None:\n",
    "            continue\n",
    "        r, f = curve_fp1000_vs_recall(yte, p, strategy=\"pr_thresholds\")\n",
    "        curves[cal] = (r, f)\n",
    "        ax.plot(r, f, style, lw=1.8, label=f\"RandomForest ({cal})\")\n",
    "\n",
    "    if not curves:\n",
    "        ax.text(0.5, 0.5, \"No RF curves available\", ha=\"center\", va=\"center\")\n",
    "        ax.axis(\"off\"); return\n",
    "\n",
    "    ax.set_xlabel(\"Recall\")\n",
    "    ax.set_ylabel(\"False positives per 1000 wafers\")\n",
    "    ax.set_title(\"D. Alarm-Load Frontier (RF)\")\n",
    "    ax.grid(True, alpha=0.3); ax.legend(fontsize=9)\n",
    "\n",
    "# -------- Compose and save\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 9))\n",
    "panel_A(axes[0,0])\n",
    "panel_B(axes[0,1])\n",
    "panel_C(axes[1,0])\n",
    "panel_D(axes[1,1])\n",
    "\n",
    "plt.suptitle(\"Executive Digest — Calibration & Cost-Sensitive Evaluation (SECOM)\", fontsize=15, y=0.995)\n",
    "plt.tight_layout(rect=[0,0,1,0.97])\n",
    "savefig(\"05_executive_digest.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
