{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a441c797",
   "metadata": {},
   "source": [
    "# SECOM Yield Prediction — End-to-End Notebook\n",
    "\n",
    "**Goal:** Predict *Fail* outcomes from process measurements to reduce scrap and downtime.\n",
    "\n",
    "**Data:** `data/secom.data`, `data/secom_labels.data`, `data/secom.names` (UCI ML Repository, real fab data).\n",
    "\n",
    "**Primary metric:** Recall on *Fail* at acceptable precision. Report PR-AUC and Balanced Error Rate (BER).\n",
    "\n",
    "> Safety: No unsupported claims. Treat outputs as decision support, not automation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b76e9",
   "metadata": {},
   "source": [
    "## Dataset Information\n",
    "```\n",
    "Additional Information\n",
    "\n",
    "A complex modern semi-conductor manufacturing process is normally under consistent surveillance via the monitoring of signals/variables collected from sensors and or process measurement points. However, not all of these signals are equally valuable in a specific monitoring system. The measured signals contain a combination of useful information, irrelevant information as well as noise. It is often the case that useful information is buried in the latter two. Engineers typically have a much larger number of signals than are actually required. If we consider each type of signal as a feature, then feature selection may be applied to identify the most relevant signals. The Process Engineers may then use these signals to determine key factors contributing to yield excursions downstream in the process. This will enable an increase in process throughput, decreased time to learning and reduce the per unit production costs.\n",
    "\n",
    "To enhance current business improvement techniques the application of feature selection as an intelligent systems technique is being investigated.\n",
    "\n",
    "The dataset presented in this case represents a selection of such features where each example represents a single production entity with associated measured features and the labels represent a simple pass/fail yield for in house line testing, figure 2, and associated date time stamp. Where –1 corresponds to a pass and 1 corresponds to a fail and the data time stamp is for that specific test point.\n",
    "\n",
    "\n",
    "Using feature selection techniques it is desired to rank features according to their impact on the overall yield for the product, causal relationships may also be considered with a view to identifying the key features.\n",
    "\n",
    "Results may be submitted in terms of feature relevance for predictability using error rates as our evaluation metrics. It is suggested that cross validation be applied to generate these results. Some baseline results are shown below for basic feature selection techniques using a simple kernel ridge classifier and 10 fold cross validation.\n",
    "\n",
    "Baseline Results: Pre-processing objects were applied to the dataset simply to standardize the data and remove the constant features and then a number of different feature selection objects selecting 40 highest ranked features were applied with a simple classifier to achieve some initial results. 10 fold cross validation was used and the balanced error rate (*BER) generated as our initial performance metric to help investigate this dataset.\n",
    "\n",
    "\n",
    "SECOM Dataset: 1567 examples 591 features, 104 fails\n",
    "\n",
    "FSmethod (40 features) BER % True + % True - %\n",
    "S2N (signal to noise) 34.5 +-2.6 57.8 +-5.3 73.1 +2.1\n",
    "Ttest 33.7 +-2.1 59.6 +-4.7 73.0 +-1.8\n",
    "Relief 40.1 +-2.8 48.3 +-5.9 71.6 +-3.2\n",
    "Pearson 34.1 +-2.0 57.4 +-4.3 74.4 +-4.9\n",
    "Ftest 33.5 +-2.2 59.1 +-4.8 73.8 +-1.8\n",
    "Gram Schmidt 35.6 +-2.4 51.2 +-11.8 77.5 +-2.3\n",
    "\n",
    "Has Missing Values?\n",
    "\n",
    "Yes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb906650",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Global style for plots\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 12,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"axes.edgecolor\": \"black\",\n",
    "    \"axes.linewidth\": 1,\n",
    "    \"xtick.color\": \"black\",\n",
    "    \"ytick.color\": \"black\",\n",
    "    \"xtick.direction\": \"out\",\n",
    "    \"ytick.direction\": \"out\",\n",
    "    \"xtick.bottom\": True,\n",
    "    \"ytick.left\": True,\n",
    "    \"xtick.top\": False,\n",
    "    \"ytick.right\": False,\n",
    "    \"figure.dpi\": 150,\n",
    "    \"legend.frameon\": True,\n",
    "    \"legend.facecolor\": \"white\",\n",
    "    \"legend.edgecolor\": \"black\",\n",
    "    \"legend.fontsize\": 12\n",
    "})\n",
    "\n",
    "# Apply to seaborn\n",
    "sns.set_theme(context=\"notebook\", style=\"ticks\")\n",
    "\n",
    "# Reproducibility\n",
    "rnd_num = 42\n",
    "np.random.seed(rnd_num)\n",
    "\n",
    "# Root directory\n",
    "ROOT = Path(os.getcwd()).parent\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = Path(ROOT/\"data\")\n",
    "RAW = Path(DATA_DIR/\"raw\")\n",
    "assert (RAW/\"secom.data\").exists() and (RAW/\"secom_labels.data\").exists(), \"Data files are missing!\"\n",
    "\n",
    "# Results directory\n",
    "RESULT_DIR = Path(ROOT/\"results\")\n",
    "RESULT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Model directory\n",
    "MODEL_DIR = Path(ROOT/\"models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32372176",
   "metadata": {},
   "source": [
    "### 2. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure saver helper (use after each plot)\n",
    "RESULT_DIR_EDA = Path(RESULT_DIR/\"EDA\")\n",
    "RESULT_DIR_EDA.mkdir(exist_ok=True, parents=True)\n",
    "def savefig(name):\n",
    "    out = RESULT_DIR_EDA/name\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out, dpi=300, bbox_inches=\"tight\")\n",
    "    print(\"Saved:\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac509808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_countplot(ax, summary, label_map={0:\"Pass\", 1:\"Fail\"}, offset_frac=0.001, position=\"top\"):\n",
    "    \"\"\"\n",
    "    Annotate bars in a countplot with count and percentage.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib Axes\n",
    "        The countplot axes object.\n",
    "    summary : DataFrame\n",
    "        DataFrame with Count and Percentage indexed by label names.\n",
    "    label_map : dict\n",
    "        Maps tick index (0,1,...) to label names in summary.\n",
    "    offset_frac : float\n",
    "        Fraction of bar height/axis to offset text.\n",
    "    position : {\"top\",\"center\",\"bottom\"}\n",
    "        Where to place annotation relative to bar.\n",
    "    \"\"\"\n",
    "    heights = [p.get_height() for p in ax.patches if p.get_height() > 0]\n",
    "    if not heights:\n",
    "        return\n",
    "    offset = max(heights) * offset_frac\n",
    "\n",
    "    for p in ax.patches:\n",
    "        h = p.get_height()\n",
    "        if h <= 0:   # skip zero-height bars from hue\n",
    "            continue\n",
    "\n",
    "        x = p.get_x() + p.get_width()/2.0\n",
    "        cls_idx = int(round(x))  \n",
    "        cls_name = label_map.get(cls_idx, str(cls_idx))\n",
    "\n",
    "        cnt = summary.loc[cls_name, \"Count\"]\n",
    "        pct = summary.loc[cls_name, \"Percentage\"]\n",
    "\n",
    "        # y-position depending on user choice\n",
    "        if position == \"top\":\n",
    "            y = h + offset\n",
    "            va = \"bottom\"\n",
    "        elif position == \"center\":\n",
    "            y = h/2\n",
    "            va = \"center\"\n",
    "        elif position == \"bottom\":\n",
    "            y = offset\n",
    "            va = \"bottom\"\n",
    "        else:\n",
    "            raise ValueError(\"position must be 'top','center', or 'bottom'\")\n",
    "\n",
    "        ax.annotate(f\"{cnt} ({pct:.1f}%)\", (x, y),\n",
    "                    ha=\"center\", va=va, fontsize=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a4318e",
   "metadata": {},
   "source": [
    "### 3. Load and Audit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c1fd5e",
   "metadata": {},
   "source": [
    "#### 3.1. Load features and labels\n",
    "\n",
    "**Key facts:** Data Structure: The data consists of 2 files the dataset file SECOM consisting of 1567 examples each with 590 features a 1567 x 590 matrix and a labels file containing the classifications and date time stamp for each example. The data is represented in a raw text file each line representing an individual example and the features separated by spaces. The null values are represented by the 'NaN' value as per MatLab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ba6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at the data features count per row.\n",
    "p = Path(str(RAW/\"secom.data\"))\n",
    "with p.open() as f:\n",
    "    counts = [len(line.split()) for line in f]\n",
    "\n",
    "print(\"min/max fields per row:\", min(counts), max(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0964259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train data.\n",
    "# Features: 590 columns, whitespace-separated, 'NaN' denotes missing\n",
    "data_X = pd.read_csv(RAW/\"secom.data\", header=None, sep=\"\\s+\", na_values=[\"NaN\"], engine=\"python\")\n",
    "data_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394f5c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audit the data structure.\n",
    "print(\"Shape:\", data_X.shape)\n",
    "print(\"\\n--- X info ---\"); data_X.info()\n",
    "print(\"\\nMissing fraction across features:\"); display(data_X.isna().mean().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9592bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels: two columns (label, timestamp)\n",
    "data_y = pd.read_csv(RAW/\"secom_labels.data\", sep=r\"\\s+\", header=None, names=[\"label\",\"timestamp\"])\n",
    "data_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65eed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audit the data structure.\n",
    "print(\"Shape:\", data_y.shape)\n",
    "print(\"\\n--- X info ---\"); data_y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad36f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestap to datetime.\n",
    "data_y[\"timestamp\"] = pd.to_datetime(data_y[\"timestamp\"], format=\"%d/%m/%Y %H:%M:%S\", errors=\"raise\")\n",
    "data_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ef4b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1428230",
   "metadata": {},
   "source": [
    "#### 3.2. Sanity check and merge the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946f5d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(data_X) == len(data_y), \"Row count mismatch\"\n",
    "assert data_y[\"timestamp\"].notna().all(), \"Bad timestamps found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9e3107",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_X.copy()\n",
    "df.columns = [f\"f{i:03d}\" for i in range(data_X.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04deff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed67d364",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = data_y[\"label\"].to_numpy()\n",
    "df[\"timestamp\"] = data_y[\"timestamp\"].to_numpy()\n",
    "df = df.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a8ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9ac58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ce811d",
   "metadata": {},
   "source": [
    "### 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df1bedf",
   "metadata": {},
   "source": [
    "#### 4.1. Map labels and check imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60fe97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map labels to 0/1. -1 -> 0 (normal), 1 -> 1 (abnormal)\n",
    "df[\"label\"] = df[\"label\"].map({-1:0, 1:1}).astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91b3518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort chronologically to avoid leakage later\n",
    "df = df.sort_values(\"timestamp\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e521af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw counts\n",
    "counts = df[\"label\"].value_counts().sort_index()\n",
    "\n",
    "# Percentages\n",
    "percentages = df[\"label\"].value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "# Combine into one table\n",
    "summary = pd.DataFrame({\n",
    "    \"Count\": counts,\n",
    "    \"Percentage\": percentages.round(2)\n",
    "}).rename(index={0:\"Pass\", 1:\"Fail\"})\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec5d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot\n",
    "ax = sns.countplot(x=\"label\", data=df, hue=\"label\", legend=True)\n",
    "\n",
    "ax.set_title(\"Label Distribution\")\n",
    "ax.set_xlabel(\"Label\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "xticks = ax.get_xticks()\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_xticklabels([\"Pass\", \"Fail\"])\n",
    "ax.legend(title=None, labels=['Pass', 'Fail'])\n",
    "annotate_countplot(ax, summary, position='top')\n",
    "savefig(\"01_label_distribution.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf82a8",
   "metadata": {},
   "source": [
    "#### 4.2. Missing value analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d707c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraction of missing per feature\n",
    "missing_frac = df.drop(columns=[\"label\", \"timestamp\"]).isna().mean()\n",
    "\n",
    "sns.histplot(missing_frac, bins=30)\n",
    "plt.xlabel(\"Fraction missing\")\n",
    "plt.ylabel(\"Count of features\")\n",
    "plt.title(\"Distribution of missing values across features\")\n",
    "savefig(\"02_missingness_distribution.png\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(missing_frac.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65d37e1",
   "metadata": {},
   "source": [
    "### 4.3. Feature distributions (compare Pass vs Fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8123d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cols = df.columns[:5]  # replace with random.sample(list(df.columns[:-1]), 5)\n",
    "\n",
    "for col in sample_cols:\n",
    "    plt.figure(figsize=(6,3))\n",
    "    sns.kdeplot(data=df, x=col, hue=\"label\", common_norm=False)\n",
    "    plt.title(f\"Distribution of {col} by label\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0729e06d",
   "metadata": {},
   "source": [
    "### 4.4. Outlier detection per feature (boxplots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in sample_cols:\n",
    "    ax = sns.boxplot(x=\"label\", y=col, data=df)\n",
    "    plt.title(f\"Boxplot of {col} by label\")\n",
    "    xticks = ax.get_xticks()\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels([\"Pass\", \"Fail\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c4544d",
   "metadata": {},
   "source": [
    "### 4.5. Correlation structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef4e28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.drop(columns=[\"label\", \"timestamp\"]).corr()\n",
    "\n",
    "#plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr.iloc[:,:], cmap=\"coolwarm\", center=0)  # subset 30x30 to avoid clutter\n",
    "plt.title(\"Feature correlation heatmap\")\n",
    "savefig(\"04_feature_correlation_heatmap.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24299bce",
   "metadata": {},
   "source": [
    "### 4.6. Variance by feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a20bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = df.drop(columns=[\"label\", \"timestamp\"]).var()\n",
    "sns.histplot(variances, bins=2)\n",
    "plt.xlabel(\"Variance\")\n",
    "plt.title(\"Distribution of feature variances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f56b314",
   "metadata": {},
   "source": [
    "### 4.7. Dimensionality check (PCA visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d1fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_scaled = sc.fit_transform(df.drop(columns=[\"label\", \"timestamp\"]).fillna(0))\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=df[\"label\"], alpha=0.6, palette=\"Set1\")\n",
    "plt.title(\"PCA projection (2 components)\")\n",
    "\n",
    "savefig(\"03_pca_projection.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875be6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3529626",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA = Path(DATA_DIR/\"interim\")\n",
    "EDA.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df.to_parquet(EDA/\"SECOM_EDA.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
