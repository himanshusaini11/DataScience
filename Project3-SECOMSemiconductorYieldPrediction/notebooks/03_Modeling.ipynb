{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52dbdcc6",
   "metadata": {},
   "source": [
    "# SECOM Yield Prediction — End-to-End Notebook\n",
    "\n",
    "**Goal:** Predict *Fail* outcomes from process measurements to reduce scrap and downtime.\n",
    "\n",
    "**Data:** `data/secom.data`, `data/secom_labels.data`, `data/secom.names` (UCI ML Repository, real fab data).\n",
    "\n",
    "**Primary metric:** Recall on *Fail* at acceptable precision. Report PR-AUC and Balanced Error Rate (BER).\n",
    "\n",
    "> Safety: No unsupported claims. Treat outputs as decision support, not automation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4a88af",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66f5789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Global style for plots\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 12,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"axes.edgecolor\": \"black\",\n",
    "    \"axes.linewidth\": 1,\n",
    "    \"xtick.color\": \"black\",\n",
    "    \"ytick.color\": \"black\",\n",
    "    \"xtick.direction\": \"out\",\n",
    "    \"ytick.direction\": \"out\",\n",
    "    \"xtick.bottom\": True,\n",
    "    \"ytick.left\": True,\n",
    "    \"xtick.top\": False,\n",
    "    \"ytick.right\": False,\n",
    "    \"figure.dpi\": 150,\n",
    "    \"legend.frameon\": True,\n",
    "    \"legend.facecolor\": \"white\",\n",
    "    \"legend.edgecolor\": \"black\",\n",
    "    \"legend.fontsize\": 12\n",
    "})\n",
    "\n",
    "# Apply to seaborn\n",
    "sns.set_theme(context=\"notebook\", style=\"ticks\")\n",
    "\n",
    "# Reproducibility\n",
    "rnd_num = 42\n",
    "np.random.seed(rnd_num)\n",
    "\n",
    "# Root directory\n",
    "ROOT = Path(os.getcwd()).parent\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = Path(ROOT/\"data\")\n",
    "RAW = Path(DATA_DIR/\"raw\")\n",
    "assert (RAW/\"secom.data\").exists() and (RAW/\"secom_labels.data\").exists(), \"Data files are missing!\"\n",
    "\n",
    "# Results directory\n",
    "RESULT_DIR = Path(ROOT/\"results\")\n",
    "RESULT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Model directory\n",
    "MODEL_DIR = Path(ROOT/\"models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07023c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure saver helper (use after each plot)\n",
    "RESULT_DIR_M = Path(RESULT_DIR/\"modeling\")\n",
    "RESULT_DIR_M.mkdir(exist_ok=True, parents=True)\n",
    "def savefig(name):\n",
    "    out = RESULT_DIR_M/name\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out, dpi=300, bbox_inches=\"tight\")\n",
    "    print(\"Saved:\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f9fa6a",
   "metadata": {},
   "source": [
    "### 6. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17076865",
   "metadata": {},
   "source": [
    "#### 6.1. Load the data from ETL step,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2594cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch, torch.nn as nn\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel, f_classif, mutual_info_classif\n",
    "\n",
    "from sklearn.metrics import (precision_recall_curve, average_precision_score,\n",
    "                             roc_auc_score, balanced_accuracy_score,\n",
    "                             confusion_matrix, classification_report,\n",
    "                             brier_score_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00882111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data produced by 02_ETL.ipynb\n",
    "PROC = Path(DATA_DIR/\"processed\")\n",
    "tr = pd.read_parquet(PROC/\"train.parquet\")\n",
    "va = pd.read_parquet(PROC/\"val.parquet\")\n",
    "te = pd.read_parquet(PROC/\"test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db320a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = [c for c in tr.columns if c not in (\"label\",\"timestamp\")]\n",
    "\n",
    "Xtr = tr[feat_cols].to_numpy(dtype=np.float32)\n",
    "ytr = tr[\"label\"].to_numpy(dtype=np.int8)\n",
    "\n",
    "Xva = va[feat_cols].to_numpy(dtype=np.float32)\n",
    "yva = va[\"label\"].to_numpy(dtype=np.int8)\n",
    "\n",
    "Xte = te[feat_cols].to_numpy(dtype=np.float32)\n",
    "yte = te[\"label\"].to_numpy(dtype=np.int8)\n",
    "\n",
    "# Global accumulator for leaderboard rows (dicts)\n",
    "all_results = []\n",
    "\n",
    "print(\"Shapes:\", Xtr.shape, Xva.shape, Xte.shape, \"| Pos rates:\", ytr.mean(), yva.mean(), yte.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9106dce",
   "metadata": {},
   "source": [
    "#### 6.2. Utilities (threshold + metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8a6624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_threshold_by_f1(y_true, proba):\n",
    "    \"\"\"Given true labels and predicted probabilities, find threshold maximizing F1.\"\"\"\n",
    "    \"\"\"y_true: array-like of shape (n_samples,) - true binary labels (0/1)\n",
    "       proba: array-like of shape (n_samples,) - predicted probabilities for the positive class\n",
    "       returns: dict with keys 'thr', 'precision', 'recall', 'f1' at optimal threshold\n",
    "    \"\"\"\n",
    "    p, r, t = precision_recall_curve(y_true, proba)\n",
    "    f1 = (2*p*r)/(p+r+1e-12)\n",
    "    j = np.argmax(f1)\n",
    "    return {\"thr\": float(np.r_[t,1.0][j]), \"precision\": float(p[j]), \"recall\": float(r[j]), \"f1\": float(f1[j])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bec601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_threshold_by_recall(y_true, proba, recall_floor=0.10):\n",
    "    \"\"\"Given true labels and predicted probabilities, find threshold achieving at least recall_floor,\n",
    "       and among those, maximizing precision.\n",
    "       y_true: array-like of shape (n_samples,) - true binary labels (0/1)\n",
    "       proba: array-like of shape (n_samples,) - predicted probabilities for the positive class\n",
    "       recall_floor: float in (0,1) - minimum recall to achieve\n",
    "       returns: dict with keys 'thr', 'precision', 'recall' at optimal threshold\n",
    "    \"\"\"\n",
    "    p, r, t = precision_recall_curve(y_true, proba)\n",
    "    ok = np.where(r >= recall_floor)[0]\n",
    "    j = ok[np.argmax(p[ok])] if len(ok) else len(r)-1\n",
    "    return {\"thr\": float(np.r_[t,1.0][j]), \"precision\": float(p[j]), \"recall\": float(r[j])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ee0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, proba, thr, name=\"model\"):\n",
    "    \"\"\"Evaluate binary classifier at given threshold.\n",
    "       y_true: array-like of shape (n_samples,) - true binary labels (0/1)\n",
    "       proba: array-like of shape (n_samples,) - predicted probabilities for the positive class\n",
    "       thr: float in [0,1] - threshold to convert proba to binary predictions\n",
    "       name: str - name of the model for reporting\n",
    "       returns: dict with metrics\n",
    "    \"\"\"\n",
    "    pred = (proba >= thr).astype(int)\n",
    "    pr = average_precision_score(y_true, proba)\n",
    "    roc = roc_auc_score(y_true, proba)\n",
    "    bal = balanced_accuracy_score(y_true, pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, pred).ravel()\n",
    "    print(f\"\\n== {name} @thr={thr:.3f} ==\")\n",
    "    print(\"Confusion:\", (tn, fp, fn, tp), \"| BalancedAcc:\", round(bal,4),\n",
    "          \"| PR-AUC:\", round(pr,4), \"| ROC-AUC:\", round(roc,4))\n",
    "    print(classification_report(y_true, pred, digits=3))\n",
    "    return {\"Model\": name, \"PR_AUC\": pr, \"ROC_AUC\": roc, \"BalancedAcc\": bal,\n",
    "            \"thr\": thr, \"TP\": int(tp), \"FP\": int(fp), \"TN\": int(tn), \"FN\": int(fn)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70699685",
   "metadata": {},
   "source": [
    "#### 6.3. Baseline: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553af6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit on train only\n",
    "lr = LogisticRegression(max_iter=2000, class_weight=\"balanced\", n_jobs=None)\n",
    "lr.fit(Xtr, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ad304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune threshold on validation\n",
    "lr_proba_val = lr.predict_proba(Xva)[:,1]\n",
    "lr_thr_f1 = pick_threshold_by_f1(yva, lr_proba_val)\n",
    "lr_thr_rec = pick_threshold_by_recall(yva, lr_proba_val, recall_floor=0.10)\n",
    "print(\"Val picks F1:\", lr_thr_f1, \"  Recall>=10%:\", lr_thr_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b2f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final test evals\n",
    "lr_proba_te = lr.predict_proba(Xte)[:,1]\n",
    "lr_res_f1 = evaluate(yte, lr_proba_te, lr_thr_f1[\"thr\"], name=\"Logistic | F1-opt\")\n",
    "lr_res_rec = evaluate(yte, lr_proba_te, lr_thr_rec[\"thr\"], name=\"Logistic | Recall>=10%\")\n",
    "\n",
    "# Save lr_res_f1 and lr_res_rec\n",
    "all_results.extend([lr_res_f1, lr_res_rec])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de22983",
   "metadata": {},
   "source": [
    "#### 6.4. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be5397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on train\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=None,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(Xtr, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e83c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholds from validation\n",
    "rf_proba_val = rf.predict_proba(Xva)[:,1]\n",
    "rf_thr_f1 = pick_threshold_by_f1(yva, rf_proba_val)\n",
    "rf_thr_rec = pick_threshold_by_recall(yva, rf_proba_val, recall_floor=0.10)\n",
    "print(\"Val picks -> F1:\", rf_thr_f1, \"  Recall≥10%:\", rf_thr_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c168b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final test evaluation\n",
    "rf_proba_te = rf.predict_proba(Xte)[:,1]\n",
    "rf_res_f1 = evaluate(yte, rf_proba_te, rf_thr_f1[\"thr\"], name=\"RF | F1-opt\")\n",
    "rf_res_rec = evaluate(yte, rf_proba_te, rf_thr_rec[\"thr\"], name=\"RF | Recall≥10%\")\n",
    "\n",
    "# Save rf_res_f1 and rf_res_rec\n",
    "all_results.extend([rf_res_f1, rf_res_rec])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac6d64a",
   "metadata": {},
   "source": [
    "#### 6.5. MLP Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7a0064",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available()\n",
    "                      else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(\"Torch device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7141b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define MLP ---\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_in):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, 256), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa2cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(Xtrain, ytrain, Xval, yval, epochs=50, lr=1e-3):\n",
    "    # tensors as float32\n",
    "    Xt = torch.tensor(Xtrain, dtype=torch.float32, device=device)\n",
    "    yt = torch.tensor(ytrain, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "    Xv = torch.tensor(Xval,   dtype=torch.float32, device=device)\n",
    "\n",
    "    model = MLP(Xtrain.shape[1]).to(device).float()\n",
    "\n",
    "    # class-weighted BCE (float32 pos_weight)\n",
    "    pos_w = np.float32((1.0 - ytrain.mean()) / (ytrain.mean() + 1e-12))\n",
    "    crit = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_w], dtype=torch.float32, device=device))\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_ap, best_state = -1.0, None\n",
    "    for ep in range(50 if epochs is None else epochs):\n",
    "        model.train(); opt.zero_grad()\n",
    "        logits = model(Xt)\n",
    "        loss = crit(logits, yt)\n",
    "        loss.backward(); opt.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pv = torch.sigmoid(model(Xv)).detach().cpu().numpy().astype(\"float32\").ravel()\n",
    "        ap = average_precision_score(yval, pv)\n",
    "        if ap > best_ap:\n",
    "            best_ap = ap\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        p_val = torch.sigmoid(model(Xv)).detach().cpu().numpy().astype(\"float32\").ravel()\n",
    "    return model, p_val, float(best_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b1a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp_probas(Xtrain, ytrain, Xval, yval, Xtest, epochs=60, lr=1e-3):\n",
    "    import numpy as np\n",
    "    import torch, torch.nn as nn\n",
    "\n",
    "    # tensors (float32) on current device\n",
    "    Xt  = torch.tensor(Xtrain, dtype=torch.float32, device=device)\n",
    "    yt  = torch.tensor(np.asarray(ytrain), dtype=torch.float32, device=device).view(-1, 1)\n",
    "    Xv  = torch.tensor(Xval,   dtype=torch.float32, device=device)\n",
    "    Xte = torch.tensor(Xtest,  dtype=torch.float32, device=device)\n",
    "\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, d_in):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(d_in, 256), nn.ReLU(), nn.Dropout(0.2),\n",
    "                nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.2),\n",
    "                nn.Linear(128, 1)\n",
    "            )\n",
    "        def forward(self, x): return self.net(x)\n",
    "\n",
    "    model = MLP(Xtrain.shape[1]).to(device)\n",
    "    pos_w = (1 - np.mean(ytrain)) / (np.mean(ytrain) + 1e-12)\n",
    "    crit = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_w], dtype=torch.float32, device=device))\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_ap, best_state = -1.0, None\n",
    "    for _ in range(epochs):\n",
    "        model.train(); opt.zero_grad()\n",
    "        out = model(Xt); loss = crit(out, yt); loss.backward(); opt.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pv = torch.sigmoid(model(Xv)).cpu().numpy().ravel()\n",
    "        from sklearn.metrics import average_precision_score\n",
    "        ap = average_precision_score(np.asarray(yval), pv)\n",
    "        if ap > best_ap:\n",
    "            best_ap, best_state = ap, {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        p_val = torch.sigmoid(model(Xv)).cpu().numpy().ravel()\n",
    "        p_te  = torch.sigmoid(model(Xte)).cpu().numpy().ravel()\n",
    "\n",
    "    return p_val, p_te, float(best_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e871d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train on train, tune on val ---\n",
    "mlp_model, p_val_mlp, ap_val = train_mlp(Xtr, ytr, Xva, yva, epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a317db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds from validation\n",
    "mlp_thr_f1  = pick_threshold_by_f1(yva, p_val_mlp)\n",
    "mlp_thr_rec = pick_threshold_by_recall(yva, p_val_mlp, recall_floor=0.10)\n",
    "print(\"Val picks F1:\", mlp_thr_f1, \"  Recall>=10%:\", mlp_thr_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faff9c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probas to feed ensembles\n",
    "mlp_proba_val, mlp_proba_te, mlp_ap = train_mlp_probas(Xtr, ytr, Xva, yva, Xte, epochs=60)\n",
    "print(\"MLP val AP:\", round(mlp_ap, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb10c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test evaluation\n",
    "Xtest_t = torch.tensor(Xte, dtype=torch.float32, device=device)\n",
    "with torch.no_grad():\n",
    "    p_test_mlp = torch.sigmoid(mlp_model(Xtest_t)).detach().cpu().numpy().astype(\"float32\").ravel()\n",
    "\n",
    "mlp_res_f1 = evaluate(yte, p_test_mlp, mlp_thr_f1[\"thr\"],  name=\"MLP | F1-opt\")\n",
    "mlp_res_rec = evaluate(yte, p_test_mlp, mlp_thr_rec[\"thr\"], name=\"MLP | Recall≥10%\")\n",
    "\n",
    "# after computing mlp_res_f1 and mlp_res_rec\n",
    "all_results.extend([mlp_res_f1, mlp_res_rec])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35155a46",
   "metadata": {},
   "source": [
    "#### 6.6. Calibration of Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- fit a calibrator on validation probs, return a callable ---\n",
    "def fit_calibrator(y_val, p_val, method=\"isotonic\"):\n",
    "    if method == \"isotonic\":\n",
    "        iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "        iso.fit(p_val, y_val)\n",
    "        return lambda p: iso.transform(p)\n",
    "    elif method == \"sigmoid\":\n",
    "        lr_cal = LogisticRegression(solver=\"lbfgs\")\n",
    "        # Platt scaling: logit(p) as feature; clip to avoid inf\n",
    "        eps = 1e-6\n",
    "        z = np.log(np.clip(p_val, eps, 1-eps) / np.clip(1-p_val, eps, 1-eps)).reshape(-1,1)\n",
    "        lr_cal.fit(z, y_val)\n",
    "        return lambda p: lr_cal.predict_proba(\n",
    "            np.log(np.clip(p, eps, 1-eps) / np.clip(1-p, eps, 1-eps)).reshape(-1,1)\n",
    "        )[:,1]\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'isotonic' or 'sigmoid'\")\n",
    "\n",
    "def calibrate_and_eval_prefit(base_model, name, Xva, yva, Xte, yte, method):\n",
    "    # get uncalibrated probabilities\n",
    "    p_val = base_model.predict_proba(Xva)[:,1]\n",
    "    p_te  = base_model.predict_proba(Xte)[:,1]\n",
    "    # fit calibrator on validation\n",
    "    cal = fit_calibrator(yva, p_val, method=method)\n",
    "    p_val_cal = cal(p_val)\n",
    "    p_te_cal  = cal(p_te)\n",
    "\n",
    "    # thresholds picked on validation-calibrated\n",
    "    thr_f1  = pick_threshold_by_f1(yva, p_val_cal)\n",
    "    thr_rec = pick_threshold_by_recall(yva, p_val_cal, recall_floor=0.10)\n",
    "    print(f\"\\n{name} ({method}) calibration\")\n",
    "    print(\"Val picks --> F1:\", thr_f1, \"  Recall≥10%:\", thr_rec)\n",
    "\n",
    "    # test eval\n",
    "    res_f1  = evaluate(yte, p_te_cal, thr_f1[\"thr\"],  name=f\"{name} ({method}) | F1-opt\")\n",
    "    res_rec = evaluate(yte, p_te_cal, thr_rec[\"thr\"], name=f\"{name} ({method}) | Recall≥10%\")\n",
    "    print(\"Brier score (test):\", round(brier_score_loss(yte, p_te_cal), 4))\n",
    "    return res_f1, res_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972feda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Logistic Regression (already trained as lr) ---\n",
    "lr_iso_res_f1,  lr_iso_res_rec = calibrate_and_eval_prefit(lr, \"Logistic\",      Xva, yva, Xte, yte, method=\"isotonic\")\n",
    "lr_sig_res_f1,  lr_sig_res_rec = calibrate_and_eval_prefit(lr, \"Logistic\",      Xva, yva, Xte, yte, method=\"sigmoid\")\n",
    "\n",
    "# Save LR calibrated\n",
    "all_results.extend([lr_iso_res_f1, lr_iso_res_rec, lr_sig_res_f1, lr_sig_res_rec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b89cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Random Forest (already trained as rf) ---\n",
    "rf_iso_res_f1,  rf_iso_res_rec = calibrate_and_eval_prefit(rf, \"RandomForest\",  Xva, yva, Xte, yte, method=\"isotonic\")\n",
    "rf_sig_res_f1,  rf_sig_res_rec = calibrate_and_eval_prefit(rf, \"RandomForest\",  Xva, yva, Xte, yte, method=\"sigmoid\")\n",
    "\n",
    "# Save RF calibrated\n",
    "all_results.extend([rf_iso_res_f1, rf_iso_res_rec, rf_sig_res_f1, rf_sig_res_rec])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c63a12f",
   "metadata": {},
   "source": [
    "#### 6.7. Leaderboard (auto-accumulate and render)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b442d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of dicts directly\n",
    "df_leader = pd.DataFrame(all_results)\n",
    "\n",
    "# enforce numeric types\n",
    "num_cols = [\"PR_AUC\",\"ROC_AUC\",\"BalancedAcc\",\"thr\",\"TP\",\"FP\",\"TN\",\"FN\"]\n",
    "for c in num_cols:\n",
    "    df_leader[c] = pd.to_numeric(df_leader[c], errors=\"coerce\")\n",
    "\n",
    "# sort by PR_AUC\n",
    "df_leader = df_leader.sort_values(\"PR_AUC\", ascending=False).reset_index(drop=True)\n",
    "print(df_leader.round(4))\n",
    "\n",
    "metrics = [\"PR_AUC\",\"ROC_AUC\",\"BalancedAcc\"]\n",
    "\n",
    "for m in metrics:\n",
    "    plt.figure(figsize=(9,5))\n",
    "    sns.barplot(data=df_leader.sort_values(m, ascending=False), x=m, y=\"Model\", color=\"gray\")\n",
    "    plt.title(f\"Model Comparison by {m}\")\n",
    "    plt.xlabel(m)\n",
    "    plt.ylabel(\"\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49142536",
   "metadata": {},
   "source": [
    "#### 6.8. Gradient Boosting (XGBoost or LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c546ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(\n",
    "        n_estimators=600, learning_rate=0.03, max_depth=6,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        reg_lambda=1.0, reg_alpha=0.0,\n",
    "        n_jobs=-1, random_state=42, tree_method=\"hist\"\n",
    "    )\n",
    "_ = xgb.fit(Xtr, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1723abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = xgb.predict_proba(Xva)[:,1]\n",
    "pick_f1  = pick_threshold_by_f1(yva, p_val)\n",
    "pick_rec = pick_threshold_by_recall(yva, p_val, recall_floor=0.10)\n",
    "\n",
    "p_te = xgb.predict_proba(Xte)[:,1]\n",
    "xgb_res_f1 = evaluate(yte, p_te, pick_f1[\"thr\"],  name=\"XGB | F1-opt\")\n",
    "xgb_res_rec= evaluate(yte, p_te, pick_rec[\"thr\"], name=\"XGB | Recall≥10%\")\n",
    "\n",
    "all_results += [xgb_res_f1, xgb_res_rec]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f2a8e3",
   "metadata": {},
   "source": [
    "#### 6.9. Feature Selection + Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- selectors ---\n",
    "def selector_anova(k): return SelectKBest(score_func=f_classif, k=k)\n",
    "def selector_mi(k): return SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "def selector_l1(C): \n",
    "    base = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", C=C, class_weight=\"balanced\", max_iter=2000)\n",
    "    return SelectFromModel(base)\n",
    "\n",
    "# --- candidate k values ---\n",
    "k_grid = [20, 40, 60, 100, 150]\n",
    "\n",
    "experiments = []\n",
    "\n",
    "# logistic regression with ANOVA/MI\n",
    "for k in k_grid:\n",
    "    for sel_name, sel_fn in [(\"ANOVA\", selector_anova), (\"MI\", selector_mi)]:\n",
    "        pipe = make_pipeline(StandardScaler(), sel_fn(k), LogisticRegression(max_iter=2000, class_weight=\"balanced\"))\n",
    "        pipe.fit(Xtr, ytr)\n",
    "        pv = pipe.predict_proba(Xva)[:,1]\n",
    "        ap = average_precision_score(yva, pv)\n",
    "        experiments.append({\"selector\": f\"{sel_name}-k{k}\", \"model\":\"Logistic\", \"val_ap\": ap, \"pipe\": pipe})\n",
    "\n",
    "# logistic regression with L1 selection\n",
    "for C in [0.1, 1.0]:\n",
    "    sel = selector_l1(C)\n",
    "    pipe = make_pipeline(StandardScaler(), sel, LogisticRegression(max_iter=2000, class_weight=\"balanced\"))\n",
    "    pipe.fit(Xtr, ytr)\n",
    "    pv = pipe.predict_proba(Xva)[:,1]\n",
    "    ap = average_precision_score(yva, pv)\n",
    "    experiments.append({\"selector\": f\"L1-C{C}\", \"model\":\"Logistic\", \"val_ap\": ap, \"pipe\": pipe})\n",
    "\n",
    "# pick top-3, retrain with MLP head\n",
    "top3 = sorted(experiments, key=lambda d: d[\"val_ap\"], reverse=True)[:3]\n",
    "for exp in top3:\n",
    "    label = exp[\"selector\"]\n",
    "    sel = exp[\"pipe\"].steps[1][1]  # reuse fitted selector\n",
    "    pipe = make_pipeline(StandardScaler(), sel, MLPClassifier(hidden_layer_sizes=(64,32), max_iter=600))\n",
    "    pipe.fit(Xtr, ytr)\n",
    "    pv = pipe.predict_proba(Xva)[:,1]\n",
    "    ap = average_precision_score(yva, pv)\n",
    "    experiments.append({\"selector\": label, \"model\":\"MLP\", \"val_ap\": ap, \"pipe\": pipe})\n",
    "\n",
    "# --- pick winner ---\n",
    "best = max(experiments, key=lambda d: d[\"val_ap\"])\n",
    "print(\"Best on validation:\", best[\"selector\"], best[\"model\"], \"Val PR-AUC:\", round(best[\"val_ap\"],4))\n",
    "\n",
    "# --- test evaluation ---\n",
    "pv = best[\"pipe\"].predict_proba(Xva)[:,1]\n",
    "thr = pick_threshold_by_f1(yva, pv)\n",
    "pt = best[\"pipe\"].predict_proba(Xte)[:,1]\n",
    "best_res = evaluate(yte, pt, thr[\"thr\"], name=f\"{best['model']} + {best['selector']}\")\n",
    "\n",
    "# store\n",
    "best_res[\"Model\"] = f\"{best['model']} + {best['selector']}\"\n",
    "all_results.append(best_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436834a7",
   "metadata": {},
   "source": [
    "#### 6.10. Ensembles: averaging + stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516dde90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Ensemble of LR, RF, MLP\n",
    "# ensure you have probas from previous models: proba_lr, proba_rf, proba_mlp\n",
    "avg_val = (lr_proba_val + rf_proba_val + mlp_proba_val) / 3\n",
    "avg_test = (lr_proba_te + rf_proba_te + mlp_proba_te) / 3\n",
    "\n",
    "thr_avg = pick_threshold_by_f1(yva, avg_val)\n",
    "avg_res_f1 = evaluate(yte, avg_test, thr_avg[\"thr\"], name=\"AvgEnsemble | F1-opt\")\n",
    "\n",
    "thr_avg_rec = pick_threshold_by_recall(yva, avg_val, recall_floor=0.10)\n",
    "avg_res_rec = evaluate(yte, avg_test, thr_avg_rec[\"thr\"], name=\"AvgEnsemble | Recall≥10%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b00ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results += [avg_res_f1, avg_res_rec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7990e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stacking Ensemble (meta-learner = Logistic Regression) ---\n",
    "Xval_stack = np.vstack([lr_proba_val, rf_proba_val, mlp_proba_val]).T\n",
    "Xte_stack  = np.vstack([lr_proba_te, rf_proba_te, mlp_proba_te]).T\n",
    "\n",
    "meta = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
    "meta.fit(Xval_stack, yva)\n",
    "stack_val = meta.predict_proba(Xval_stack)[:,1]\n",
    "stack_test = meta.predict_proba(Xte_stack)[:,1]\n",
    "\n",
    "thr_stack = pick_threshold_by_f1(yva, stack_val)\n",
    "stack_res_f1 = evaluate(yte, stack_test, thr_stack[\"thr\"], name=\"Stacking | F1-opt\")\n",
    "\n",
    "thr_stack_rec = pick_threshold_by_recall(yva, stack_val, recall_floor=0.10)\n",
    "stack_res_rec = evaluate(yte, stack_test, thr_stack_rec[\"thr\"], name=\"Stacking | Recall≥10%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c96d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results += [stack_res_f1, stack_res_rec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fd9823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required columns are already in all_results\n",
    "cols = [\"Model\",\"PR_AUC\",\"ROC_AUC\",\"BalancedAcc\",\"thr\",\"TP\",\"FP\",\"TN\",\"FN\"]\n",
    "\n",
    "df_leader = (pd.DataFrame(all_results)[cols]\n",
    "             .drop_duplicates()\n",
    "             .sort_values(\"PR_AUC\", ascending=False)\n",
    "             .reset_index(drop=True))\n",
    "\n",
    "print(\"Final Leaderboard (sorted by PR-AUC):\")\n",
    "display(df_leader.round(4))\n",
    "\n",
    "# save\n",
    "df_leader.to_csv(RESULT_DIR/\"modeling_final_leaderboard.csv\", index=False)\n",
    "\n",
    "# quick plots\n",
    "plt.figure(figsize=(9,5))\n",
    "sns.barplot(data=df_leader, x=\"PR_AUC\", y=\"Model\", color=\"gray\")\n",
    "plt.title(\"Model Comparison by PR-AUC\")\n",
    "plt.xlabel(\"PR-AUC\")\n",
    "plt.ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "savefig(\"01_model_leaderboard_pr_auc.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "sns.barplot(data=df_leader.sort_values(\"ROC_AUC\", ascending=False),\n",
    "            x=\"ROC_AUC\", y=\"Model\", color=\"gray\")\n",
    "plt.title(\"Model Comparison by ROC-AUC\")\n",
    "plt.xlabel(\"ROC-AUC\")\n",
    "plt.ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "savefig(\"02_model_leaderboard_roc_auc.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864307f0",
   "metadata": {},
   "source": [
    "#### 6.11. Save all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f254b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(lr, MODEL_DIR/\"logistic.pkl\")\n",
    "joblib.dump(rf, MODEL_DIR/\"random_forest.pkl\")\n",
    "joblib.dump(xgb, MODEL_DIR/\"xgb.pkl\")\n",
    "\n",
    "# feature-selection winner\n",
    "best_pipe = best['pipe']\n",
    "joblib.dump(best_pipe, MODEL_DIR/\"feature_select_winner.pkl\")\n",
    "\n",
    "# stacking meta\n",
    "joblib.dump(meta, MODEL_DIR/\"stack_meta.pkl\")\n",
    "\n",
    "# torch MLP\n",
    "torch.save(mlp_model.state_dict(), MODEL_DIR/\"mlp_state.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
