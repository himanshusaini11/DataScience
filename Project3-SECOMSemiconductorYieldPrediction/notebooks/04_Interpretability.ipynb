{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c53898b8",
   "metadata": {},
   "source": [
    "# SECOM Yield Prediction — End-to-End Notebook\n",
    "\n",
    "**Goal:** Predict *Fail* outcomes from process measurements to reduce scrap and downtime.\n",
    "\n",
    "**Data:** `data/secom.data`, `data/secom_labels.data`, `data/secom.names` (UCI ML Repository, real fab data).\n",
    "\n",
    "**Primary metric:** Recall on *Fail* at acceptable precision. Report PR-AUC and Balanced Error Rate (BER)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b078f5",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eaac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Global style for plots\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 12,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"axes.edgecolor\": \"black\",\n",
    "    \"axes.linewidth\": 1,\n",
    "    \"xtick.color\": \"black\",\n",
    "    \"ytick.color\": \"black\",\n",
    "    \"xtick.direction\": \"out\",\n",
    "    \"ytick.direction\": \"out\",\n",
    "    \"xtick.bottom\": True,\n",
    "    \"ytick.left\": True,\n",
    "    \"xtick.top\": False,\n",
    "    \"ytick.right\": False,\n",
    "    \"figure.dpi\": 150,\n",
    "    \"legend.frameon\": True,\n",
    "    \"legend.facecolor\": \"white\",\n",
    "    \"legend.edgecolor\": \"black\",\n",
    "    \"legend.fontsize\": 12\n",
    "})\n",
    "\n",
    "# Apply to seaborn\n",
    "sns.set_theme(context=\"notebook\", style=\"ticks\")\n",
    "\n",
    "# Reproducibility\n",
    "rnd_num = 42\n",
    "rng = np.random.seed(rnd_num)\n",
    "\n",
    "# Root directory\n",
    "ROOT = Path(os.getcwd()).parent\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = Path(ROOT/\"data\")\n",
    "RAW = Path(DATA_DIR/\"raw\")\n",
    "assert (RAW/\"secom.data\").exists() and (RAW/\"secom_labels.data\").exists(), \"Data files are missing!\"\n",
    "\n",
    "# Results directory\n",
    "RESULT_DIR = Path(ROOT/\"results\")\n",
    "RESULT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Model directory\n",
    "MODEL_DIR = Path(ROOT/\"models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a179bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure saver helper (use after each plot)\n",
    "RESULT_DIR_IR = Path(RESULT_DIR/\"interpretability\")\n",
    "RESULT_DIR_IR.mkdir(exist_ok=True, parents=True)\n",
    "def savefig(name):\n",
    "    out = RESULT_DIR_IR/name\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out, dpi=300, bbox_inches=\"tight\")\n",
    "    print(\"Saved:\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d3fc51",
   "metadata": {},
   "source": [
    "### 2. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6511a022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "import shap\n",
    "from sklearn.metrics import (precision_recall_curve, average_precision_score,\n",
    "                             roc_auc_score, balanced_accuracy_score,\n",
    "                             confusion_matrix, classification_report,\n",
    "                             brier_score_loss, roc_curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65aca09",
   "metadata": {},
   "source": [
    "### 3. Reload data from `03_Modeling.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b9d7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths\n",
    "PROC = Path(DATA_DIR/\"processed\")\n",
    "MODELS = Path(ROOT/\"models\")\n",
    "\n",
    "# Reload data\n",
    "tr = pd.read_parquet(PROC/\"train.parquet\")\n",
    "va = pd.read_parquet(PROC/\"val.parquet\")\n",
    "te = pd.read_parquet(PROC/\"test.parquet\")\n",
    "feat_cols = [c for c in tr.columns if c not in (\"label\",\"timestamp\")]\n",
    "\n",
    "Xva, yva = va[feat_cols].to_numpy(dtype=np.float32), va[\"label\"].to_numpy(dtype=np.int8)\n",
    "Xte, yte = te[feat_cols].to_numpy(dtype=np.float32), te[\"label\"].to_numpy(dtype=np.int8)\n",
    "\n",
    "# Reload models\n",
    "lr   = joblib.load(MODELS/\"logistic.pkl\")\n",
    "rf   = joblib.load(MODELS/\"random_forest.pkl\")\n",
    "xgb  = joblib.load(MODELS/\"xgb.pkl\")\n",
    "best_sel_pipe = joblib.load(MODELS/\"feature_select_winner.pkl\")\n",
    "meta = joblib.load(MODELS/\"stack_meta.pkl\")\n",
    "\n",
    "# Reload MLP (torch)\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, d_in):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_in, 256), torch.nn.ReLU(), torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(256, 128), torch.nn.ReLU(), torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(128, 1)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "mlp_model = MLP(len(feat_cols))\n",
    "mlp_model.load_state_dict(torch.load(MODELS/\"mlp_state.pt\", map_location=\"cpu\"))\n",
    "mlp_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b9fc0d",
   "metadata": {},
   "source": [
    "### 4. Evaluation curves: PR & ROC for all saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3413371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proba_sklearn(model, X):\n",
    "    # works for LR, RF, XGB, pipelines, meta\n",
    "    return model.predict_proba(X)[:, 1].astype(\"float64\")\n",
    "\n",
    "def proba_mlp(mlp, X):\n",
    "    device = next(mlp.parameters()).device if list(mlp.parameters()) else torch.device(\"cpu\")\n",
    "    with torch.no_grad():\n",
    "        Xt = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "        p  = torch.sigmoid(mlp(Xt)).cpu().numpy().ravel()\n",
    "    return p.astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b70803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect model probabilities on TEST\n",
    "curves = {}\n",
    "\n",
    "try:    curves[\"Logistic\"] = proba_sklearn(lr, Xte)\n",
    "except: pass\n",
    "try:    curves[\"RandomForest\"] = proba_sklearn(rf, Xte)\n",
    "except: pass\n",
    "try:    curves[\"XGBoost\"] = proba_sklearn(xgb, Xte)\n",
    "except: pass\n",
    "try:    curves[\"FeatureSel (best pipe)\"] = proba_sklearn(best_sel_pipe, Xte)\n",
    "except: pass\n",
    "try:    curves[\"MLP\"] = proba_mlp(mlp_model, Xte)\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c404a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally include stacking/meta if base probas are available:\n",
    "try:\n",
    "    # rebuild meta inputs using the same base models as training time\n",
    "    lr_p  = proba_sklearn(lr, Xte)\n",
    "    rf_p  = proba_sklearn(rf, Xte)\n",
    "    mlp_p = proba_mlp(mlp_model, Xte)\n",
    "    Xte_stack = np.vstack([lr_p, rf_p, mlp_p]).T\n",
    "    curves[\"Stacking (meta)\"] = proba_sklearn(meta, Xte_stack)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd133545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plot Precision–Recall\n",
    "plt.figure(figsize=(7.5, 5.5))\n",
    "for name, p in curves.items():\n",
    "    prec, rec, _ = precision_recall_curve(yte, p)\n",
    "    ap = average_precision_score(yte, p)\n",
    "    plt.step(rec, prec, where=\"post\", label=f\"{name} (AP={ap:.3f})\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision–Recall (Test)\")\n",
    "plt.legend(frameon=True)\n",
    "plt.tight_layout()\n",
    "savefig(\"01_pr_curve_test.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9897819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plot ROC\n",
    "plt.figure(figsize=(7.5, 5.5))\n",
    "for name, p in curves.items():\n",
    "    fpr, tpr, _ = roc_curve(yte, p)\n",
    "    auc = roc_auc_score(yte, p)\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC={auc:.3f})\")\n",
    "plt.plot([0,1],[0,1],\"--\", lw=1, color=\"gray\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC (Test)\")\n",
    "plt.legend(frameon=True)\n",
    "plt.tight_layout()\n",
    "savefig(\"02_roc_curve_test.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fb0694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Threshold -> Precision/Recall for the best PR-AUC model\n",
    "aps = {name: average_precision_score(yte, p) for name, p in curves.items()}\n",
    "best_name = max(aps, key=aps.get)\n",
    "best_p = curves[best_name]\n",
    "prec, rec, thr = precision_recall_curve(yte, best_p)\n",
    "thr_full = np.r_[thr, 1.0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.5, 5.0))\n",
    "ax.plot(thr_full, prec, label=\"Precision\")\n",
    "ax.plot(thr_full, rec, label=\"Recall\")\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(f\"Precision/Recall vs Threshold — {best_name} (AP={aps[best_name]:.3f})\")\n",
    "ax.legend(frameon=True)\n",
    "plt.tight_layout()\n",
    "savefig(f\"03_pr_recall_vs_threshold_{best_name.lower().replace(' ','_')}.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Included models:\", list(curves.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa702168",
   "metadata": {},
   "source": [
    "#### 3. SHAP (SHapley Additive exPlanations) Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f872007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def pr_align(y, p):\n",
    "    P, R, T = precision_recall_curve(y, p); return P[:-1], R[:-1], T\n",
    "def thr_f1(y, p, beta=1.0):\n",
    "    P, R, T = pr_align(y, p); F = (1+beta**2)*(P*R)/(beta**2*P+R+1e-12)\n",
    "    i = np.nanargmax(F); return float(T[i])\n",
    "def thr_recall(y, p, floor=0.10):\n",
    "    P, R, T = pr_align(y, p); idx = np.where(R>=floor)[0]\n",
    "    return float(T[idx[np.argmax(P[idx])]]) if idx.size else float(T[-1])\n",
    "def eval_at(name, y, p, thr):\n",
    "    pred = (p>=thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "    return dict(Model=name, thr=float(thr),\n",
    "                PR_AUC=average_precision_score(y, p),\n",
    "                ROC_AUC=roc_auc_score(y, p),\n",
    "                BalancedAcc=balanced_accuracy_score(y, pred),\n",
    "                TP=int(tp), FP=int(fp), TN=int(tn), FN=int(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d4efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather candidates robustly (uses whatever exists)\n",
    "cands = []  # (name, p_val, p_test)\n",
    "def add_if(name, p_val, p_test):\n",
    "    if p_val is None or p_test is None: return\n",
    "    if len(p_val)==len(yva) and len(p_test)==len(yte):\n",
    "        cands.append((name, np.asarray(p_val), np.asarray(p_test)))\n",
    "\n",
    "try: add_if(\"Logistic\", lr_proba_val, lr_proba_te)\n",
    "except NameError: pass\n",
    "try: add_if(\"RandomForest\", rf_proba_val, rf_proba_te)\n",
    "except NameError: pass\n",
    "try: add_if(\"XGBoost\", p_val, p_te)             # from your XGB section\n",
    "except NameError: pass\n",
    "try: add_if(\"MLP\", mlp_proba_val, mlp_proba_te)\n",
    "except NameError: pass\n",
    "try: add_if(\"AvgEnsemble\", (lr_proba_val+rf_proba_val+mlp_proba_val)/3, (lr_proba_te +rf_proba_te +mlp_proba_te )/3)\n",
    "except NameError: pass\n",
    "try: add_if(\"FeatureSel (best pipe)\",\n",
    "            best[\"pipe\"].predict_proba(Xva)[:,1],\n",
    "            best[\"pipe\"].predict_proba(Xte)[:,1])\n",
    "except Exception: pass\n",
    "try: add_if(\"Stacking (meta)\", stack_val, stack_test)\n",
    "except NameError: pass\n",
    "\n",
    "print(\"Included models:\", [n for n,_,_ in cands])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ccd80b",
   "metadata": {},
   "source": [
    "#### 3.2. Cross-model evaluation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8615fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect per-model probabilities on VALIDATION and TEST\n",
    "val_probs, test_probs = {}, {}\n",
    "\n",
    "def add_model(name, prob_val_fn, prob_test_fn):\n",
    "    try:\n",
    "        pv = prob_val_fn()\n",
    "        pt = prob_test_fn()\n",
    "        if (len(pv)==len(yva)) and (len(pt)==len(yte)):\n",
    "            val_probs[name]  = pv.astype(\"float64\")\n",
    "            test_probs[name] = pt.astype(\"float64\")\n",
    "    except Exception as e:\n",
    "        print(f\"[skip] {name}: {e}\")\n",
    "\n",
    "# base models\n",
    "add_model(\"Logistic\",\n",
    "          lambda: proba_sklearn(lr, Xva),\n",
    "          lambda: proba_sklearn(lr, Xte))\n",
    "\n",
    "add_model(\"RandomForest\",\n",
    "          lambda: proba_sklearn(rf, Xva),\n",
    "          lambda: proba_sklearn(rf, Xte))\n",
    "\n",
    "add_model(\"XGBoost\",\n",
    "          lambda: proba_sklearn(xgb, Xva),\n",
    "          lambda: proba_sklearn(xgb, Xte))\n",
    "\n",
    "add_model(\"FeatureSel (best pipe)\",\n",
    "          lambda: proba_sklearn(best_sel_pipe, Xva),\n",
    "          lambda: proba_sklearn(best_sel_pipe, Xte))\n",
    "\n",
    "add_model(\"MLP\",\n",
    "          lambda: proba_mlp(mlp_model, Xva),\n",
    "          lambda: proba_mlp(mlp_model, Xte))\n",
    "\n",
    "# stacking/meta (rebuild meta inputs exactly like in Section 2)\n",
    "def stack_val():\n",
    "    lr_p  = proba_sklearn(lr, Xva)\n",
    "    rf_p  = proba_sklearn(rf, Xva)\n",
    "    mlp_p = proba_mlp(mlp_model, Xva)\n",
    "    return meta.predict_proba(np.vstack([lr_p, rf_p, mlp_p]).T)[:,1]\n",
    "\n",
    "def stack_test():\n",
    "    lr_p  = proba_sklearn(lr, Xte)\n",
    "    rf_p  = proba_sklearn(rf, Xte)\n",
    "    mlp_p = proba_mlp(mlp_model, Xte)\n",
    "    return meta.predict_proba(np.vstack([lr_p, rf_p, mlp_p]).T)[:,1]\n",
    "\n",
    "add_model(\"Stacking (meta)\", stack_val, stack_test)\n",
    "\n",
    "print(\"Included models:\", list(test_probs.keys()))\n",
    "\n",
    "# --- helpers (thresholds & metrics) ---\n",
    "\n",
    "def _align_pr(y, p):\n",
    "    P, R, T = precision_recall_curve(y, p)\n",
    "    return P[:-1], R[:-1], T  # align thresholds\n",
    "\n",
    "def thr_f1(y, p, beta=1.0):\n",
    "    P, R, T = _align_pr(y, p)\n",
    "    F = (1+beta**2)*(P*R)/(beta**2*P + R + 1e-12)\n",
    "    i = int(np.nanargmax(F))\n",
    "    return float(T[i])\n",
    "\n",
    "def thr_recall_floor(y, p, floor=0.10):\n",
    "    P, R, T = _align_pr(y, p)\n",
    "    ok = np.where(R >= floor)[0]\n",
    "    if ok.size == 0:  # fallback: F1\n",
    "        return thr_f1(y, p)\n",
    "    j = ok[np.argmax(P[ok])]\n",
    "    return float(T[j])\n",
    "\n",
    "def eval_at(name, y, p, thr):\n",
    "    pred = (p >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "    return dict(\n",
    "        Model=name, thr=float(thr),\n",
    "        PR_AUC=average_precision_score(y, p),\n",
    "        ROC_AUC=roc_auc_score(y, p),\n",
    "        BalancedAcc=balanced_accuracy_score(y, pred),\n",
    "        TP=int(tp), FP=int(fp), TN=int(tn), FN=int(fn)\n",
    "    )\n",
    "\n",
    "# --- build table for two policies using VAL to pick thr, TEST to report ---\n",
    "rows = []\n",
    "for name in test_probs.keys():\n",
    "    pv, pt = val_probs[name], test_probs[name]\n",
    "    t1 = thr_f1(yva, pv)\n",
    "    t2 = thr_recall_floor(yva, pv, floor=0.10)\n",
    "    rows.append({**eval_at(name, yte, pt, t1), \"Policy\":\"F1-opt\"})\n",
    "    rows.append({**eval_at(name, yte, pt, t2), \"Policy\":\"Recall≥10%\"})\n",
    "\n",
    "if rows:\n",
    "    df_eval = (pd.DataFrame(rows)\n",
    "               .sort_values([\"Policy\",\"PR_AUC\"], ascending=[True, False])\n",
    "               .reset_index(drop=True))\n",
    "    display(df_eval[[\"Model\",\"Policy\",\"PR_AUC\",\"ROC_AUC\",\"BalancedAcc\",\"thr\",\"TP\",\"FP\",\"TN\",\"FN\"]].round(4))\n",
    "else:\n",
    "    print(\"No models produced probabilities — nothing to evaluate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee00f5",
   "metadata": {},
   "source": [
    "#### 3.3. SHAP Interpretability (RandomForest + XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0666c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset once\n",
    "shap.initjs()\n",
    "rng = np.random.default_rng(42)\n",
    "Xte_df  = pd.DataFrame(Xte, columns=feat_cols)\n",
    "idx_sub = rng.choice(len(Xte_df), size=min(300, len(Xte_df)), replace=False)\n",
    "Xte_sub = Xte_df.iloc[idx_sub]\n",
    "\n",
    "def shap_pos_class(vals, n_feats):\n",
    "    \"\"\"Return class-1 SHAP matrix aligned to Xte_sub (n_samples, n_features).\"\"\"\n",
    "    out = vals[1] if isinstance(vals, list) else np.asarray(vals)\n",
    "    # drop optional bias column if present\n",
    "    if out.ndim == 3 and out.shape[-1] >= 2:\n",
    "        out = out[:, :, 1]\n",
    "    if out.shape[1] == n_feats + 1:\n",
    "        out = out[:, :-1]\n",
    "    if out.shape[1] != n_feats:          # final safeguard\n",
    "        out = out[:, :n_feats]\n",
    "    return out\n",
    "\n",
    "# --- RandomForest ---\n",
    "expl_rf  = shap.TreeExplainer(rf)\n",
    "shap_rf  = shap_pos_class(expl_rf.shap_values(Xte_sub), Xte_sub.shape[1])\n",
    "imp_rf   = pd.Series(np.abs(shap_rf).mean(axis=0), index=Xte_sub.columns)\n",
    "top_rf   = imp_rf.nlargest(3).index.tolist()\n",
    "\n",
    "shap.summary_plot(shap_rf, Xte_sub, feature_names=Xte_sub.columns, plot_type=\"bar\", show=False)\n",
    "savefig(\"04_shap_rf_summary_bar.png\")\n",
    "plt.show()\n",
    "\n",
    "shap.summary_plot(shap_rf, Xte_sub, feature_names=Xte_sub.columns, show=False)\n",
    "savefig(\"05_shap_rf_beeswarm.png\")\n",
    "plt.show()\n",
    "\n",
    "for f in top_rf:\n",
    "    shap.dependence_plot(f, shap_rf, Xte_sub, interaction_index=None, show=True)\n",
    "\n",
    "# --- XGBoost ---\n",
    "expl_xgb = shap.TreeExplainer(xgb)\n",
    "shap_xgb = shap_pos_class(expl_xgb.shap_values(Xte_sub), Xte_sub.shape[1])\n",
    "imp_xgb  = pd.Series(np.abs(shap_xgb).mean(axis=0), index=Xte_sub.columns)\n",
    "top_xgb  = imp_xgb.nlargest(3).index.tolist()\n",
    "\n",
    "shap.summary_plot(shap_xgb, Xte_sub, feature_names=Xte_sub.columns, plot_type=\"bar\", show=False)\n",
    "savefig(\"06_shap_xgb_summary_bar.png\")\n",
    "plt.show()\n",
    "\n",
    "shap.summary_plot(shap_xgb, Xte_sub, feature_names=Xte_sub.columns, show=False)\n",
    "savefig(\"07_shap_xgb_beeswarm.png\")\n",
    "plt.show()\n",
    "\n",
    "for f in top_xgb:\n",
    "    shap.dependence_plot(f, shap_xgb, Xte_sub, interaction_index=None, show=True)\n",
    "\n",
    "# --- Optional: agreement / tables ---\n",
    "def topk_table(shap_vals, names, k=10):\n",
    "    mv  = np.abs(shap_vals).mean(axis=0)\n",
    "    idx = np.argsort(mv)[::-1][:k]\n",
    "    return pd.DataFrame({\"feature\": np.array(names)[idx], \"mean|SHAP|\": mv[idx]})\n",
    "\n",
    "top15_rf = set(imp_rf.nlargest(15).index)\n",
    "top15_xg = set(imp_xgb.nlargest(15).index)\n",
    "jaccard  = len(top15_rf & top15_xg) / len(top15_rf | top15_xg)\n",
    "print(\"Top-15 overlap:\", sorted(top15_rf & top15_xg))\n",
    "print(f\"Jaccard (Top-15): {jaccard:.2f}\")\n",
    "\n",
    "print(\"\\nTop-10 (RF):\")\n",
    "print(topk_table(shap_rf, Xte_sub.columns, 10).to_string(index=False))\n",
    "print(\"\\nTop-10 (XGB):\")\n",
    "print(topk_table(shap_xgb, Xte_sub.columns, 10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5d4e96",
   "metadata": {},
   "source": [
    "#### 3.4 Cost-Sensitive Evaluation (validation-tuned thresholds --> reported on test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f412bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- helpers ----\n",
    "def proba_sklearn(model, X):\n",
    "    return model.predict_proba(X)[:, 1].astype(\"float64\")\n",
    "\n",
    "def proba_mlp(mlp, X):\n",
    "    device = next(mlp.parameters()).device if list(mlp.parameters()) else torch.device(\"cpu\")\n",
    "    with torch.no_grad():\n",
    "        Xt = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "        p  = torch.sigmoid(mlp(Xt)).cpu().numpy().ravel()\n",
    "    return p.astype(\"float64\")\n",
    "\n",
    "def best_thr_by_cost(y, p, C_fp=1, C_fn=10):\n",
    "    # choose threshold on *validation* that minimizes expected cost = C_fp*FP + C_fn*FN\n",
    "    P, R, T = precision_recall_curve(y, p)   # T has length n-1\n",
    "    Tfull = np.unique(np.r_[0.0, T, 1.0])    # include extremes\n",
    "    costs, thrs = [], []\n",
    "    for thr in Tfull:\n",
    "        pred = (p >= thr).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "        costs.append(C_fp*fp + C_fn*fn); thrs.append(thr)\n",
    "    j = int(np.argmin(costs))\n",
    "    return float(thrs[j])\n",
    "\n",
    "def eval_at(y, p, thr, C_fp=1, C_fn=10):\n",
    "    pred = (p >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "    return {\n",
    "        \"thr\": float(thr),\n",
    "        \"Cost\": int(C_fp*fp + C_fn*fn),\n",
    "        \"PR_AUC\": float(average_precision_score(y, p)),\n",
    "        \"ROC_AUC\": float(roc_auc_score(y, p)),\n",
    "        \"BalancedAcc\": float(balanced_accuracy_score(y, pred)),\n",
    "        \"Precision\": float(tp/(tp+fp) if (tp+fp)>0 else 0.0),\n",
    "        \"Recall\": float(tp/(tp+fn) if (tp+fn)>0 else 0.0),\n",
    "        \"TP\": int(tp), \"FP\": int(fp), \"TN\": int(tn), \"FN\": int(fn),\n",
    "    }\n",
    "\n",
    "# ---- collect validation/test probabilities for all loaded models ----\n",
    "cands = {}\n",
    "\n",
    "try: cands[\"Logistic\"] = (proba_sklearn(lr, Xva), proba_sklearn(lr, Xte))\n",
    "except: pass\n",
    "try: cands[\"RandomForest\"] = (proba_sklearn(rf, Xva), proba_sklearn(rf, Xte))\n",
    "except: pass\n",
    "try: cands[\"XGBoost\"] = (proba_sklearn(xgb, Xva), proba_sklearn(xgb, Xte))\n",
    "except: pass\n",
    "try: cands[\"FeatureSel (best pipe)\"] = (proba_sklearn(best_sel_pipe, Xva), proba_sklearn(best_sel_pipe, Xte))\n",
    "except: pass\n",
    "try: cands[\"MLP\"] = (proba_mlp(mlp_model, Xva), proba_mlp(mlp_model, Xte))\n",
    "except: pass\n",
    "# Stacking (rebuild inputs from base models)\n",
    "try:\n",
    "    lr_v, rf_v, mlp_v = proba_sklearn(lr, Xva), proba_sklearn(rf, Xva), proba_mlp(mlp_model, Xva)\n",
    "    lr_t, rf_t, mlp_t = proba_sklearn(lr, Xte), proba_sklearn(rf, Xte), proba_mlp(mlp_model, Xte)\n",
    "    Xva_stack = np.vstack([lr_v, rf_v, mlp_v]).T\n",
    "    Xte_stack = np.vstack([lr_t, rf_t, mlp_t]).T\n",
    "    cands[\"Stacking (meta)\"] = (proba_sklearn(meta, Xva_stack), proba_sklearn(meta, Xte_stack))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"Cost-sensitive eval — included models:\", list(cands.keys()))\n",
    "\n",
    "# ---- sweep FN:FP ratios, tune on val, report on test ----\n",
    "ratios = [5, 10, 20]  # adjust if you want a denser sweep\n",
    "rows = []\n",
    "for name, (p_val, p_test) in cands.items():\n",
    "    for r in ratios:\n",
    "        thr = best_thr_by_cost(yva, p_val, C_fp=1, C_fn=r)\n",
    "        res = eval_at(yte, p_test, thr, C_fp=1, C_fn=r)\n",
    "        rows.append({\"Model\": name, \"FN:FP\": f\"{r}:1\", **res})\n",
    "\n",
    "df_cost = (pd.DataFrame(rows)\n",
    "           .sort_values([\"FN:FP\",\"Cost\",\"Model\"])\n",
    "           .reset_index(drop=True))\n",
    "\n",
    "display(df_cost[[\"Model\",\"FN:FP\",\"Cost\",\"thr\",\"Precision\",\"Recall\",\"BalancedAcc\",\"PR_AUC\",\"ROC_AUC\",\"TP\",\"FP\",\"TN\",\"FN\"]]\n",
    "        .round({\"thr\":4,\"Precision\":3,\"Recall\":3,\"BalancedAcc\":3,\"PR_AUC\":3,\"ROC_AUC\":3}))\n",
    "\n",
    "# ---- (optional) per-ratio winner summary ----\n",
    "winners = (df_cost.loc[df_cost.groupby(\"FN:FP\")[\"Cost\"].idxmin(), [\"FN:FP\",\"Model\",\"Cost\",\"thr\",\"Precision\",\"Recall\"]]\n",
    "           .reset_index(drop=True))\n",
    "print(\"\\nPer-ratio minimum-cost model:\")\n",
    "print(winners.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86f50d1",
   "metadata": {},
   "source": [
    "#### 3.5. Cost-Ratio Sweep (FN:FP = 1 --> 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a557ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 3.5 — Cost-Ratio Sweep with stable crossovers =====\n",
    "# Config\n",
    "BASELINE  = \"Logistic\"   # change if you want a different reference\n",
    "MAX_RATIO = 50          # supports 1:300\n",
    "STEP      = 1\n",
    "K_STABLE  = 3            # require K consecutive ratios cheaper than baseline\n",
    "\n",
    "# Helpers\n",
    "def best_cost_for_ratio(y_true, p, ratio):\n",
    "    prec, rec, thr = precision_recall_curve(y_true, p)\n",
    "    thr_grid = np.unique(np.r_[thr, 0.0, 1.0])\n",
    "    best = None\n",
    "    for t in thr_grid:\n",
    "        pred = (p >= t).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, pred).ravel()\n",
    "        cost = fp*1 + fn*ratio\n",
    "        if (best is None) or (cost < best[\"Cost\"]):\n",
    "            best = {\n",
    "                \"thr\": float(t),\n",
    "                \"Cost\": int(cost),\n",
    "                \"Precision\": tp/(tp+fp) if (tp+fp) > 0 else 0.0,\n",
    "                \"Recall\":    tp/(tp+fn) if (tp+fn) > 0 else 0.0,\n",
    "                \"TP\": int(tp), \"FP\": int(fp), \"TN\": int(tn), \"FN\": int(fn)\n",
    "            }\n",
    "    return best\n",
    "\n",
    "# Collect model->proba from your 'curves' dict (built earlier in 3.2)\n",
    "models_cost = {name: proba for name, proba in curves.items()}\n",
    "\n",
    "# Sweep\n",
    "ratios = list(range(1, MAX_RATIO + 1, STEP))\n",
    "rows = []\n",
    "for name, proba in models_cost.items():\n",
    "    for r in ratios:\n",
    "        best = best_cost_for_ratio(yte, proba, r)\n",
    "        best.update({\"Model\": name, \"Ratio\": r})\n",
    "        rows.append(best)\n",
    "df_cost = pd.DataFrame(rows)\n",
    "\n",
    "# Plot cost curves\n",
    "plt.figure(figsize=(10,5.5))\n",
    "for name in df_cost[\"Model\"].unique():\n",
    "    sub = df_cost[df_cost[\"Model\"] == name]\n",
    "    plt.plot(sub[\"Ratio\"], sub[\"Cost\"], label=name)\n",
    "plt.xlabel(\"FN:FP cost ratio\")\n",
    "plt.ylabel(\"Min expected cost\")\n",
    "plt.title(\"Cost vs FN:FP ratio (Test set)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.4)\n",
    "plt.tight_layout()\n",
    "savefig(\"08_cost_vs_fn_fp_ratio.png\")\n",
    "plt.show()\n",
    "\n",
    "# Winners per ratio\n",
    "winners = (df_cost.loc[df_cost.groupby(\"Ratio\")[\"Cost\"].idxmin()]\n",
    "           [[\"Ratio\",\"Model\",\"Cost\",\"thr\",\"Precision\",\"Recall\"]])\n",
    "print(\"Per-ratio minimum-cost models:\")\n",
    "print(winners.to_string(index=False))\n",
    "\n",
    "# Stable crossover finder vs baseline\n",
    "def crossovers_vs_baseline(df_cost, baseline, ratios, k=K_STABLE):\n",
    "    if baseline not in df_cost[\"Model\"].unique():\n",
    "        print(f\"[warn] Baseline '{baseline}' not found among models.\")\n",
    "        return pd.DataFrame(columns=[\"Model\",\"Crossover_at_ratio\"])\n",
    "    base = (df_cost[df_cost.Model == baseline]\n",
    "            .set_index(\"Ratio\")[\"Cost\"]\n",
    "            .reindex(ratios))\n",
    "\n",
    "    out = []\n",
    "    for name in df_cost[\"Model\"].unique():\n",
    "        if name == baseline:\n",
    "            continue\n",
    "        other = (df_cost[df_cost.Model == name]\n",
    "                 .set_index(\"Ratio\")[\"Cost\"]\n",
    "                 .reindex(ratios))\n",
    "        diff = (other - base).to_numpy()  # negative => other cheaper\n",
    "\n",
    "        # 1) Stable crossover: first index where k consecutive diffs < 0\n",
    "        stable_idx = None\n",
    "        if len(diff) >= k:\n",
    "            run = np.convolve((diff < 0).astype(int), np.ones(k, dtype=int), mode=\"valid\")\n",
    "            hits = np.where(run == k)[0]\n",
    "            if hits.size:\n",
    "                stable_idx = hits[0]  # window start\n",
    "        if stable_idx is not None:\n",
    "            out.append({\"Model\": name, \"Crossover_at_ratio\": ratios[stable_idx]})\n",
    "            continue\n",
    "\n",
    "        # 2) Fallback: classic sign change (any crossing)\n",
    "        sign = np.sign(diff)\n",
    "        flip = np.where(sign[:-1] * sign[1:] < 0)[0]\n",
    "        if flip.size:\n",
    "            out.append({\"Model\": name, \"Crossover_at_ratio\": ratios[flip[0] + 1]})\n",
    "            continue\n",
    "\n",
    "        # 3) Dominance summaries\n",
    "        if np.all(diff < 0):\n",
    "            out.append({\"Model\": name, \"Crossover_at_ratio\": f\"always cheaper (1→{MAX_RATIO})\"})\n",
    "        elif np.all(diff > 0):\n",
    "            out.append({\"Model\": name, \"Crossover_at_ratio\": f\"always costlier (1→{MAX_RATIO})\"})\n",
    "        else:\n",
    "            out.append({\"Model\": name, \"Crossover_at_ratio\": \"no clean crossover\"})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "def pretty_print_crossovers(df, baseline, max_ratio=MAX_RATIO):\n",
    "    print(f\"\\nStable crossovers vs baseline: {baseline}\")\n",
    "    for _, row in df.iterrows():\n",
    "        model = row[\"Model\"]\n",
    "        val   = row[\"Crossover_at_ratio\"]\n",
    "        if isinstance(val, (int, np.integer, float, np.floating)):\n",
    "            r = int(round(val))\n",
    "            print(f\"Crossover {model}: FN:FP ≈ {r}:1\")\n",
    "        else:\n",
    "            s = str(val)\n",
    "            if \"always cheaper\" in s:\n",
    "                print(f\"Crossover {model}: always cheaper across 1–{max_ratio}:1\")\n",
    "            elif \"always costlier\" in s:\n",
    "                print(f\"Crossover {model}: always costlier across 1–{max_ratio}:1\")\n",
    "            else:\n",
    "                print(f\"Crossover {model}: {s}\")\n",
    "\n",
    "res = crossovers_vs_baseline(df_cost, BASELINE, ratios)\n",
    "pretty_print_crossovers(res, BASELINE, MAX_RATIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d785b",
   "metadata": {},
   "source": [
    "#### 3.6 — Bootstrap Confidence Intervals (PR-AUC, ROC-AUC, BalancedAcc, Precision/Recall @ fixed thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d69521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses validation to fix thresholds (F1-opt and Recall≥10%), then bootstraps the TEST set.\n",
    "B = 1000                      # bootstrap replicates (reduce to 300 for speed)\n",
    "RECALL_TARGET = 0.10\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# --- helper: fixed thresholds from validation ---\n",
    "def thr_f1_from_val(y, p):\n",
    "    prec, rec, thr = precision_recall_curve(y, p)\n",
    "    # f1 defined for points with a threshold (skip the last PR point without thr)\n",
    "    f1 = (2*prec[:-1]*rec[:-1]) / (prec[:-1] + rec[:-1] + 1e-12)\n",
    "    return float(thr[np.nanargmax(f1)]) if len(thr) else 0.5\n",
    "\n",
    "def thr_recall_from_val(y, p, target=RECALL_TARGET):\n",
    "    # smallest threshold that achieves >= target recall on validation\n",
    "    prec, rec, thr = precision_recall_curve(y, p)\n",
    "    mask = rec[:-1] >= target\n",
    "    if mask.any():\n",
    "        # choose the largest threshold that still keeps recall >= target (more precision)\n",
    "        idx = np.where(mask)[0][-1]\n",
    "        return float(thr[idx])\n",
    "    # fallback: guarantee at least one positive by taking min threshold\n",
    "    return float(np.min(p) - 1e-12)\n",
    "\n",
    "# --- collect validation probabilities (to set thresholds) ---\n",
    "def proba_of(model, X):\n",
    "    try:\n",
    "        return proba_sklearn(model, X)\n",
    "    except Exception:\n",
    "        return proba_mlp(model, X)\n",
    "\n",
    "val_probs = {}\n",
    "for name in curves.keys():\n",
    "    if name == \"Logistic\":\n",
    "        val_probs[name] = proba_of(lr, Xva)\n",
    "    elif name == \"RandomForest\":\n",
    "        val_probs[name] = proba_of(rf, Xva)\n",
    "    elif name == \"XGBoost\":\n",
    "         val_probs[name] = proba_of(xgb, Xva)\n",
    "    elif name == \"FeatureSel (best pipe)\":\n",
    "        val_probs[name] = proba_of(best_sel_pipe, Xva)\n",
    "    elif name == \"MLP\":\n",
    "         val_probs[name] = proba_mlp(mlp_model, Xva)\n",
    "    elif name == \"Stacking (meta)\":\n",
    "        # rebuild meta inputs on VAL the same way as test\n",
    "        lr_p, rf_p, mlp_p = proba_of(lr, Xva), proba_of(rf, Xva), proba_mlp(mlp_model, Xva)\n",
    "        Xva_stack = np.vstack([lr_p, rf_p, mlp_p]).T\n",
    "        val_probs[name] = proba_of(meta, Xva_stack)\n",
    "\n",
    "# thresholds per model (fixed from validation)\n",
    "thr_f1 = {m: thr_f1_from_val(yva, pv) for m, pv in val_probs.items()}\n",
    "thr_r10 = {m: thr_recall_from_val(yva, pv, RECALL_TARGET) for m, pv in val_probs.items()}\n",
    "\n",
    "# --- metrics helpers on a bootstrapped sample ---\n",
    "def safe_auc(func, y, p):\n",
    "    # returns NaN if only one class present in the sample\n",
    "    try:\n",
    "        return float(func(y, p))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def eval_at_threshold(y, p, t):\n",
    "    pred = (p >= t).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "    prec = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
    "    rec  = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "    bal  = balanced_accuracy_score(y, pred)\n",
    "    return prec, rec, bal\n",
    "\n",
    "# --- bootstrap loop (vectorized sampling of indices) ---\n",
    "n = len(yte)\n",
    "idx_mat = rng.integers(0, n, size=(B, n))  # (B x n) indices with replacement\n",
    "\n",
    "results = []  # one dataframe per model, then concat\n",
    "\n",
    "for model_name, p_test in curves.items():\n",
    "    p_test = np.asarray(p_test, dtype=float)\n",
    "    # storage\n",
    "    pr_auc = np.empty(B); roc_auc = np.empty(B)\n",
    "    prec_f1 = np.empty(B); rec_f1 = np.empty(B); bal_f1 = np.empty(B)\n",
    "    prec_r10 = np.empty(B); rec_r10 = np.empty(B); bal_r10 = np.empty(B)\n",
    "\n",
    "    t1 = thr_f1.get(model_name, 0.5)\n",
    "    t2 = thr_r10.get(model_name, 0.0)\n",
    "\n",
    "    for b in range(B):\n",
    "        idx = idx_mat[b]\n",
    "        yb, pb = yte[idx], p_test[idx]\n",
    "        pr_auc[b]  = safe_auc(average_precision_score, yb, pb)\n",
    "        roc_auc[b] = safe_auc(roc_auc_score, yb, pb)\n",
    "\n",
    "        pf, rf_, bf = eval_at_threshold(yb, pb, t1)\n",
    "        pr10, rr10, br10 = eval_at_threshold(yb, pb, t2)\n",
    "\n",
    "        prec_f1[b], rec_f1[b], bal_f1[b] = pf, rf_, bf\n",
    "        prec_r10[b], rec_r10[b], bal_r10[b] = pr10, rr10, br10\n",
    "\n",
    "    def ci_stats(a):\n",
    "        lo, hi = np.nanpercentile(a, [2.5, 97.5])\n",
    "        return np.nanmean(a), lo, hi\n",
    "\n",
    "    rows = []\n",
    "    for metric, arr in [\n",
    "        (\"PR_AUC\", pr_auc),\n",
    "        (\"ROC_AUC\", roc_auc),\n",
    "        (\"Precision@F1thr\", prec_f1),\n",
    "        (\"Recall@F1thr\",    rec_f1),\n",
    "        (\"BalancedAcc@F1thr\", bal_f1),\n",
    "        (f\"Precision@Rec≥{int(RECALL_TARGET*100)}%thr\", prec_r10),\n",
    "        (f\"Recall@Rec≥{int(RECALL_TARGET*100)}%thr\",    rec_r10),\n",
    "        (f\"BalancedAcc@Rec≥{int(RECALL_TARGET*100)}%thr\", bal_r10),\n",
    "    ]:\n",
    "        mean, lo, hi = ci_stats(arr)\n",
    "        rows.append({\"Model\": model_name, \"Metric\": metric,\n",
    "                     \"Mean\": mean, \"CI_low\": lo, \"CI_high\": hi})\n",
    "\n",
    "    results.append(pd.DataFrame(rows))\n",
    "\n",
    "df_boot = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# pretty table per model\n",
    "def fmt_row(r):\n",
    "    return f\"{r['Mean']:.3f} [{r['CI_low']:.3f},{r['CI_high']:.3f}]\"\n",
    "pivot = (df_boot\n",
    "         .assign(CI=lambda d: d.apply(fmt_row, axis=1))\n",
    "         .pivot(index=\"Model\", columns=\"Metric\", values=\"CI\")\n",
    "         .reset_index())\n",
    "\n",
    "print(f\"Bootstrap 95% CIs (B={B}) — metrics on TEST; thresholds fixed from VAL:\")\n",
    "display(pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7d3352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forest plots for bootstrap CIs\n",
    "# expects df_boot with columns: Model, Metric, Mean, CI_low, CI_high\n",
    "# (from the bootstrap block you just ran)\n",
    "def plot_forest(df_boot, baseline=None, alpha_ci=0.2):\n",
    "    metrics = df_boot[\"Metric\"].unique()\n",
    "    for metric in metrics:\n",
    "        sub = (df_boot[df_boot[\"Metric\"] == metric]\n",
    "               .sort_values(\"Mean\").reset_index(drop=True))\n",
    "        y = np.arange(len(sub))\n",
    "        err_low  = sub[\"Mean\"] - sub[\"CI_low\"]\n",
    "        err_high = sub[\"CI_high\"] - sub[\"Mean\"]\n",
    "\n",
    "        h = 0.55 + 0.25*len(sub)  # dynamic height\n",
    "        fig, ax = plt.subplots(figsize=(7.5, h))\n",
    "\n",
    "        ax.errorbar(sub[\"Mean\"], y,\n",
    "                    xerr=[err_low, err_high],\n",
    "                    fmt=\"o\", capsize=3, elinewidth=1.2, lw=0)\n",
    "        \n",
    "        for i, m in enumerate(sub[\"Mean\"]):\n",
    "            ax.text(m, i, f\" {m:.3f}\", va=\"center\", ha=\"left\", fontsize=9)\n",
    "\n",
    "        # Optional: shade CI band extremes for context\n",
    "        ax.axvspan(sub[\"CI_low\"].min(), sub[\"CI_high\"].max(), color=\"gray\", alpha=alpha_ci*0.2)\n",
    "\n",
    "        # Optional: baseline vertical line\n",
    "        if baseline is not None and baseline in set(sub[\"Model\"]):\n",
    "            try:\n",
    "                base_mean = float(sub.loc[sub[\"Model\"] == baseline, \"Mean\"].iloc[0])\n",
    "                ax.axvline(base_mean, ls=\"--\", lw=1.2, alpha=0.5, color=\"black\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        ax.set_yticks(y)\n",
    "        ax.set_yticklabels(sub[\"Model\"])\n",
    "        ax.set_xlabel(metric)\n",
    "        ax.set_title(f\"Bootstrap 95% CI — {metric}\")\n",
    "        ax.grid(True, axis=\"x\", alpha=0.35)\n",
    "\n",
    "        # Save per metric\n",
    "        savefig(f\"09_bootstrap_forest_{metric.replace(' ','_').replace('≥','ge').replace(':','_')}.png\")\n",
    "\n",
    "        # Nice x-limits with a little padding\n",
    "        x_min = (sub[\"CI_low\"].min())\n",
    "        x_max = (sub[\"CI_high\"].max())\n",
    "        pad = 0.05 * (x_max - x_min if x_max > x_min else 1.0)\n",
    "        ax.set_xlim(x_min - pad, x_max + pad)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Call it (uses BASELINE from 3.5 if present)\n",
    "try:\n",
    "    plot_forest(df_boot, baseline=BASELINE)\n",
    "except NameError:\n",
    "    plot_forest(df_boot, baseline=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8001d176",
   "metadata": {},
   "source": [
    "#### 3.7. Save all the necessary information for further reserach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e54428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Artifacts\n",
    "ART = Path(ROOT/\"artifacts\"); ART.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) data & features\n",
    "joblib.dump({\"feat_cols\": feat_cols,\n",
    "             \"Xva\": Xva, \"yva\": yva,\n",
    "             \"Xte\": Xte, \"yte\": yte}, ART/\"data_splits.joblib\")\n",
    "\n",
    "# 2) test probs (curves) and val probs\n",
    "joblib.dump(curves, ART/\"test_probs.joblib\")        # dict: model -> p_test\n",
    "try:\n",
    "    joblib.dump(val_probs, ART/\"val_probs.joblib\")  # dict: model -> p_val\n",
    "except NameError:\n",
    "    # rebuild minimal val_probs from loaded models\n",
    "    def _prob(model, X):\n",
    "        try: return model.predict_proba(X)[:,1]\n",
    "        except: return proba_mlp(mlp_model, X)\n",
    "    _val = {}\n",
    "    if \"Logistic\" in curves:    _val[\"Logistic\"] = _prob(lr, Xva)\n",
    "    if \"RandomForest\" in curves:_val[\"RandomForest\"] = _prob(rf, Xva)\n",
    "    if \"XGBoost\" in curves:     _val[\"XGBoost\"] = _prob(xgb, Xva)\n",
    "    if \"FeatureSel (best pipe)\" in curves: _val[\"FeatureSel (best pipe)\"] = _prob(best_sel_pipe, Xva)\n",
    "    if \"MLP\" in curves:         _val[\"MLP\"] = proba_mlp(mlp_model, Xva)\n",
    "    if \"Stacking (meta)\" in curves:\n",
    "        lr_v, rf_v, mlp_v = _prob(lr,Xva), _prob(rf,Xva), proba_mlp(mlp_model,Xva)\n",
    "        Xva_stack = np.vstack([lr_v, rf_v, mlp_v]).T\n",
    "        _val[\"Stacking (meta)\"] = meta.predict_proba(Xva_stack)[:,1]\n",
    "    joblib.dump(_val, ART/\"val_probs.joblib\")\n",
    "\n",
    "# 3) thresholds from validation (F1-opt, Recall≥10%)\n",
    "def _align_pr(y, p):\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    P,R,T = precision_recall_curve(y, p); return P[:-1], R[:-1], T\n",
    "def _thr_f1(y,p, beta=1.0):\n",
    "    P,R,T = _align_pr(y,p); F=(1+beta**2)*(P*R)/(beta**2*P+R+1e-12)\n",
    "    return float(T[int(np.nanargmax(F))]) if len(T) else 0.5\n",
    "def _thr_r10(y,p, floor=0.10):\n",
    "    P,R,T = _align_pr(y,p); ok=np.where(R>=floor)[0]\n",
    "    return float(T[ok[np.argmax(P[ok])]]) if ok.size else (_thr_f1(y,p))\n",
    "\n",
    "_val = joblib.load(ART/\"val_probs.joblib\")\n",
    "thr_f1  = {m: _thr_f1(yva, pv) for m, pv in _val.items()}\n",
    "thr_r10 = {m: _thr_r10(yva, pv, 0.10) for m, pv in _val.items()}\n",
    "joblib.dump({\"thr_f1\": thr_f1, \"thr_r10\": thr_r10}, ART/\"thresholds.joblib\")\n",
    "\n",
    "# 4) SHAP top-k features (if computed)\n",
    "try:\n",
    "    top_rf  = list(pd.Series(np.abs(shap_rf).mean(axis=0), index=Xte_sub.columns).nlargest(10).index)\n",
    "    top_xgb = list(pd.Series(np.abs(shap_xgb).mean(axis=0), index=Xte_sub.columns).nlargest(10).index)\n",
    "    joblib.dump({\"RF_top10\": top_rf, \"XGB_top10\": top_xgb}, ART/\"shap_topk.joblib\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 5) Optional: cost sweep & bootstrap tables if present\n",
    "try: joblib.dump(df_cost, ART/\"cost_sweep_df.joblib\")\n",
    "except Exception: pass\n",
    "try: joblib.dump(df_boot, ART/\"bootstrap_df.joblib\")\n",
    "except Exception: pass\n",
    "\n",
    "print(\"Saved artifacts to:\", ART.resolve())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
