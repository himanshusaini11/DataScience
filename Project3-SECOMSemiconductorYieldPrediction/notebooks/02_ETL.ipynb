{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdb9eed",
   "metadata": {},
   "source": [
    "# SECOM Yield Prediction â€” End-to-End Notebook\n",
    "\n",
    "**Goal:** Predict *Fail* outcomes from process measurements to reduce scrap and downtime.\n",
    "\n",
    "**Data:** `data/secom.data`, `data/secom_labels.data`, `data/secom.names` (UCI ML Repository, real fab data).\n",
    "\n",
    "**Primary metric:** Recall on *Fail* at acceptable precision. Report PR-AUC and Balanced Error Rate (BER).\n",
    "\n",
    "> Safety: No unsupported claims. Treat outputs as decision support, not automation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77101320",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c74488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Global style for plots\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 12,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"axes.edgecolor\": \"black\",\n",
    "    \"axes.linewidth\": 1,\n",
    "    \"xtick.color\": \"black\",\n",
    "    \"ytick.color\": \"black\",\n",
    "    \"xtick.direction\": \"out\",\n",
    "    \"ytick.direction\": \"out\",\n",
    "    \"xtick.bottom\": True,\n",
    "    \"ytick.left\": True,\n",
    "    \"xtick.top\": False,\n",
    "    \"ytick.right\": False,\n",
    "    \"figure.dpi\": 150,\n",
    "    \"legend.frameon\": True,\n",
    "    \"legend.facecolor\": \"white\",\n",
    "    \"legend.edgecolor\": \"black\",\n",
    "    \"legend.fontsize\": 12\n",
    "})\n",
    "\n",
    "# Apply to seaborn\n",
    "sns.set_theme(context=\"notebook\", style=\"ticks\")\n",
    "\n",
    "# Reproducibility\n",
    "rnd_num = 42\n",
    "np.random.seed(rnd_num)\n",
    "\n",
    "# Root directory\n",
    "ROOT = Path(os.getcwd()).parent\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = Path(ROOT/\"data\")\n",
    "RAW = Path(DATA_DIR/\"raw\")\n",
    "assert (RAW/\"secom.data\").exists() and (RAW/\"secom_labels.data\").exists(), \"Data files are missing!\"\n",
    "\n",
    "# Results directory\n",
    "RESULT_DIR = Path(ROOT/\"results\")\n",
    "RESULT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Model directory\n",
    "MODEL_DIR = Path(ROOT/\"models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f0ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure saver helper (use after each plot)\n",
    "RESULT_DIR_ETL = Path(RESULT_DIR/\"ETL\")\n",
    "RESULT_DIR_ETL.mkdir(exist_ok=True, parents=True)\n",
    "def savefig(name):\n",
    "    out = RESULT_DIR_ETL/name\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out, dpi=300, bbox_inches=\"tight\")\n",
    "    print(\"Saved:\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a41d9f9",
   "metadata": {},
   "source": [
    "Note: Numbering continue from 01_EDA.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f1e087",
   "metadata": {},
   "source": [
    "### 5. Extract Transform Load (ETL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87c27e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b1aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from EDA step.\n",
    "df_EDA = pd.read_parquet(DATA_DIR/\"interim/SECOM_EDA.parquet\")\n",
    "df = df_EDA.copy()\n",
    "df = df.drop(columns=[\"timestamp\"])  # timestamp not needed for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cbd261",
   "metadata": {},
   "source": [
    "#### 5.1. Missing value handling (audit + strategy)\n",
    "\n",
    "From EDA we know:\n",
    "- Most features have almost no missing values.\n",
    "- A few have extreme missingness (up to `~91%`).\n",
    "- Average missingness `~4.5%`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92df4003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraction of missing per feature\n",
    "missing_frac = df.drop(columns=[\"label\"]).isna().mean()\n",
    "\n",
    "# Candidate thresholds to test\n",
    "thresh_values = np.linspace(0, 1, 21)\n",
    "\n",
    "kept, dropped = [], []\n",
    "\n",
    "for t in thresh_values:\n",
    "    drop_cols = (missing_frac > t).sum()\n",
    "    keep_cols = (missing_frac <= t).sum()\n",
    "    dropped.append(drop_cols)\n",
    "    kept.append(keep_cols)\n",
    "\n",
    "plt.plot(thresh_values, kept, marker=\"o\", label=\"Kept Features\")\n",
    "plt.plot(thresh_values, dropped, marker=\"s\", label=\"Dropped Features\")\n",
    "plt.axvline(0.7, color=\"red\", linestyle=\"--\", label=\"drop_thresh=0.7\")\n",
    "plt.xlabel(\"Drop threshold (fraction missing)\")\n",
    "plt.ylabel(\"Number of features\")\n",
    "plt.title(\"Impact of drop_thresh on features retained\")\n",
    "plt.legend()\n",
    "savefig(\"01_missing_drop_curve.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fd3981",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_counts = []\n",
    "for t in thresh_values:\n",
    "    flag_cols = ((missing_frac > t) & (missing_frac <= 0.7)).sum()\n",
    "    flag_counts.append(flag_cols)\n",
    "\n",
    "plt.plot(thresh_values, flag_counts, marker=\"d\", color=\"purple\")\n",
    "plt.axvline(0.1, color=\"red\", linestyle=\"--\", label=\"flag_thresh=0.1\")\n",
    "plt.xlabel(\"Flag threshold (fraction missing)\")\n",
    "plt.ylabel(\"Number of flagged features\")\n",
    "plt.title(\"Impact of flag_thresh on missingness indicators\")\n",
    "plt.legend()\n",
    "savefig(\"02_missing_flag_curve.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e2e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Audit missingness (from EDA)\n",
    "print(missing_frac.describe())\n",
    "\n",
    "# 2. Define thresholds\n",
    "drop_thresh = 0.7   # drop if >70% missing\n",
    "flag_thresh = 0.1   # add missing indicator if >10% missing\n",
    "\n",
    "# 3. Identify feature groups\n",
    "drop_cols = missing_frac[missing_frac > drop_thresh].index.tolist()\n",
    "flag_cols = missing_frac[(missing_frac > flag_thresh) & (missing_frac <= drop_thresh)].index.tolist()\n",
    "keep_cols = missing_frac[missing_frac <= drop_thresh].index.tolist()\n",
    "\n",
    "print(f\"Drop {len(drop_cols)} features with >70% missing\")\n",
    "print(f\"Flag {len(flag_cols)} features with 10-70% missing\")\n",
    "print(f\"Keep {len(keep_cols)} features with <70% missing\")\n",
    "\n",
    "# 4. Drop high-missing features\n",
    "df_etl = df.drop(columns=drop_cols)\n",
    "\n",
    "# 5. Add missingness indicators\n",
    "for col in flag_cols:\n",
    "    df_etl[col+\"_missing\"] = df_etl[col].isna().astype(int)\n",
    "\n",
    "# 6. Impute remaining missing with median\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "df_etl[keep_cols] = imputer.fit_transform(df_etl[keep_cols])\n",
    "\n",
    "print(\"Shape:\", df_etl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426139bb",
   "metadata": {},
   "source": [
    "#### 5.2. Outlier Handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f6ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "strategies = {}\n",
    "X = df_etl.drop(columns=\"label\")\n",
    "\n",
    "for col in X.columns:\n",
    "    s = X[col].dropna()\n",
    "    sk = skew(s)\n",
    "    if abs(sk) > 1 and (s >= 0).all():\n",
    "        strategies[col] = \"log\"\n",
    "    elif s.quantile(0.99) > s.quantile(0.5) * 5:  # extreme tail\n",
    "        strategies[col] = \"winsor\"\n",
    "    else:\n",
    "        strategies[col] = \"keep\"\n",
    "\n",
    "# Summary of counts\n",
    "from collections import Counter\n",
    "print(Counter(strategies.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cd9bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for outlier handling.\n",
    "def winsorize_col(s: pd.Series, lower=0.01, upper=0.99):\n",
    "    \"\"\"Clip to [lower, upper] quantiles. Works with NaNs.\"\"\"\n",
    "    ql, qu = s.quantile(lower), s.quantile(upper)\n",
    "    return s.clip(lower=ql, upper=qu)\n",
    "\n",
    "def apply_outlier_transforms(df_in: pd.DataFrame,\n",
    "                             strategies: dict,\n",
    "                             winsor=(0.01, 0.99),\n",
    "                             log_safe=True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply per-column outlier strategy:\n",
    "        'log'     -> log1p (if non-negative; else fallback to winsor)\n",
    "        'winsor'  -> clip to quantiles\n",
    "        'keep'    -> no change\n",
    "    Non-feature columns like 'label' are passed through unchanged.\n",
    "    \"\"\"\n",
    "    df = df_in.copy()\n",
    "    feat_cols = [c for c in df.columns if c != \"label\"]\n",
    "\n",
    "    # Summary counters\n",
    "    n_log = n_win = n_keep = n_badlog = 0\n",
    "\n",
    "    for c in feat_cols:\n",
    "        strat = strategies.get(c, \"keep\")\n",
    "\n",
    "        # Choose action\n",
    "        if strat == \"log\":\n",
    "            s = df[c]\n",
    "            # If any negatives and log_safe, fallback to winsor\n",
    "            if log_safe and (s.min(skipna=True) < 0):\n",
    "                df[c] = winsorize_col(s, *winsor)\n",
    "                n_badlog += 1; n_win += 1\n",
    "            else:\n",
    "                # log1p handles zeros; keep NaNs as-is (assumed imputed earlier)\n",
    "                df[c] = np.log1p(s)\n",
    "                n_log += 1\n",
    "\n",
    "        elif strat == \"winsor\":\n",
    "            df[c] = winsorize_col(df[c], *winsor)\n",
    "            n_win += 1\n",
    "\n",
    "        else:  # 'keep'\n",
    "            n_keep += 1\n",
    "\n",
    "    print(f\"Applied transforms --> log: {n_log}, winsor: {n_win} \"\n",
    "          f\"(fallbacks from log: {n_badlog}), keep: {n_keep}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8771a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- run on df_etl dataframe ---\n",
    "df_etl2 = apply_outlier_transforms(df_etl, strategies, winsor=(0.01, 0.99))\n",
    "print(\"After outliers handling step:\", df_etl2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5021a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sanity check after ETL Outlier Handiling Step ---\n",
    "\n",
    "# 1. Identify indicator columns (added in Step 1)\n",
    "indicator_cols = [c for c in df_etl2.columns if c.endswith(\"_missing\")]\n",
    "\n",
    "# 2. Original sensor features (those in strategy dict)\n",
    "sensor_cols = list(strategies.keys())\n",
    "\n",
    "# 3. Label column\n",
    "label_col = \"label\"\n",
    "\n",
    "# 4. Safety check: All other columns should be accounted for\n",
    "other_cols = [c for c in df_etl2.columns \n",
    "              if c not in indicator_cols and c not in sensor_cols and c != label_col]\n",
    "\n",
    "# --- Counts ---\n",
    "print(f\"Total columns: {df_etl2.shape[1]}\")\n",
    "print(f\"  Sensor features: {len(sensor_cols)}\")\n",
    "print(f\"  Indicator features: {len(indicator_cols)}\")\n",
    "print(f\"  Label: 1\")\n",
    "print(f\"  Other (unexpected): {len(other_cols)}\")\n",
    "\n",
    "# --- Quick data type check ---\n",
    "print(\"\\nIndicator columns dtype check (should all be int or 0/1):\")\n",
    "print(df_etl2[indicator_cols].dtypes.value_counts())\n",
    "\n",
    "# --- Sample preview ---\n",
    "print(\"\\nSample indicator preview (first 5 rows):\")\n",
    "print(df_etl2[indicator_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad437f6",
   "metadata": {},
   "source": [
    "#### 5.3. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c63400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features vs label\n",
    "X = df_etl2.drop(columns=\"label\")\n",
    "y = df_etl2[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1ae643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify which columns to scale: numeric sensors only (exclude indicators)\n",
    "scale_cols = [c for c in X.columns if not c.endswith(\"_missing\")]\n",
    "scale_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfdddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit StandardScaler on numeric features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "X_scaled[scale_cols] = scaler.fit_transform(X[scale_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326bf856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataset\n",
    "df_etl3 = X_scaled.join(y)\n",
    "\n",
    "print(\"After ETL Feature Scaling:\", df_etl3.shape)\n",
    "print(\"Mean of first 5 scaled features:\")\n",
    "print(df_etl3[scale_cols].mean().head())\n",
    "print(\"Std of first 5 scaled features:\")\n",
    "print(df_etl3[scale_cols].std().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d891e0d",
   "metadata": {},
   "source": [
    "#### 5.4. Chronological split (no leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd91525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-req: df_etl3 has features + 'label'\n",
    "#          df_EDA has a 'timestamp' column aligned by row index\n",
    "\n",
    "# 1) attach timestamp (align by index)\n",
    "assert len(df_etl3) == len(df_EDA), \"row count mismatch\"\n",
    "df_etl4 = df_etl3.copy()\n",
    "df_etl4[\"timestamp\"] = df_EDA[\"timestamp\"]\n",
    "\n",
    "# 2) drop rows with missing timestamps (should be none)\n",
    "df_etl4 = df_etl4.dropna(subset=[\"timestamp\"])\n",
    "print(\"After attaching timestamp:\", df_etl4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4385b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) sort by time\n",
    "df_etl4 = df_etl4.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "# 4) time-based split indices (60/20/20)\n",
    "n = len(df_etl4)\n",
    "i_tr  = int(0.60 * n)\n",
    "i_val = int(0.80 * n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6326d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) build splits\n",
    "cols_feat = [c for c in df_etl4.columns if c not in (\"label\",\"timestamp\")]\n",
    "X_train, y_train = df_etl4.loc[:i_tr-1, cols_feat].values, df_etl4.loc[:i_tr-1, \"label\"].values\n",
    "X_val,   y_val   = df_etl4.loc[i_tr:i_val-1, cols_feat].values, df_etl4.loc[i_tr:i_val-1, \"label\"].values\n",
    "X_test,  y_test  = df_etl4.loc[i_val:, cols_feat].values, df_etl4.loc[i_val:, \"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9788f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) quick sanity\n",
    "def stats(y):\n",
    "    return dict(n=len(y), fails=int((y==1).sum()), fail_rate=float((y==1).mean()))\n",
    "print(\"Train:\", stats(y_train))\n",
    "print(\"Val:  \", stats(y_val))\n",
    "print(\"Test: \", stats(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c16c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup: keep the split DataFrames for inspection\n",
    "train_df = df_etl4.loc[:i_tr-1, cols_feat + [\"label\",\"timestamp\"]]\n",
    "val_df   = df_etl4.loc[i_tr:i_val-1, cols_feat + [\"label\",\"timestamp\"]]\n",
    "test_df  = df_etl4.loc[i_val:, cols_feat + [\"label\",\"timestamp\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e686e25",
   "metadata": {},
   "source": [
    "#### 5.5. Low-variance and duplicate feature pruning (train-only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cfde3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_feat # List of feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4136c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on TRAIN ONLY to avoid leakage\n",
    "# Use col_feat to extract features for modeling\n",
    "Xtr_df = train_df[cols_feat]\n",
    "ytr = train_df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033da4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5a) Low-variance filter (std < 1e-8 after scaling â‡’ constant)\n",
    "vt = VarianceThreshold(threshold=1e-8)\n",
    "vt.fit(Xtr_df.values)\n",
    "keep_mask = vt.get_support()\n",
    "keep_cols_lv = list(np.array(cols_feat)[keep_mask])\n",
    "print(f\"Low-variance removed: {len(cols_feat) - len(keep_cols_lv)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64625fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5b) Duplicate columns (exact duplicates on train)\n",
    "Xtr_lv = Xtr_df[keep_cols_lv]\n",
    "dups = {}\n",
    "seen = {}\n",
    "for c in keep_cols_lv:\n",
    "    key = tuple(np.round(Xtr_lv[c].values, 8))  # robust equality\n",
    "    if key in seen:\n",
    "        dups[c] = seen[key]\n",
    "    else:\n",
    "        seen[key] = c\n",
    "drop_dups = list(dups.keys())\n",
    "keep_cols_uniq = [c for c in keep_cols_lv if c not in drop_dups]\n",
    "print(f\"Duplicate columns removed: {len(drop_dups)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bed0a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to all splits\n",
    "def apply_colsubset(df, cols):\n",
    "    return df[cols + [\"label\",\"timestamp\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946cc37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df5 = apply_colsubset(train_df, keep_cols_uniq)\n",
    "val_df5   = apply_colsubset(val_df,   keep_cols_uniq)\n",
    "test_df5  = apply_colsubset(test_df,  keep_cols_uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97855213",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shapes after Low-variance and duplicate feature pruning:\", train_df5.shape, val_df5.shape, test_df5.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa899e23",
   "metadata": {},
   "source": [
    "#### 5.6. Correlation pruning (train-only, within highly correlated groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e64dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove one feature from any pair with |corr| >= 0.98 (train-only)\n",
    "Xtr = train_df5.drop(columns=[\"label\",\"timestamp\"])\n",
    "corr = Xtr.corr().abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7a649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper triangle mask\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "high_corr_cols = [column for column in upper.columns if (upper[column] >= 0.98).any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce2640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep = all minus high-corr columns\n",
    "keep_cols_corr = [c for c in Xtr.columns if c not in set(high_corr_cols)]\n",
    "print(f\"Correlation-pruned: {len(Xtr.columns) - len(keep_cols_corr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e11f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to splits\n",
    "def apply_corrsubset(df, cols):\n",
    "    return df[cols + [\"label\",\"timestamp\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f8a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df6 = apply_corrsubset(train_df5, keep_cols_corr)\n",
    "val_df6   = apply_corrsubset(val_df5,   keep_cols_corr)\n",
    "test_df6  = apply_corrsubset(test_df5,  keep_cols_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9fc7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shapes after Correlation pruning:\", train_df6.shape, val_df6.shape, test_df6.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e060b27",
   "metadata": {},
   "source": [
    "#### 5.7. Persist clean artifacts for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b2775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROC = Path(\"../data/processed\")\n",
    "PROC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_df6.to_parquet(PROC/\"train.parquet\", index=False)\n",
    "val_df6.to_parquet(PROC/\"val.parquet\", index=False)\n",
    "test_df6.to_parquet(PROC/\"test.parquet\", index=False)\n",
    "\n",
    "# Also save feature list\n",
    "feat_final = [c for c in train_df6.columns if c not in (\"label\",\"timestamp\")]\n",
    "pd.Series(feat_final, name=\"features\").to_csv(PROC/\"features_final.txt\", index=False)\n",
    "\n",
    "print(\"Saved:\", list(PROC.iterdir()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
