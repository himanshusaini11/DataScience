{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Advanced Research & Dash App\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Dash App creation"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udca1 Feature: Guided User Input with Validation Hints\n", "\n", "To improve user experience and reduce input errors in the loan default prediction app, we implemented the following enhancements:\n", "\n", "### 1. Data Type Detection\n", "- For each input feature, we determine its **expected data type** from the training dataset.\n", "- This allows us to set suitable **placeholder values** and define meaningful **tooltip hints**.\n", "\n", "### 2. Placeholder Hints\n", "- Each input field displays `\"Enter Value (e.g., 15000)\"` as a placeholder.\n", "- The example is dynamically generated based on the actual data type or common value range.\n", "\n", "### 3. Mouse Hover Tooltip\n", "- When a user hovers over a field, a tooltip appears showing a **detailed description or example** for the input.\n", "- This helps prevent format errors (e.g., entering strings where integers are expected).\n", "\n", "### 4. Optional Future Enhancements\n", "- Input validation that restricts typing to allowed formats.\n", "- Auto-suggestion or drop-downs for categorical fields.\n", "\n", "This combination ensures a cleaner UI and smarter form input handling for model deployment."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# \ud83c\udf9b\ufe0f Input UI Design for Dash App\n", "\n", "## \ud83c\udfaf Objective\n", "\n", "To enhance user experience and ensure data integrity, we aim to create a user-friendly input form where each input field:\n", "- Accepts only valid values based on the feature's data type.\n", "- Provides a default prompt like \"Enter Value\".\n", "- Shows an example tooltip on hover for guidance.\n", "\n", "## \ud83e\udde0 How It Works\n", "\n", "1. **7x5 Grid Layout**:\n", "   - Inputs are arranged in a grid layout of 7 rows and 5 columns.\n", "   - This ensures compactness and better use of screen space.\n", "\n", "2. **Dynamic Tooltips**:\n", "   - Each input field has a tooltip that displays an example value extracted from the training dataset.\n", "   - Hovering over the input field will show guidance like:  \n", "     _\"Example: 12000\"_ or _\"Example: Verified\"_.\n", "\n", "3. **Input Type Enforcement**:\n", "   - Numerical fields use `type='number'` to prevent invalid entries.\n", "   - Categorical fields use dropdowns or text with validation logic in the backend.\n", "\n", "## \u2705 Benefits\n", "\n", "- Prevents user input errors at the UI level.\n", "- Reduces backend validation complexity.\n", "- Ensures better form completion rate and usability.\n", "\n", "## \ud83d\udd0d Example Input Field (in Dash)\n", "```python\n", "dcc.Input(\n", "    id='loan_amount',\n", "    type='number',\n", "    placeholder='Enter Loan Amount',\n", "    debounce=True,\n", "    title='Example: 12000'\n", ")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# \ud83e\udde0 Why the Dash App Fails Without Preprocessing?\n", "\n", "When we train a machine learning model using **scikit-learn**, we typically convert all categorical (text-based) features into **numerical representations** \u2014 because models like `SVC`, `RandomForest`, or `XGBoost` **cannot operate on raw string data**.\n", "\n", "This process is called **feature engineering**, which often includes:\n", "\n", "- **One-hot encoding** (e.g., `Home Ownership = RENT` \u2192 column `Home Ownership_RENT = 1`)\n", "- **Label encoding**\n", "- **Scaling** (e.g., MinMaxScaler or StandardScaler)\n", "- **Handling missing values**\n", "- **Date feature extraction** (e.g., month, year)\n", "\n", "During training, this transformation was applied **before the model saw the data**, and the model only learned from the transformed features \u2014 which are **all numeric**.\n", "\n", "---\n", "\n", "### \u26a0\ufe0f Problem in the Current Dash App\n", "\n", "Right now, the app takes user input as raw values like:\n", "```python\n", "\"Term\" = \"Short Term\"  # string\n", "\"Home Ownership\" = \"RENT\"  # string\n", "```\n", "\n", "But then it sends these **raw strings** to the trained model:\n", "```python\n", "model.predict(input_data)\n", "```\n", "\n", "The model expects preprocessed numeric input like:\n", "```python\n", "\"Term_Short Term\" = 1\n", "\"Home Ownership_RENT\" = 1\n", "\"Home Ownership_OWN\" = 0\n", "```\n", "\n", "Hence, you're getting errors like:\n", "```plaintext\n", "could not convert string to float: 'Short Term'\n", "```\n", "\n", "---\n", "\n", "### \u2705 Correct Approach for Deployment\n", "\n", "1. **Recreate your exact preprocessing pipeline** used during training.\n", "2. **Save this pipeline** as a `Pipeline` object that combines both preprocessing and the model.\n", "3. In the Dash app:\n", "   - Accept raw user input\n", "   - Pass it through the same pipeline\n", "   - Then send it to `.predict()` or `.predict_proba()`\n", "\n", "This ensures your input format **matches** the model\u2019s training data structure.\n", "\n", "---\n", "\n", "### \ud83d\udee0\ufe0f What Should You Do Next?\n", "\n", "You need to **retrain or refit** your model inside a `Pipeline`, like this:\n", "\n", "```python\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.preprocessing import OneHotEncoder\n", "from sklearn.compose import ColumnTransformer\n", "from sklearn.svm import SVC\n", "\n", "# Step 1: Define preprocessing\n", "categorical_cols = [\"Term\", \"Home Ownership\", ...]\n", "numerical_cols = [\"Loan Amount\", \"Interest Rate\", ...]\n", "\n", "preprocessor = ColumnTransformer([\n", "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n", "    (\"num\", \"passthrough\", numerical_cols)\n", "])\n", "\n", "# Step 2: Combine preprocessing + model\n", "pipeline = Pipeline([\n", "    (\"preprocessor\", preprocessor),\n", "    (\"classifier\", SVC(probability=True))\n", "])\n", "\n", "# Step 3: Fit pipeline\n", "pipeline.fit(X_raw, y)  # X_raw = original unprocessed DataFrame\n", "\n", "# Step 4: Save pipeline\n", "import joblib\n", "joblib.dump(pipeline, \"svc_pipeline_model.pkl\")\n", "```\n", "\n", "Then in your Dash app, simply load and use:\n", "\n", "```python\n", "model = joblib.load(\"svc_pipeline_model.pkl\")\n", "model.predict(user_input_df)\n", "```\n", "\n", "---\n", "\n", "This guarantees your deployed app is using the **same pipeline logic** as used during training, making it robust and production-ready."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd01 Why Retrain Inside a Pipeline?\n", "When you trained your model earlier, you:\n", "\n", "    - Cleaned and encoded your DataFrame using pandas (like one-hot encoding)\n", "    - Dropped or transformed columns manually\n", "    - Fed the final numerical DataFrame into the model\n", "    - That was great for offline evaluation, but now in your Dash app:\n", "\n", "The user is entering raw values (like \"RENT\" or \"Short Term\")\n", "The model expects preprocessed numbers\n", "So you need to bundle the preprocessing + model together into a Pipeline so it handles everything seamlessly during prediction\n", "\n", "## \ud83e\udde0 Next Step\n", "Now that you're ready, let\u2019s:\n", "\n", "    - Rebuild your preprocessing logic using ```ColumnTransformer```\n", "    - Combine it with ```SVC(probability=True)``` inside a ```Pipeline```\n", "    - Fit the pipeline on raw training data (train.csv)\n", "    - Save the pipeline as ```svc_pipeline_model.pkl```\n", "\n", "## Use it in Dash \ud83c\udfaf"]}, {"cell_type": "code", "execution_count": 317, "metadata": {}, "outputs": [], "source": ["from sklearn.base import BaseEstimator, TransformerMixin\n", "from sklearn.preprocessing import LabelEncoder\n", "import pandas as pd\n", "\n", "class BankLoanPreprocessor(BaseEstimator, TransformerMixin):\n", "    def __init__(self):\n", "        self.grade_order = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7}\n", "        self.label_cols = [\"Sub Grade\", \"Batch Enrolled\"]\n", "        self.label_encoders = {}\n", "        self.freq_map = None\n", "        self.one_hot_cols = [\"Initial List Status\", \"Employment Duration\", \"Verification Status\"]\n", "        self.one_hot_columns_fitted = None  # to align test data with train\n", "\n", "    def fit(self, X, y=None):\n", "        # Fit Label Encoders\n", "        for col in self.label_cols:\n", "            le = LabelEncoder()\n", "            le.fit(X[col].astype(str))\n", "            self.label_encoders[col] = le\n", "\n", "        # Fit Frequency Encoding\n", "        self.freq_map = X[\"Loan Title\"].value_counts().to_dict()\n", "\n", "        # Fit One-Hot Columns\n", "        dummies = pd.get_dummies(X[self.one_hot_cols], drop_first=True)\n", "        self.one_hot_columns_fitted = dummies.columns.tolist()\n", "\n", "        return self\n", "\n", "    def transform(self, X):\n", "        X = X.copy()\n", "\n", "        # Grade Ordinal Mapping\n", "        X[\"Grade\"] = X[\"Grade\"].map(self.grade_order)\n", "\n", "        # Label Encoding\n", "        for col in self.label_cols:\n", "            X[col] = self.label_encoders[col].transform(X[col].astype(str))\n", "\n", "        # Frequency Encoding\n", "        X[\"Loan Title\"] = X[\"Loan Title\"].map(self.freq_map).fillna(0)\n", "\n", "        # One-Hot Encoding (align columns)\n", "        dummies = pd.get_dummies(X[self.one_hot_cols], drop_first=True)\n", "        for col in self.one_hot_columns_fitted:\n", "            if col not in dummies:\n", "                dummies[col] = 0\n", "        dummies = dummies[self.one_hot_columns_fitted]\n", "        X = X.drop(columns=self.one_hot_cols)\n", "        X = pd.concat([X, dummies], axis=1)\n", "\n", "        return X"]}, {"cell_type": "code", "execution_count": 324, "metadata": {}, "outputs": [{"ename": "ValueError", "evalue": "could not convert string to float: 'n'", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)", "Cell \u001b[0;32mIn[324], line 36\u001b[0m\n\u001b[1;32m     29\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m ImbPipeline([\n\u001b[1;32m     30\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m, BankLoanPreprocessor()),\n\u001b[1;32m     31\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmote\u001b[39m\u001b[38;5;124m\"\u001b[39m, SMOTE(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)),\n\u001b[1;32m     32\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, RandomForestClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))\n\u001b[1;32m     33\u001b[0m ])\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# --- 4. Fit the Pipeline ---\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X_raw, y_raw)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# --- 5. Evaluate ---\u001b[39;00m\n\u001b[1;32m     39\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_kaggle_test)\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/imblearn/pipeline.py:329\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03mFit all the transforms/samplers one after the other and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03m    This estimator.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    328\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m--> 329\u001b[0m Xt, yt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, routed_params)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/imblearn/pipeline.py:265\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, routed_params)\u001b[0m\n\u001b[1;32m    255\u001b[0m     X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[1;32m    256\u001b[0m         cloned_transformer,\n\u001b[1;32m    257\u001b[0m         X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m         params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[1;32m    263\u001b[0m     )\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cloned_transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_resample\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 265\u001b[0m     X, y, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_resample_one_cached(\n\u001b[1;32m    266\u001b[0m         cloned_transformer,\n\u001b[1;32m    267\u001b[0m         X,\n\u001b[1;32m    268\u001b[0m         y,\n\u001b[1;32m    269\u001b[0m         message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    270\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[1;32m    271\u001b[0m         params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[1;32m    272\u001b[0m     )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/joblib/memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/imblearn/pipeline.py:1057\u001b[0m, in \u001b[0;36m_fit_resample_one\u001b[0;34m(sampler, X, y, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit_resample_one\u001b[39m(sampler, X, y, message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m-> 1057\u001b[0m         X_res, y_res \u001b[38;5;241m=\u001b[39m sampler\u001b[38;5;241m.\u001b[39mfit_resample(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_resample\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[1;32m   1059\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m X_res, y_res, sampler\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/imblearn/base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_resample(X, y)\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/imblearn/base.py:106\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    104\u001b[0m check_classification_targets(y)\n\u001b[1;32m    105\u001b[0m arrays_transformer \u001b[38;5;241m=\u001b[39m ArraysTransformer(X, y)\n\u001b[0;32m--> 106\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[1;32m    110\u001b[0m )\n\u001b[1;32m    112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y)\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/imblearn/base.py:161\u001b[0m, in \u001b[0;36mBaseSampler._check_X_y\u001b[0;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[1;32m    159\u001b[0m     accept_sparse \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    160\u001b[0m y, binarize_y \u001b[38;5;241m=\u001b[39m check_target_type(y, indicate_one_vs_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 161\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, y, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, binarize_y\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/sklearn/base.py:480\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    474\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    475\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`BaseEstimator._validate_data` is deprecated in 1.6 and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 1.7. Use `sklearn.utils.validation.validate_data` instead. This \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction becomes public and is part of the scikit-learn developer API.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    479\u001b[0m     )\n\u001b[0;32m--> 480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validate_data(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/sklearn/utils/validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/sklearn/utils/validation.py:1370\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1368\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[0;32m-> 1370\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1371\u001b[0m     X,\n\u001b[1;32m   1372\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   1373\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[1;32m   1374\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1375\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[1;32m   1376\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m   1377\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39mforce_writeable,\n\u001b[1;32m   1378\u001b[0m     ensure_all_finite\u001b[38;5;241m=\u001b[39mensure_all_finite,\n\u001b[1;32m   1379\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[1;32m   1380\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[1;32m   1381\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[1;32m   1382\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[1;32m   1383\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m   1384\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1385\u001b[0m )\n\u001b[1;32m   1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1389\u001b[0m check_consistent_length(X, y)\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/sklearn/utils/validation.py:973\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pandas_requires_conversion:\n\u001b[1;32m    969\u001b[0m     \u001b[38;5;66;03m# pandas dataframe requires conversion earlier to handle extension dtypes with\u001b[39;00m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;66;03m# nans\u001b[39;00m\n\u001b[1;32m    971\u001b[0m     \u001b[38;5;66;03m# Use the original dtype for conversion if dtype is None\u001b[39;00m\n\u001b[1;32m    972\u001b[0m     new_dtype \u001b[38;5;241m=\u001b[39m dtype_orig \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n\u001b[0;32m--> 973\u001b[0m     array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(new_dtype)\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;66;03m# Since we converted here, we do not need to convert again later\u001b[39;00m\n\u001b[1;32m    975\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/pandas/core/generic.py:6643\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6637\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   6638\u001b[0m         ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   6639\u001b[0m     ]\n\u001b[1;32m   6641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6642\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6643\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mastype(dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   6644\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   6645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/pandas/core/internals/managers.py:430\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    428\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    432\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    433\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    434\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    435\u001b[0m     using_cow\u001b[38;5;241m=\u001b[39musing_copy_on_write(),\n\u001b[1;32m    436\u001b[0m )\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/pandas/core/internals/managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/pandas/core/internals/blocks.py:758\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not squeeze with more than one column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    756\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m new_values \u001b[38;5;241m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m    760\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    762\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:237\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m astype_array(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:182\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    179\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     values \u001b[38;5;241m=\u001b[39m _astype_nansafe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:133\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n", "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'n'"]}], "source": ["# --- 1. Imports ---\n", "from sklearn.ensemble import RandomForestClassifier\n", "from imblearn.pipeline import Pipeline as ImbPipeline\n", "from imblearn.over_sampling import SMOTE\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import classification_report, confusion_matrix\n", "import joblib\n", "\n", "# Custom transformer from previous step\n", "#from bank_preprocessor import BankLoanPreprocessor  # or copy-paste the class if in same notebook\n", "\n", "# --- 2. Prepare Data ---\n", "# 1. Split back into raw training and test data\n", "df_train_raw = df_combined.loc[\"train\"].copy()\n", "df_test_raw = df_combined.loc[\"test\"].copy()\n", "\n", "# 2. Define features and target\n", "X_raw = df_train_raw.drop(columns=[\"Loan Status\"])\n", "y_raw = df_train_raw[\"Loan Status\"]\n", "\n", "X_kaggle_test = df_test_raw.drop(columns=[\"Loan Status\"])\n", "y_kaggle_test = df_test_raw[\"Loan Status\"]\n", "\n", "\n", "# Split for evaluation purposes\n", "#X_train_raw, X_val_raw, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y_raw)\n", "\n", "# --- 3. Define Pipeline ---\n", "pipeline = ImbPipeline([\n", "    (\"preprocessor\", BankLoanPreprocessor()),\n", "    (\"smote\", SMOTE(random_state=42)),\n", "    (\"classifier\", RandomForestClassifier(random_state=42))\n", "])\n", "\n", "# --- 4. Fit the Pipeline ---\n", "pipeline.fit(X_raw, y_raw)\n", "\n", "# --- 5. Evaluate ---\n", "y_pred = pipeline.predict(X_kaggle_test)\n", "print(\"\ud83d\udcca Classification Report:\\n\", classification_report(y_kaggle_test, y_pred))\n", "print(\"\ud83e\uddf1 Confusion Matrix:\\n\", confusion_matrix(y_kaggle_test, y_pred))\n", "\n", "# --- 6. Save the model ---\n", "joblib.dump(pipeline, \"rf_pipeline_model.pkl\")\n", "print(\"\u2705 Model saved as 'rf_pipeline_model.pkl'\")\n"]}, {"cell_type": "code", "execution_count": 321, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["              precision    recall  f1-score   support\n", "\n", "           0       0.53      0.98      0.69     15300\n", "           1       0.56      0.02      0.05     13613\n", "\n", "    accuracy                           0.53     28913\n", "   macro avg       0.55      0.50      0.37     28913\n", "weighted avg       0.55      0.53      0.39     28913\n", "\n"]}], "source": ["# Predict on Kaggle test set (raw)\n", "y_pred_kaggle = pipeline.predict(df_test)\n", "print(classification_report(df_target[\"Loan Status\"], y_pred_kaggle))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# \ud83e\udde0 Why the Dash App Fails Without Preprocessing?\n", "\n", "When we train a machine learning model using **scikit-learn**, we typically convert all categorical (text-based) features into **numerical representations** \u2014 because models like `SVC`, `RandomForest`, or `XGBoost` **cannot operate on raw string data**.\n", "\n", "This process is called **feature engineering**, which often includes:\n", "\n", "- **One-hot encoding** (e.g., `Home Ownership = RENT` \u2192 column `Home Ownership_RENT = 1`)\n", "- **Label encoding**\n", "- **Scaling** (e.g., MinMaxScaler or StandardScaler)\n", "- **Handling missing values**\n", "- **Date feature extraction** (e.g., month, year)\n", "\n", "During training, this transformation was applied **after merging** `df_train` and `df_test` into `df_combined`. This ensured **consistent encoding across all values**, even if a value appeared only in the test set (e.g., `'n'` or `'BAT2522922'` in `Batch Enrolled`).\n", "\n", "---\n", "\n", "### \u26a0\ufe0f Problem in the Current Pipeline\n", "\n", "Now, when trying to train or deploy a new model with only `df_train` or `df_test` separately, these inconsistencies **resurface** because unseen labels are not accounted for:\n", "\n", "```plaintext\n", "ValueError: could not convert string to float: 'n'\n", "```\n", "\n", "---\n", "\n", "### \u2705 Solution: Reuse Consistent Transformation\n", "\n", "To ensure consistent encoding, we need to:\n", "\n", "1. Combine the original cleaned `df_train` and `df_test` again into `df_combined`:\n", "```python\n", "combined = pd.concat([df_train, df_test], axis=0, keys=[\"train\", \"test\"])\n", "```\n", "\n", "2. Apply encoding to the entire `df_combined`:\n", "```python\n", "# Grade ordinal encoding\n", "combined['Grade'] = combined['Grade'].map(grade_order)\n", "\n", "# Label encoding\n", "for col in label_cols:\n", "    le = LabelEncoder()\n", "    combined[col] = le.fit_transform(combined[col].astype(str))\n", "\n", "# Frequency encoding for Loan Title\n", "combined['Loan Title'] = combined['Loan Title'].map(freq_map).fillna(0)\n", "\n", "# One-hot encoding\n", "combined = pd.get_dummies(combined, columns=one_hot_cols, drop_first=True)\n", "```\n", "\n", "3. Split them back:\n", "```python\n", "X_train_final = combined.loc['train'].drop(columns=['Loan Status'])\n", "y_train_final = combined.loc['train']['Loan Status']\n", "X_test_final = combined.loc['test'].drop(columns=['Loan Status'])\n", "y_test_final = combined.loc['test']['Loan Status']\n", "```\n", "\n", "4. Use these for fitting your final pipeline.\n", "\n", "---\n", "\n", "### \ud83e\uddf1 Outcome\n", "\n", "By encoding consistently across both training and test datasets upfront, you avoid mismatches and ensure your model generalizes well at inference time \u2014 especially when deployed or reused in apps.\n", "\n", "Let me know to implement this in code.\n"]}, {"cell_type": "code", "execution_count": 325, "metadata": {}, "outputs": [{"ename": "ValueError", "evalue": "Input y_true contains NaN.", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)", "Cell \u001b[0;32mIn[325], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m     44\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test_final)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\ud83d\udcca Classification Report:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, classification_report(y_test_final, y_pred))\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\ud83e\uddf1 Confusion Matrix:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, confusion_matrix(y_test_final, y_pred))\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Save model\u001b[39;00m\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2671\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[1;32m   2564\u001b[0m \n\u001b[1;32m   2565\u001b[0m \u001b[38;5;124;03mRead more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2667\u001b[0m \u001b[38;5;124;03m<BLANKLINE>\u001b[39;00m\n\u001b[1;32m   2668\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2670\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m attach_unique(y_true, y_pred)\n\u001b[0;32m-> 2671\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2674\u001b[0m     labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/sklearn/metrics/_classification.py:99\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     97\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[1;32m     98\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m---> 99\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/sklearn/utils/multiclass.py:417\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name, raise_unknown)\u001b[0m\n\u001b[1;32m    415\u001b[0m     data \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mif\u001b[39;00m issparse(y) \u001b[38;5;28;01melse\u001b[39;00m y\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39many(data \u001b[38;5;241m!=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(data, \u001b[38;5;28mint\u001b[39m)):\n\u001b[0;32m--> 417\u001b[0m         _assert_all_finite(data, input_name\u001b[38;5;241m=\u001b[39minput_name)\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m suffix\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# Check multiclass\u001b[39;00m\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/sklearn/utils/validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    121\u001b[0m     X,\n\u001b[1;32m    122\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[1;32m    123\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[1;32m    124\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[1;32m    125\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    126\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    127\u001b[0m )\n", "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.11/site-packages/sklearn/utils/validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n", "\u001b[0;31mValueError\u001b[0m: Input y_true contains NaN."]}], "source": ["from imblearn.pipeline import Pipeline as ImbPipeline\n", "from imblearn.over_sampling import SMOTE\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.metrics import classification_report, confusion_matrix\n", "from sklearn.preprocessing import LabelEncoder\n", "\n", "# Combine cleaned train and test sets\n", "combined = pd.concat([df_train, df_test], axis=0, keys=[\"train\", \"test\"])\n", "\n", "# Grade ordinal encoding\n", "grade_order = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7}\n", "combined['Grade'] = combined['Grade'].map(grade_order)\n", "\n", "# Label encoding\n", "label_cols = [\"Sub Grade\", \"Batch Enrolled\"]\n", "for col in label_cols:\n", "    le = LabelEncoder()\n", "    combined[col] = le.fit_transform(combined[col].astype(str))\n", "\n", "# Frequency encoding for Loan Title\n", "freq_map = combined[\"Loan Title\"].value_counts().to_dict()\n", "combined[\"Loan Title\"] = combined[\"Loan Title\"].map(freq_map).fillna(0)\n", "\n", "# One-hot encoding\n", "one_hot_cols = [\"Initial List Status\", \"Employment Duration\", \"Verification Status\"]\n", "combined = pd.get_dummies(combined, columns=one_hot_cols, drop_first=True)\n", "\n", "# Final train-test split\n", "X_train_final = combined.loc['train'].drop(columns=['Loan Status'])\n", "y_train_final = combined.loc['train']['Loan Status']\n", "X_test_final = combined.loc['test'].drop(columns=['Loan Status'])\n", "y_test_final = combined.loc['test']['Loan Status']\n", "\n", "# Define pipeline\n", "pipeline = ImbPipeline([\n", "    (\"smote\", SMOTE(random_state=42)),\n", "    (\"classifier\", RandomForestClassifier(random_state=42))\n", "])\n", "\n", "# Train pipeline\n", "pipeline.fit(X_train_final, y_train_final)\n", "\n", "# Evaluate\n", "y_pred = pipeline.predict(X_test_final)\n", "print(\"\ud83d\udcca Classification Report:\\n\", classification_report(y_test_final, y_pred))\n", "print(\"\ud83e\uddf1 Confusion Matrix:\\n\", confusion_matrix(y_test_final, y_pred))\n", "\n", "# Save model\n", "import joblib\n", "joblib.dump(pipeline, \"final_rf_model.pkl\")\n", "print(\"\u2705 Model saved as 'final_rf_model.pkl'\")"]}, {"cell_type": "code", "execution_count": 327, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\u2705 Model saved as 'final_rf_model.pkl'\n", "\ud83d\udcc1 'final_submission.csv' created successfully!\n"]}], "source": ["from imblearn.pipeline import Pipeline as ImbPipeline\n", "from imblearn.over_sampling import SMOTE\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.metrics import classification_report, confusion_matrix\n", "from sklearn.preprocessing import LabelEncoder\n", "import joblib\n", "\n", "# Combine cleaned train and test sets\n", "combined = pd.concat([df_train, df_test], axis=0, keys=[\"train\", \"test\"])\n", "\n", "# Grade ordinal encoding\n", "grade_order = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7}\n", "combined['Grade'] = combined['Grade'].map(grade_order)\n", "\n", "# Label encoding\n", "label_cols = [\"Sub Grade\", \"Batch Enrolled\"]\n", "for col in label_cols:\n", "    le = LabelEncoder()\n", "    combined[col] = le.fit_transform(combined[col].astype(str))\n", "\n", "# Frequency encoding for Loan Title\n", "freq_map = combined[\"Loan Title\"].value_counts().to_dict()\n", "combined[\"Loan Title\"] = combined[\"Loan Title\"].map(freq_map).fillna(0)\n", "\n", "# One-hot encoding\n", "one_hot_cols = [\"Initial List Status\", \"Employment Duration\", \"Verification Status\"]\n", "combined = pd.get_dummies(combined, columns=one_hot_cols, drop_first=True)\n", "\n", "# Final train-test split\n", "X_train_final = combined.loc['train'].drop(columns=['Loan Status'])\n", "y_train_final = combined.loc['train']['Loan Status']\n", "X_test_final = combined.loc['test'].drop(columns=['Loan Status'], errors='ignore')\n", "\n", "# Define pipeline\n", "pipeline = ImbPipeline([\n", "    (\"smote\", SMOTE(random_state=42)),\n", "    (\"classifier\", RandomForestClassifier(random_state=42))\n", "])\n", "\n", "# Train pipeline\n", "pipeline.fit(X_train_final, y_train_final)\n", "\n", "# Save model\n", "joblib.dump(pipeline, \"final_rf_model.pkl\")\n", "print(\"\u2705 Model saved as 'final_rf_model.pkl'\")\n", "\n", "# Create final submission\n", "preds = pipeline.predict(X_test_final)\n", "submission = pd.DataFrame({\n", "    \"ID\": original_test[\"ID\"],\n", "    \"Loan Status\": preds\n", "})\n", "submission.to_csv(\"final_submission.csv\", index=False)\n", "print(\"\ud83d\udcc1 'final_submission.csv' created successfully!\")"]}], "metadata": {"kernelspec": {"display_name": "DS", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.0"}}, "nbformat": 4, "nbformat_minor": 2}