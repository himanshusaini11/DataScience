{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank Loan Defaulter Hackathon Project\n",
    "### Submitted by Himanshu Saini\n",
    "#### Dataset Link: https://www.kaggle.com/datasets/ankitkalauni/bank-loan-defaulter-prediction-hackathon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category = UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Global style for plots\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 12,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"axes.edgecolor\": \"black\",\n",
    "    \"axes.linewidth\": 1,\n",
    "    \"xtick.color\": \"black\",\n",
    "    \"ytick.color\": \"black\",\n",
    "    \"xtick.direction\": \"out\",\n",
    "    \"ytick.direction\": \"out\",\n",
    "    \"xtick.bottom\": True,\n",
    "    \"ytick.left\": True,\n",
    "    \"xtick.top\": False,\n",
    "    \"ytick.right\": False,\n",
    "    \"figure.dpi\": 150,\n",
    "    \"legend.frameon\": True,\n",
    "    \"legend.facecolor\": \"white\",\n",
    "    \"legend.edgecolor\": \"black\",\n",
    "    \"legend.fontsize\": 12\n",
    "})\n",
    "\n",
    "# Apply to seaborn\n",
    "sns.set_theme(context=\"notebook\", style=\"ticks\")\n",
    "\n",
    "# Reproducibility\n",
    "rnd_num = 42\n",
    "np.random.seed(rnd_num)\n",
    "\n",
    "# Root directory\n",
    "ROOT = Path(os.getcwd()).parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure saver helper (use after each plot)\n",
    "def savefig(name, out_dir):\n",
    "    out = Path(out_dir)\n",
    "    out.mkdir(exist_ok=True, parents=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out/name, dpi=300, bbox_inches=\"tight\")\n",
    "    print(\"Saved:\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data\n",
    "df_train = pd.read_csv(\"../data/raw/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/raw/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First five rows.\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype, cols in df_train.groupby(df_train.dtypes, axis=1):\n",
    "    print(f\"{dtype} ({len(cols.columns)} columns):\")\n",
    "    print(list(cols.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statics.\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First five rows.\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype, cols in df_test.groupby(df_test.dtypes, axis=1):\n",
    "    print(f\"{dtype} ({len(cols.columns)} columns):\")\n",
    "    print(list(cols.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statics.\n",
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the test data (remove target column).\n",
    "df_test.drop(\"Loan Status\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify train dataset column types.\n",
    "df_train_categorical_cols = df_train.select_dtypes(include=\"object\").columns.tolist()\n",
    "df_train_numerical_cols = df_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify test dataset column types.\n",
    "df_test_categorical_cols = df_test.select_dtypes(include=\"object\").columns.tolist()\n",
    "df_test_numerical_cols = df_test.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_train_numerical_cols:\n",
    "    if df_train[col].nunique()<=200:\n",
    "        print (col, df_train[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_train_categorical_cols:\n",
    "    if df_train[col].nunique()<=200:\n",
    "        print (col, df_train[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.nunique().sort_values().plot(kind=\"barh\", figsize=(10, 12))\n",
    "plt.xscale(\"log\")\n",
    "plt.title(\"Unique Values per Column\")\n",
    "plt.axvline(x=40, color='red', linestyle='--', label='Threshold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "savefig(\"01_UniqueValuesPerColumn.png\", \"../results\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the categorical features out from train data.\n",
    "df_train_filter = df_train.loc[:, df_train.nunique() <= 40]\n",
    "df_train_filter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define outlier capping function (calling and flooring).\n",
    "def outlier_processing(x):\n",
    "     x = x.clip(lower=x.quantile(0.05), upper=x.quantile(0.95))\n",
    "     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Capping to Numeric Columns of train dataset.\n",
    "df_train[df_train_numerical_cols] = df_train[df_train_numerical_cols].apply(outlier_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe Data After Capping\n",
    "df_train[df_train_numerical_cols].describe(percentiles=[0.01,0.05,0.25,0.50,0.75,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance of Columns\n",
    "df_train[df_test_numerical_cols].var().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds\n",
    "drop_threshold = 0.0001\n",
    "flag_threshold = 0.05\n",
    "\n",
    "# Drop zero/near-zero variance columns\n",
    "zero_var_cols = df_train[df_test_numerical_cols].var(numeric_only=True)\n",
    "zero_var_cols = zero_var_cols[zero_var_cols < drop_threshold].index.tolist()\n",
    "\n",
    "# Flag low-variance columns for review (but not yet dropped)\n",
    "low_var_cols = df_train[df_test_numerical_cols].var(numeric_only=True)\n",
    "low_var_cols = low_var_cols[(low_var_cols >= drop_threshold) & (low_var_cols < flag_threshold)].index.tolist()\n",
    "\n",
    "print(\"Drop these (zero or near-zero variance):\", zero_var_cols)\n",
    "print(\"Review these (low variance):\", low_var_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize result dictionary\n",
    "cat_analysis = {}\n",
    "\n",
    "# Thresholds\n",
    "dominance_threshold = 0.95\n",
    "high_card_threshold = 100\n",
    "\n",
    "# Analyze each categorical column\n",
    "for col in df_test_categorical_cols:\n",
    "    n_unique = df_train[col].nunique(dropna=False)\n",
    "    top_freq = df_train[col].value_counts(normalize=True, dropna=False).iloc[0]\n",
    "    cat_analysis[col] = {\n",
    "        \"Unique Values\": n_unique,\n",
    "        \"Top Category %\": round(top_freq * 100, 2),\n",
    "        \"Drop (High Cardinality)\": n_unique > high_card_threshold,\n",
    "        \"Drop (Dominant Category)\": top_freq > dominance_threshold\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame\n",
    "cat_analysis_df = pd.DataFrame(cat_analysis).T.sort_values(by=\"Unique Values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_analysis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the above analysis we found the following:\n",
    "### 1. Drop numerical col \"Collection 12 months Medical\", \"Accounts Delinquent\" because they have zero variance therefore, should drop.\n",
    "### 2. Drop categorical col \"Payment Plan\", \"Application Type\" because contant values and highly imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col = ['ID', 'Collection 12 months Medical', 'Accounts Delinquent', 'Payment Plan', 'Application Type']\n",
    "df_train.drop(columns=drop_col, inplace=True)\n",
    "df_test.drop(columns=[col for col in drop_col if col in df_test.columns], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Train & Test for Uniform Processing.\n",
    "# use keys to track the data.\n",
    "df_combined = pd.concat([df_train, df_test], axis=0, keys=[\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select categorical columns\n",
    "cat_cols = df_train.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Prepare summary\n",
    "encoding_suggestions = []\n",
    "\n",
    "for col in cat_cols:\n",
    "    unique_vals = df_train[col].nunique(dropna=False)\n",
    "    top_cat_pct = df_train[col].value_counts(normalize=True, dropna=False).iloc[0] # The percentage of rows that belong to the most frequent category in a given column.\n",
    "\n",
    "    if unique_vals <= 10:\n",
    "        if col in ['Grade']:  # Known ordinal from domain\n",
    "            encoding_type = \"Ordinal Encoding\"\n",
    "        else:\n",
    "            encoding_type = \"One-Hot Encoding\"\n",
    "    elif 10 < unique_vals <= 50:\n",
    "        encoding_type = \"Label Encoding\"\n",
    "    elif unique_vals > 50:\n",
    "        encoding_type = \"Frequency Encoding / Target Encoding\"\n",
    "    else:\n",
    "        encoding_type = \"Review Manually\"\n",
    "\n",
    "    dominance_flag = \"Dominant Category\" if top_cat_pct > 0.95 else \"\"\n",
    "    \n",
    "    encoding_suggestions.append({\n",
    "        \"Column\": col,\n",
    "        \"Unique Values\": unique_vals,\n",
    "        \"Top Category %\": round(top_cat_pct * 100, 2),\n",
    "        \"Suggested Encoding\": encoding_type,\n",
    "        \"Note\": dominance_flag\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "encoding_df = pd.DataFrame(encoding_suggestions).sort_values(by=\"Unique Values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemnting encoding\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --- 1. Ordinal Encoding for 'Grade'\n",
    "grade_order = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7}\n",
    "df_combined[\"Grade\"] = df_combined[\"Grade\"].map(grade_order)\n",
    "\n",
    "# --- 2. Label Encoding for 'Sub Grade' and 'Batch Enrolled'\n",
    "label_cols = [\"Sub Grade\", \"Batch Enrolled\"]\n",
    "le = LabelEncoder()\n",
    "for col in label_cols:\n",
    "    df_combined[col] = le.fit_transform(df_combined[col])\n",
    "\n",
    "# --- 3. Frequency Encoding for 'Loan Title'\n",
    "freq_map = df_combined[\"Loan Title\"].value_counts().to_dict()\n",
    "df_combined[\"Loan Title\"] = df_combined[\"Loan Title\"].map(freq_map)\n",
    "\n",
    "# --- 4. One-Hot Encoding for 'Initial List Status', 'Employment Duration', 'Verification Status'\n",
    "one_hot_cols = [\"Initial List Status\", \"Employment Duration\", \"Verification Status\"]\n",
    "df_combined = pd.get_dummies(df_combined, columns=one_hot_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore train and test from MultiIndex\n",
    "df_train_encoded = df_combined.loc[\"train\"].copy()\n",
    "df_test_encoded = df_combined.loc[\"test\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Loan Status\" in df_train_encoded.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Loan Status\" in df_test_encoded.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_encoded[\"Loan Status\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_encoded.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dixtribution Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Features and Target\n",
    "X = df_train_encoded.drop(columns=[\"Loan Status\"])\n",
    "y = df_train_encoded[\"Loan Status\"]\n",
    "X_test = df_test_encoded.drop(columns=[\"Loan Status\"])\n",
    "\n",
    "print(\"Original Class Distribution:\")\n",
    "print(Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "print(\"After SMOTE\")  \n",
    "print(Counter(y_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply NearMiss\n",
    "nearmiss = NearMiss(version=1)\n",
    "X_nm, y_nm = nearmiss.fit_resample(X, y)\n",
    "print(\"After NearMiss:\")\n",
    "print(Counter(y_nm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    \"Original\": pd.Series(Counter(y)),\n",
    "    \"SMOTE\": pd.Series(Counter(y_smote)),\n",
    "    \"NearMiss\": pd.Series(Counter(y_nm))\n",
    "}).T\n",
    "comparison_df.columns = [\"Non-Defaulter (0)\", \"Defaulter (1)\"]\n",
    "print(\"Class Distribution Comparison:\\n\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "comparison_df.plot(kind=\"bar\", figsize=(8, 4), colormap=\"viridis\")\n",
    "plt.title(\"Class Distribution: Original vs SMOTE vs NearMiss\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "savefig(\"02_DistributionPlot.png\", \"../results\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Metrics (for classification)\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Model Selection (Optional for tuning)\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_test, y_test, name=\"\"):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob) if y_prob is not None else None\n",
    "    con_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f\"Results: {name}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Accuracy      : {acc:.4f}\")\n",
    "    print(f\"Precision     : {prec:.4f}\")\n",
    "    print(f\"Recall        : {rec:.4f}\")\n",
    "    print(f\"F1 Score      : {f1:.4f}\")\n",
    "    print(f\"ROC AUC Score : {roc_auc:.4f}\" if roc_auc is not None else \"ROC AUC not available\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(con_mat)\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Add feature importance if supported\n",
    "    feature_importance = None\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        feature_importance = model.feature_importances_\n",
    "    elif hasattr(model, \"coef_\"):\n",
    "        try:\n",
    "            feature_importance = abs(model.coef_[0])  # Optional, if linear\n",
    "        except:\n",
    "            feature_importance = None\n",
    "    \n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1 Score\": f1,\n",
    "        \"ROC AUC\": roc_auc,\n",
    "        \"Confusion Matrix\": con_mat,\n",
    "        \"Feature Importance\": feature_importance\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split the data (80/20) for clean validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Original (X, y).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Test case\n",
    "test = df_test_encoded.drop(columns=[\"Loan Status\"])\n",
    "\n",
    "# Over-sampled using SMOTE (X_smote, y_smote)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote_train, y_smote_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Under-sampled using NearMiss (X_nm, y_nm)\n",
    "nearmiss = NearMiss()\n",
    "X_nm_train, y_nm_train = nearmiss.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Original\n",
    "result_lr_orig = evaluate_model(lr, X_train, y_train, X_test, y_test, name=\"LogReg - Original\")\n",
    "\n",
    "# SMOTE\n",
    "result_lr_smote = evaluate_model(lr, X_smote_train, y_smote_train, X_test, y_test, name=\"LogReg - SMOTE\")\n",
    "\n",
    "# NearMiss\n",
    "result_lr_nm = evaluate_model(lr, X_nm_train, y_nm_train, X_test, y_test, name=\"LogReg - NearMiss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2. Decison Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model setup\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Original\n",
    "result_dt_orig = evaluate_model(dt, X_train, y_train, X_test, y_test, name=\"DecisionTree - Original\")\n",
    "\n",
    "# SMOTE\n",
    "result_dt_smote = evaluate_model(dt, X_smote_train, y_smote_train, X_test, y_test, name=\"DecisionTree - SMOTE\")\n",
    "\n",
    "# NearMiss\n",
    "result_dt_nm = evaluate_model(dt, X_nm_train, y_nm_train, X_test, y_test, name=\"DecisionTree - NearMiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with base settings\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Original\n",
    "result_rf_orig = evaluate_model(rf, X_train, y_train, X_test, y_test, name=\"RandomForest - Original\")\n",
    "\n",
    "# SMOTE\n",
    "result_rf_smote = evaluate_model(rf, X_smote_train, y_smote_train, X_test, y_test, name=\"RandomForest - SMOTE\")\n",
    "\n",
    "# NearMiss\n",
    "result_rf_nm = evaluate_model(rf, X_nm_train, y_nm_train, X_test, y_test, name=\"RandomForest - NearMiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with probability enabled for ROC AUC\n",
    "svc = SVC(probability=True, random_state=42)\n",
    "\n",
    "# Original\n",
    "result_svc_orig = evaluate_model(svc, X_train, y_train, X_test, y_test, name=\"SVC - Original\")\n",
    "\n",
    "# SMOTE\n",
    "result_svc_smote = evaluate_model(svc, X_smote_train, y_smote_train, X_test, y_test, name=\"SVC - SMOTE\")\n",
    "\n",
    "# NearMiss\n",
    "result_svc_nm = evaluate_model(svc, X_nm_train, y_nm_train, X_test, y_test, name=\"SVC - NearMiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "\n",
    "# Evaluate on Original Data\n",
    "result_xgb_orig = evaluate_model(xgb, X_train, y_train, X_test, y_test, name=\"XGBoost - Original\")\n",
    "\n",
    "# Evaluate on SMOTE-balanced Data\n",
    "result_xgb_smote = evaluate_model(xgb, X_smote_train, y_smote_train, X_test, y_test, name=\"XGBoost - SMOTE\")\n",
    "\n",
    "# Evaluate on NearMiss-balanced Data\n",
    "result_xgb_nm = evaluate_model(xgb, X_nm_train, y_nm_train, X_test, y_test, name=\"XGBoost - NearMiss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrices = pd.DataFrame([result_lr_orig, result_lr_smote, result_lr_nm, result_dt_orig, result_dt_smote, result_dt_nm, result_rf_orig, result_rf_smote, result_rf_nm, result_svc_orig, result_svc_smote, result_svc_nm, result_xgb_orig, result_xgb_smote, result_xgb_nm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLot the Accuracies of the different models\n",
    "# Sort models by Accuracy for better visualization\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(model_metrices.sort_values(by=\"Accuracy\", ascending=True)[\"Model\"], model_metrices.sort_values(by=\"Accuracy\", ascending=True)[\"Accuracy\"], color=\"dimgray\")\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.title(\"Model Accuracy Comparison\")\n",
    "plt.grid(axis='x', linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Annotate bars\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.005, bar.get_y() + bar.get_height() / 2, f\"{width:.2f}\", va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "savefig(\"03_ModelAccuracyComparison.png\", \"../results\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLot the Precision of the different models\n",
    "# Sort models by Accuracy for better visualization\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(model_metrices.sort_values(by=\"Precision\", ascending=True)[\"Model\"], model_metrices.sort_values(by=\"Precision\", ascending=True)[\"Precision\"], color=\"dimgray\")\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.title(\"Model Precision Comparison\")\n",
    "plt.grid(axis='x', linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Annotate bars\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.005, bar.get_y() + bar.get_height() / 2, f\"{width:.2f}\", va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "savefig(\"04_ModelPrecisionComparison.png\", \"../results\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLot the Recall of the different models\n",
    "# Sort models by Recall for better visualization\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(model_metrices.sort_values(by=\"Recall\", ascending=True)[\"Model\"], model_metrices.sort_values(by=\"Recall\", ascending=True)[\"Recall\"], color=\"dimgray\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.title(\"Model Recall Comparison\")\n",
    "plt.grid(axis='x', linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Annotate bars\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.005, bar.get_y() + bar.get_height() / 2, f\"{width:.2f}\", va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "savefig(\"05_ModelRecallComparison.png\", \"../results\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLot the F1 Score of the different models\n",
    "# Sort models by Recall for better visualization\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(model_metrices.sort_values(by=\"F1 Score\", ascending=True)[\"Model\"], model_metrices.sort_values(by=\"F1 Score\", ascending=True)[\"F1 Score\"], color=\"dimgray\")\n",
    "plt.xlabel(\"F1 Score\")\n",
    "plt.title(\"Model F1 Score Comparison\")\n",
    "plt.grid(axis='x', linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Annotate bars\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.005, bar.get_y() + bar.get_height() / 2, f\"{width:.2f}\", va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "savefig(\"06_ModelF1ScoreComparison.png\", \"../results\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLot the ROC AUC of the different models\n",
    "# Sort models by Recall for better visualization\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(model_metrices.sort_values(by=\"ROC AUC\", ascending=True)[\"Model\"], model_metrices.sort_values(by=\"ROC AUC\", ascending=True)[\"ROC AUC\"], color=\"dimgray\")\n",
    "plt.xlabel(\"ROC AUC\")\n",
    "plt.title(\"Model ROC AUC Comparison\")\n",
    "plt.grid(axis='x', linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Annotate bars\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.005, bar.get_y() + bar.get_height() / 2, f\"{width:.2f}\", va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "savefig(\"07_Model_ROC_AUC_Comparison.png\", \"../results\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter tree-based models with valid feature importance ===\n",
    "tree_models = model_metrices[model_metrices[\"Feature Importance\"].notnull()].copy()\n",
    "\n",
    "# Plot each model's top N important features ===\n",
    "N = 10  # Top N features to display (change if needed)\n",
    "feature_names = X_train.columns  # Ensure you're using correct feature list\n",
    "count = 0\n",
    "for idx, row in tree_models.iterrows():\n",
    "    count += 1\n",
    "    importances = np.array(row[\"Feature Importance\"])\n",
    "    \n",
    "    # Get indices of top N features\n",
    "    top_idx = np.argsort(importances)[-N:][::-1]\n",
    "    top_features = feature_names[top_idx]\n",
    "    top_importances = importances[top_idx]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.barh(top_features[::-1], top_importances[::-1])  # Reversed for descending bars\n",
    "    plt.title(f\"Feature Importance: {row['Model']}\")\n",
    "    plt.xlabel(\"Importance Score\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "    fig = str(str(count) + \"_\" + str(row['Model']))\n",
    "    savefig(fig, \"../results\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "param_grid = {'max_depth': [3, 4, 5], 'max_features': ['int', 'sqrt', 'log2']}\n",
    "modelgrid = GridSearchCV(RandomForestClassifier(), param_grid, refit=True, verbose=3, cv=cv, scoring='neg_log_loss')\n",
    "modelgrid.fit(X_smote_train, y_smote_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelgrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf_cv = RandomForestClassifier(max_depth=5, max_features='sqrt', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf_cv.fit(X_smote_train, y_smote_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation (Log loss metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicting probablities\n",
    "predict_train = model_rf_cv.predict_proba(X_smote_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_val = model_rf_cv.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_smote_train, predict_train), log_loss(y_test, predict_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cv_smt = model_rf_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_pred_cv_smt)\n",
    "prec = precision_score(y_test, y_pred_cv_smt, zero_division=0)\n",
    "rec = recall_score(y_test, y_pred_cv_smt)\n",
    "f1 = f1_score(y_test, y_pred_cv_smt)\n",
    "roc_auc = roc_auc_score(y_test, predict_val[:, 1])\n",
    "con_mat = confusion_matrix(y_test, y_pred_cv_smt)\n",
    "\n",
    "print(f\"Results:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Accuracy      : {acc:.4f}\")\n",
    "print(f\"Precision     : {prec:.4f}\")\n",
    "print(f\"Recall        : {rec:.4f}\")\n",
    "print(f\"F1 Score      : {f1:.4f}\")\n",
    "print(f\"ROC AUC Score : {roc_auc:.4f}\" if roc_auc is not None else \"ROC AUC not available\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(con_mat)\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = model_rf_cv.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genrate Submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = pd.DataFrame(predict_test)\n",
    "final_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(final_result[1])\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.columns = [\"Loan Status\"]\n",
    "submission[\"Loan Status\"] = round(submission[\"Loan Status\"], 6)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"../results/HS_Submission_SMT.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Nearmiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline \n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"nm\", NearMiss()),\n",
    "    (\"clf\", RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"nm__version\": [1, 2, 3],\n",
    "    \"clf__n_estimators\": [300, 500, 800],\n",
    "    \"clf__max_depth\": [None, 10, 20],\n",
    "    \"clf__min_samples_split\": [2, 5, 10],\n",
    "    \"clf__min_samples_leaf\": [1, 2, 4],\n",
    "    \"clf__max_features\": [\"sqrt\", \"log2\"],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    scoring=make_scorer(recall_score, pos_label=1),\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    verbose=1,\n",
    "    return_train_score=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicting probablities\n",
    "predict_train_gs = gs.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_val_gs = gs.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_train, predict_train_gs), log_loss(y_test, predict_val_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gs = gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_val_gs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_pred_gs)\n",
    "prec = precision_score(y_test, y_pred_gs, zero_division=0)\n",
    "rec = recall_score(y_test, y_pred_gs)\n",
    "f1 = f1_score(y_test, y_pred_gs)\n",
    "roc_auc = roc_auc_score(y_test, predict_val_gs[:, 1])\n",
    "con_mat = confusion_matrix(y_test, y_pred_gs)\n",
    "\n",
    "print(f\"Results:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Accuracy      : {acc:.4f}\")\n",
    "print(f\"Precision     : {prec:.4f}\")\n",
    "print(f\"Recall        : {rec:.4f}\")\n",
    "print(f\"F1 Score      : {f1:.4f}\")\n",
    "print(f\"ROC AUC Score : {roc_auc:.4f}\" if roc_auc is not None else \"ROC AUC not available\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(con_mat)\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test_gs = gs.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test_gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genrate Submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = pd.DataFrame(predict_test_gs)\n",
    "final_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(final_result[1])\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.columns = [\"Loan Status\"]\n",
    "submission[\"Loan Status\"] = round(submission[\"Loan Status\"], 6)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"../results/HS_Submission_NM_GS.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"nm\", NearMiss()),  # sampler runs per fold; can be toggled off via grid\n",
    "    (\"clf\", RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_dist = {\n",
    "    # toggle sampler + its most impactful knob\n",
    "    \"nm\": [NearMiss(version=1), NearMiss(version=3), \"passthrough\"],\n",
    "    # smaller, high-impact RF knobs for recall\n",
    "    \"clf__n_estimators\": randint(120, 320),          # smaller during search; retrain bigger later\n",
    "    \"clf__max_depth\": [None, 10, 20],\n",
    "    \"clf__min_samples_split\": randint(2, 10),\n",
    "    \"clf__min_samples_leaf\": randint(1, 6),\n",
    "    \"clf__max_features\": [\"sqrt\", \"log2\"],           # stable choices for high-dim tabular\n",
    "    \"clf__class_weight\": [None, \"balanced\"]          # compare against no-sampler path\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=48,                         # ~5–10× fewer fits than your grid\n",
    "    scoring=make_scorer(recall_score, pos_label=1),\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    "    return_train_score=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicting probablities\n",
    "predict_train_rs = rs.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_val_rs = rs.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_train, predict_train_rs), log_loss(y_test, predict_val_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rs = gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_pred_rs)\n",
    "prec = precision_score(y_test, y_pred_rs, zero_division=0)\n",
    "rec = recall_score(y_test, y_pred_rs)\n",
    "f1 = f1_score(y_test, y_pred_rs)\n",
    "roc_auc = roc_auc_score(y_test, predict_val_rs[:, 1])\n",
    "con_mat = confusion_matrix(y_test, y_pred_rs)\n",
    "\n",
    "print(f\"Results:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Accuracy      : {acc:.4f}\")\n",
    "print(f\"Precision     : {prec:.4f}\")\n",
    "print(f\"Recall        : {rec:.4f}\")\n",
    "print(f\"F1 Score      : {f1:.4f}\")\n",
    "print(f\"ROC AUC Score : {roc_auc:.4f}\" if roc_auc is not None else \"ROC AUC not available\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(con_mat)\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Plot for Final Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract trained model from GridSearchCV\n",
    "final_rf = modelgrid.best_estimator_\n",
    "final_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = final_rf.feature_importances_\n",
    "feature_names = X.columns  # Make sure this matches the columns used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "importances_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": importances\n",
    "}).sort_values(by=\"Importance\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Top 20 Features\n",
    "top_n = 10\n",
    "top_features = importances_df.head(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=top_features, palette=\"viridis\")\n",
    "plt.title(f\"Top {top_n} Important Features - Random Forest\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "savefig(\"13_ImportantFeatures_RF.png\", \"../results\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
